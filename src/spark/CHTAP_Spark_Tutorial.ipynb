{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14540416\n"
     ]
    }
   ],
   "source": [
    "# Seeing if PySpark works\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting correct column...\n",
      "Processing in parallel\n",
      "Distributing data...\n",
      "Parsing distributed data...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys, os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "#from multiprocessing import Pool\n",
    "\n",
    "# Getting config\n",
    "with open('/dfs/scratch1/jdunnmon/data/memex-data/config/config_spark.json') as fl:\n",
    "    config = json.load(fl)\n",
    "\n",
    "# Adding path for utils\n",
    "sys.path.append('/dfs/scratch1/jdunnmon/repos/extractors/src/utils')\n",
    "\n",
    "# Setting random seed\n",
    "seed = config['seed']\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# If memex_raw_content is a content_field, uses term as a regex in raw data in addition to getting title and body\n",
    "#term = r'\\b[Ll]ocation:|\\b[cC]ity:'\n",
    "#term = r'([Ll]ocation:.{0,100}|[cC]ity:.{0,100}|\\d\\dyo\\W|\\d\\d.{0,10}\\Wyo\\W|\\d\\d.{0,10}\\Wold\\W|\\d\\d.{0,10}\\Wyoung\\W|\\Wage\\W.{0,10}\\d\\d)'\n",
    "term = r'([Ll]ocation:[\\w\\W]{1,200}</.{0,20}>|\\W[cC]ity:[\\w\\W]{1,200}</.{0,20}>|\\d\\dyo\\W|\\d\\d.{0,10}\\Wyo\\W|\\d\\d.{0,10}\\Wold\\W|\\d\\d.{0,10}\\Wyoung\\W|\\Wage\\W.{0,10}\\d\\d)'\n",
    "\n",
    "# Setting up arguments (can also get from argparse)\n",
    "args = {}\n",
    "args['data_loc'] = '/dfs/scratch1/jdunnmon/data/memex-data/tsvs/output_all_slicetest'\n",
    "# Getting raw_content column\n",
    "print('Getting correct column...')\n",
    "files = os.listdir(args['data_loc'])\n",
    "df = pd.read_csv(os.path.join(args['data_loc'],files[0]),sep='\\t',nrows=10)\n",
    "col = df.columns.get_loc(\"memex_raw_content\")\n",
    "\n",
    "# Getting path and setting up data\n",
    "path = args['data_loc']\n",
    "file_list = os.listdir(path)\n",
    "path_list = [os.path.join(path, file) for file in file_list]\n",
    "file_data = [(path, term, col) for path in path_list if path.endswith('tsv')]\n",
    "\n",
    "out_dir = path + '/parsed/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "print('Processing in parallel')\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"parse_html\")\n",
    "sc.addPyFile('/dfs/scratch1/jdunnmon/repos/extractors/src/utils/dataset_utils.py')\n",
    "from dataset_utils import parse_html\n",
    "print('Distributing data...')\n",
    "distData = sc.parallelize(file_data)\n",
    "print('Parsing distributed data...')\n",
    "distData.foreach(parse_html)\n",
    "sc.stop()\n",
    "#threads = 30\n",
    "#pool = Pool(threads) \n",
    "#results = pool.map(parse_html, file_data)\n",
    "#pool.close()\n",
    "#pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snorkel]",
   "language": "python",
   "name": "conda-env-snorkel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
