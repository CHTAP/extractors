{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14540416\n"
     ]
    }
   ],
   "source": [
    "# Seeing if PySpark works\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting correct column...\n",
      "Processing in parallel\n",
      "Distributing data...\n",
      "Parsing distributed data...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys, os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "#from multiprocessing import Pool\n",
    "\n",
    "# Getting config\n",
    "with open('/dfs/scratch1/jdunnmon/data/memex-data/config/config_spark.json') as fl:\n",
    "    config = json.load(fl)\n",
    "\n",
    "# Adding path for utils\n",
    "sys.path.append('/dfs/scratch1/jdunnmon/repos/extractors/src/utils')\n",
    "\n",
    "# Setting random seed\n",
    "seed = config['seed']\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# If memex_raw_content is a content_field, uses term as a regex in raw data in addition to getting title and body\n",
    "#term = r'\\b[Ll]ocation:|\\b[cC]ity:'\n",
    "#term = r'([Ll]ocation:.{0,100}|[cC]ity:.{0,100}|\\d\\dyo\\W|\\d\\d.{0,10}\\Wyo\\W|\\d\\d.{0,10}\\Wold\\W|\\d\\d.{0,10}\\Wyoung\\W|\\Wage\\W.{0,10}\\d\\d)'\n",
    "term = r'([Ll]ocation:[\\w\\W]{1,200}</.{0,20}>|\\W[cC]ity:[\\w\\W]{1,200}</.{0,20}>|\\d\\dyo\\W|\\d\\d.{0,10}\\Wyo\\W|\\d\\d.{0,10}\\Wold\\W|\\d\\d.{0,10}\\Wyoung\\W|\\Wage\\W.{0,10}\\d\\d)'\n",
    "\n",
    "# Setting up arguments (can also get from argparse)\n",
    "args = {}\n",
    "args['data_loc'] = '/dfs/scratch1/jdunnmon/data/memex-data/tsvs/output_all_slicetest'\n",
    "# Getting raw_content column\n",
    "print('Getting correct column...')\n",
    "files = os.listdir(args['data_loc'])\n",
    "df = pd.read_csv(os.path.join(args['data_loc'],files[0]),sep='\\t',nrows=10)\n",
    "col = df.columns.get_loc(\"memex_raw_content\")\n",
    "\n",
    "# Getting path and setting up data\n",
    "path = args['data_loc']\n",
    "file_list = os.listdir(path)\n",
    "path_list = [os.path.join(path, file) for file in file_list]\n",
    "file_data = [(path, term, col) for path in path_list if path.endswith('tsv')]\n",
    "\n",
    "out_dir = path + '/parsed/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "print('Processing in parallel')\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"parse_html\")\n",
    "sc.addPyFile('/dfs/scratch1/jdunnmon/repos/extractors/src/utils/dataset_utils.py')\n",
    "from dataset_utils import parse_html\n",
    "print('Distributing data...')\n",
    "distData = sc.parallelize(file_data)\n",
    "print('Parsing distributed data...')\n",
    "distData.foreach(parse_html)\n",
    "sc.stop()\n",
    "#threads = 30\n",
    "#pool = Pool(threads) \n",
    "#results = pool.map(parse_html, file_data)\n",
    "#pool.close()\n",
    "#pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "pth='/lfs/local/0/jdunnmon/data/memex-data/escorts/2015/05/08/0000.jsonl.gz'\n",
    "with gzip.GzipFile(pth, 'r') as in_file:\n",
    "        \n",
    "        # Getting file objects\n",
    "        json_reader = in_file.read()\n",
    "        \n",
    "        for ii, chunk in enumerate(json_reader.splitlines()):\n",
    "            data = json.loads(chunk)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['type', 'crawl_data', 'url', 'timestamp', 'extractions', 'raw_content', 'extracted_metadata', 'version', 'extracted_text', 'content_type', 'team', 'doc_id', 'crawler'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_dict = {'id':'doc_id', 'uuid':'', 'memex_id':'doc_id', 'memex_doc_type':'type', 'memex_raw_content':'raw_content', 'memex_url':'url', 'url':'', 'extractions':'extractions'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'uuid', 'memex_id', 'memex_doc_type', 'memex_raw_content', 'memex_url', 'url', 'extractions'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on line: 293\n",
      "Error on line: 2759\n",
      "Error on line: 4247\n",
      "Error on line: 6240\n",
      "Error on line: 8249\n",
      "Error on line: 8801\n",
      "Error on line: 10022\n",
      "Error on line: 10517\n",
      "Error on line: 10518\n",
      "Error on line: 10954\n",
      "Error on line: 11458\n",
      "Error on line: 11705\n",
      "Error on line: 13726\n",
      "Error on line: 14000\n",
      "Error on line: 14444\n",
      "Error on line: 15012\n",
      "Error on line: 15311\n",
      "Error on line: 17569\n",
      "Error on line: 18056\n",
      "Error on line: 18318\n",
      "Error on line: 18325\n",
      "Error on line: 18366\n",
      "Error on line: 18961\n",
      "Error on line: 20161\n",
      "Error on line: 20485\n",
      "Error on line: 20843\n",
      "Error on line: 21346\n",
      "Error on line: 21535\n",
      "Error on line: 22567\n",
      "Error on line: 22639\n",
      "Error on line: 22641\n",
      "Error on line: 24061\n",
      "Error on line: 25125\n",
      "Error on line: 25774\n",
      "Error on line: 25798\n",
      "Error on line: 29836\n",
      "Error on line: 31447\n",
      "Error on line: 31460\n",
      "Error on line: 31594\n",
      "Error on line: 31840\n",
      "Error on line: 32446\n",
      "Error on line: 32659\n",
      "Error on line: 35157\n",
      "Error on line: 36897\n",
      "Error on line: 38125\n",
      "Error on line: 38569\n",
      "Error on line: 39943\n",
      "Error on line: 40149\n",
      "Error on line: 41859\n",
      "Error on line: 42414\n",
      "Error on line: 42987\n",
      "Error on line: 43100\n",
      "Error on line: 44913\n",
      "Error on line: 46473\n",
      "Error on line: 46836\n",
      "Error on line: 46930\n",
      "Error on line: 47049\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "sys.path.append('../utils')\n",
    "from dataset_utils import parse_html_from_jsonl_gz\n",
    "\n",
    "in_loc = '/lfs/local/0/jdunnmon/data/memex-data/escorts/2015/05/08/0000.jsonl.gz'\n",
    "in_parts = in_loc.split('/')\n",
    "in_parts[in_parts.index('escorts')] = 'escorts_preproc'\n",
    "in_parts[-1] = in_parts[-1].split('.')[0]+'.tsv'\n",
    "out_loc = '/'.join(in_parts)\n",
    "term = r'([Ll]ocation:[\\w\\W]{1,200}</.{0,20}>|\\W[cC]ity:[\\w\\W]{1,200}</.{0,20}>|\\d\\dyo\\W|\\d\\d.{0,10}\\Wyo\\W|\\d\\d.{0,10}\\Wold\\W|\\d\\d.{0,10}\\Wyoung\\W|\\Wage\\W.{0,10}\\d\\d)'\n",
    "field_dict = {'id':'doc_id', 'uuid':'', 'memex_id':'doc_id', 'memex_doc_type':'type', 'memex_raw_content':'raw_content', 'memex_url':'url', 'url':'', 'extractions':'extractions'}\n",
    "content_field = 'memex_raw_content'\n",
    "file_data = (in_loc, out_loc, term, field_dict, content_field)\n",
    "parse_html_from_jsonl_gz(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snorkel)",
   "language": "python",
   "name": "snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
