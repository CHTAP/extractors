{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "import sys\n",
    "sys.path.append('../utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Environment and Spark SQL\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Snorkel MEMEX Preprocessing\") \\\n",
    "    .config(\"spark.cores.max\", \"96\") \\\n",
    "    .config(\"spark.executor.memory\", '20g')\\\n",
    "    .config(\"spark.driver.memory\", '50g')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.memory', '50g'),\n",
       " ('spark.cores.max', '96'),\n",
       " ('spark.driver.host', '172.24.75.93'),\n",
       " ('spark.executor.memory', '20g'),\n",
       " ('spark.app.id', 'local-1539757444894'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '44634'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'Snorkel MEMEX Preprocessing')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = spark.sparkContext.getConf()\n",
    "c.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parent data path\n",
    "from dataset_utils import retrieve_all_files\n",
    "#pth='/lfs/local/0/jdunnmon/data/memex-data/escorts/2015/05/08' #/0000.jsonl.gz\n",
    "root_pth = '/lfs/local/0/jdunnmon/data/memex-data'\n",
    "json_path = 'escorts/2017'\n",
    "pth = os.path.join(root_pth,json_path)\n",
    "pth_lst = retrieve_all_files(pth)\n",
    "%time test_df = spark.read.json(pth_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of attributes to extract\n",
    "attr_list = ['doc_id','type', 'raw_content','url',\\\n",
    "             'extractions.phonenumber.results',\n",
    "             'extractions.age.results',\n",
    "             'extractions.rate.results',\n",
    "             #'extractions.location.results',\n",
    "             'extractions.ethnicity.results',\n",
    "             'extractions.email.results',\n",
    "             'extractions.incall.results'\n",
    "            ]\n",
    "\n",
    "cols = ['doc_id','type', 'raw_content','url','extracted_phone','extracted_age',\n",
    "        'extracted_rate','extracted_ethnicity',\\\n",
    "        'extracted_email','extracted_incall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping samples with no relevant data\n",
    "test_df = test_df.na.drop(subset=['doc_id','raw_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting examples\n",
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "sys.path.append('../utils')\n",
    "spark.sparkContext.addPyFile('../utils/dataset_utils.py')\n",
    "from dataset_utils import get_posting_html_fast, parse_url\n",
    "from pyspark.sql.functions import udf\n",
    "from functools import partial\n",
    "\n",
    "# Transforming raw content column\n",
    "term = r'([Ll]ocation:[\\w\\W]{1,200}</.{0,20}>|\\W[cC]ity:[\\w\\W]{1,200}</.{0,20}>|\\d\\dyo\\W|\\d\\d.{0,10}\\Wyo\\W|\\d\\d.{0,10}\\Wold\\W|\\d\\d.{0,10}\\Wyoung\\W|\\Wage\\W.{0,10}\\d\\d)'\n",
    "get_posting_html_fast_udf = udf(partial(get_posting_html_fast, search_term=term), StringType())\n",
    "parse_url_udf = udf(parse_url, StringType())\n",
    "\n",
    "# Testing on single row\n",
    "#test_row = test_df.where(test_df.doc_id == 'F1729609E6729B799FB4CC9B5EA5EE0743D8DE10741622DDA4D768443DB64242')\n",
    "#test_row = test_row.withColumn(\"raw_content_parsed\", get_posting_html_fast_udf(test_row.raw_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining query over entire df \n",
    "test_df = test_df.withColumn(\"raw_content_parsed\", get_posting_html_fast_udf(test_df.raw_content))\n",
    "test_df = test_df.withColumn(\"url_parsed\", parse_url_udf(test_df.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing and showing entire df\n",
    "%time test_df = test_df.select(attr_list+['raw_content_parsed', 'url_parsed'])\n",
    "cols = cols +['raw_content_parsed', 'url_parsed']\n",
    "test_df = test_df.toDF(*cols)\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    if 'extracted' in col:\n",
    "        test_df = test_df.withColumn(col, concat_ws(',',col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing single example\n",
    "#%time test_df_row = test_df.where(test_df.doc_id == 'F1729609E6729B799FB4CC9B5EA5EE0743D8DE10741622DDA4D768443DB64242').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_write = ['doc_id','type', 'url','url_parsed','raw_content_parsed','extracted_phone','extracted_age',\n",
    "        'extracted_rate','extracted_ethnicity',\\\n",
    "        'extracted_email','extracted_incall']\n",
    "write_path = os.path.join(root_pth,'escorts_preproc/spark_test/2017')\n",
    "%time test_df.select(cols_to_write).write.csv(write_path,header = 'true',sep='\\t',mode='overwrite')\n",
    "#.write.option(\"maxRecordsPerFile\", 1000000)\n",
    "#.coalesce(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing schema\n",
    "#test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import Context, Candidate\n",
    "from snorkel.contrib.models.text import RawText\n",
    "\n",
    "# Make sure DB is cleared\n",
    "session.query(Context).delete()\n",
    "session.query(Candidate).delete()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running phone extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import set_preprocessor\n",
    "from snorkel.parser import CorpusParser\n",
    "from parser_utils import SimpleTokenizer\n",
    "from snorkel.models import Document, Sentence\n",
    "from snorkel.parser import HTMLDocPreprocessor as pp\n",
    "\n",
    "dbname = \"\".join(in_loc.split('/'))\n",
    "postgres_location = 'postgresql:///'\n",
    "os.system(f'../utils/kill_db.sh {dbname}')\n",
    "os.system(f'createdb {dbname}')\n",
    "os.environ['SNORKELDB'] = os.path.join(postgres_location, db_name)\n",
    "\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Now we create the candidates with a simple loop\n",
    "candidate_docs = test_df.select(attr_list+['raw_content_parsed']).orderBy(\"doc_id\").distinct()\n",
    "\n",
    "# export SNORKELDB=postgresql:///${DBNAME}; cd /dfs/scratch1/jdunnmon/repos/extractors/src/shell; \n",
    "#sh ../utils/kill_db.sh ${DBNAME}; source activate snorkel; python create_dbs.py -f ${FOLDERS[$iter]}; bash -l\n",
    "\n",
    "# Using Spark to add candidates\n",
    "def spark_preprocessor(cd, verbose=False, max_doc_length=2000):\n",
    "    # Fields to add\n",
    "    text_fields = ['raw_content_parsed','url']\n",
    "    doc_text = cd['raw_content_parsed']+\" \"+cd['url_parsed']\n",
    "    doc_text = doc_text.strip()\n",
    "    doc_name = cd['doc_id']\n",
    "    extractions = cd['extractions']\n",
    "\n",
    "    # Short documents are usually parsing errors...\n",
    "    if len(doc_text) < 10:\n",
    "        if verbose:\n",
    "            print('Short Doc!')\n",
    "        continue\n",
    "\n",
    "    if max_doc_length and len(doc_text) > max_doc_length:\n",
    "        if verbose:\n",
    "            print('Long document')\n",
    "        continue\n",
    "\n",
    "    stable_id = pp.get_stable_id(doc_name)\n",
    "\n",
    "    # Yielding results, adding useful info to metadata\n",
    "    doc = Document(\n",
    "        name=doc_name, stable_id=stable_id,\n",
    "        meta={'extractions':extractions,\n",
    "              'url':url}\n",
    "    )\n",
    "    yield doc, doc_text\n",
    "    \n",
    "# Setting parser and applying corpus preprocessor\n",
    "parser=SimpleTokenizer(delim='<|>')\n",
    "corpus_parser = CorpusParser(parser=parser)\n",
    "corpus_parser.apply(list(doc_preprocessor), parallelism=parallelism, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import create_candidate_class\n",
    "extraction_type = 'phone'\n",
    "\n",
    "# Creating candidate class\n",
    "candidate_class, candidate_class_name = create_candidate_class(extraction_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_bodies = candidate_labeled_tweets \\\n",
    "    .select(\"tweet_id\", \"tweet_body\") \\\n",
    "    .orderBy(\"tweet_id\") \\\n",
    "    .distinct()\n",
    "\n",
    "# Generate and store the tweet candidates to be classified\n",
    "# Note: We split the tweets in two sets: one for which the crowd \n",
    "# labels are not available to Snorkel (test, 10%) and one for which we assume\n",
    "# crowd labels are obtained (to be used for training, 90%)\n",
    "total_tweets = tweet_bodies.count()\n",
    "test_split = total_tweets*0.1\n",
    "for i, t in enumerate(tweet_bodies.collect()):\n",
    "    split = 1 if i <= test_split else 0\n",
    "    raw_text = RawText(stable_id=t.tweet_id, name=t.tweet_id, text=t.tweet_body)\n",
    "    tweet = Tweet(tweet=raw_text, split=split)\n",
    "    session.add(tweet)\n",
    "session.commit()\n",
    "\n",
    "\n",
    "# Getting gold label for each doc\n",
    "print(\"Running regex extractor...\")\n",
    "doc_extractions = {}\n",
    "for ii, _ in enumerate(eval_cands):\n",
    "    doc_extractions[doc.name] = {}\n",
    "    if ii % 1000 == 0:\n",
    "        print(f'Extracting regexes from doc {ii} out of {len(eval_cands)}')\n",
    "    doc_extractions[doc.name]['phone'] = regex_matcher(doc, mode='phonenumbers')\n",
    "\n",
    "# Setting filename\n",
    "out_filename = \"phone_extraction_\"+postgres_db_name+\".jsonl\"\n",
    "out_folder = os.path.join(config['output_dir'], 'phone')\n",
    "out_path = os.path.join(out_folder, out_filename)\n",
    "\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "                          \n",
    "# Saving file to jsonl in extractions format\n",
    "print(f\"Saving output to {out_path}\")\n",
    "with open(out_path, 'w') as outfile:\n",
    "    for k,v in doc_extractions.items():\n",
    "        v['id'] = k\n",
    "        v['phone'] = list(v['phone'])\n",
    "        print(json.dumps(v), file=outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SANDBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"export SNORKELDB=postgresql://jdunnmon:123@localhost:5432/${DBNAME}; export CUDA_VISIBLE_DEVICES=0; \n",
    "#cd /dfs/scratch1/jdunnmon/repos/extractors/src/shell; source activate snorkel; \n",
    "#echo 'Evaluating phone extractor for node ${NODES[NODE_INDEX]} with database ${DBNAME} ...';\n",
    "#python evaluate_phone_extractor.py -f ${FOLDERS[$iter]}; bash -l\" &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing if PySpark works\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting correct column...\n",
      "Processing in parallel\n",
      "Distributing data...\n",
      "Parsing distributed data...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys, os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "#from multiprocessing import Pool\n",
    "\n",
    "# Getting config\n",
    "with open('/dfs/scratch1/jdunnmon/data/memex-data/config/config_spark.json') as fl:\n",
    "    config = json.load(fl)\n",
    "\n",
    "# Adding path for utils\n",
    "sys.path.append('/dfs/scratch1/jdunnmon/repos/extractors/src/utils')\n",
    "\n",
    "# Setting random seed\n",
    "seed = config['seed']\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# If memex_raw_content is a content_field, uses term as a regex in raw data in addition to getting title and body\n",
    "#term = r'\\b[Ll]ocation:|\\b[cC]ity:'\n",
    "#term = r'([Ll]ocation:.{0,100}|[cC]ity:.{0,100}|\\d\\dyo\\W|\\d\\d.{0,10}\\Wyo\\W|\\d\\d.{0,10}\\Wold\\W|\\d\\d.{0,10}\\Wyoung\\W|\\Wage\\W.{0,10}\\d\\d)'\n",
    "term = r'([Ll]ocation:[\\w\\W]{1,200}</.{0,20}>|\\W[cC]ity:[\\w\\W]{1,200}</.{0,20}>|\\d\\dyo\\W|\\d\\d.{0,10}\\Wyo\\W|\\d\\d.{0,10}\\Wold\\W|\\d\\d.{0,10}\\Wyoung\\W|\\Wage\\W.{0,10}\\d\\d)'\n",
    "\n",
    "# Setting up arguments (can also get from argparse)\n",
    "args = {}\n",
    "args['data_loc'] = '/dfs/scratch1/jdunnmon/data/memex-data/tsvs/output_all_slicetest'\n",
    "# Getting raw_content column\n",
    "print('Getting correct column...')\n",
    "files = os.listdir(args['data_loc'])\n",
    "df = pd.read_csv(os.path.join(args['data_loc'],files[0]),sep='\\t',nrows=10)\n",
    "col = df.columns.get_loc(\"memex_raw_content\")\n",
    "\n",
    "# Getting path and setting up data\n",
    "path = args['data_loc']\n",
    "file_list = os.listdir(path)\n",
    "path_list = [os.path.join(path, file) for file in file_list]\n",
    "file_data = [(path, term, col) for path in path_list if path.endswith('tsv')]\n",
    "\n",
    "out_dir = path + '/parsed/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "print('Processing in parallel')\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"parse_html\")\n",
    "sc.addPyFile('/dfs/scratch1/jdunnmon/repos/extractors/src/utils/dataset_utils.py')\n",
    "from dataset_utils import parse_html\n",
    "print('Distributing data...')\n",
    "distData = sc.parallelize(file_data)\n",
    "print('Parsing distributed data...')\n",
    "distData.foreach(parse_html)\n",
    "sc.stop()\n",
    "#threads = 30\n",
    "#pool = Pool(threads) \n",
    "#results = pool.map(parse_html, file_data)\n",
    "#pool.close()\n",
    "#pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "pth='/lfs/local/0/jdunnmon/data/memex-data/escorts/2015/05/08/0000.jsonl.gz'\n",
    "with gzip.GzipFile(pth, 'r') as in_file:\n",
    "        \n",
    "        # Getting file objects\n",
    "        json_reader = in_file.read()\n",
    "        \n",
    "        for ii, chunk in enumerate(json_reader.splitlines()):\n",
    "            data = json.loads(chunk)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['type', 'crawl_data', 'url', 'timestamp', 'extractions', 'raw_content', 'extracted_metadata', 'version', 'extracted_text', 'content_type', 'team', 'doc_id', 'crawler'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_dict = {'id':'doc_id', 'uuid':'', 'memex_id':'doc_id', 'memex_doc_type':'type', 'memex_raw_content':'raw_content', 'memex_url':'url', 'url':'', 'extractions':'extractions'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'uuid', 'memex_id', 'memex_doc_type', 'memex_raw_content', 'memex_url', 'url', 'extractions'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on line: 293\n",
      "Error on line: 2759\n",
      "Error on line: 4247\n",
      "Error on line: 6240\n",
      "Error on line: 8249\n",
      "Error on line: 8801\n",
      "Error on line: 10022\n",
      "Error on line: 10517\n",
      "Error on line: 10518\n",
      "Error on line: 10954\n",
      "Error on line: 11458\n",
      "Error on line: 11705\n",
      "Error on line: 13726\n",
      "Error on line: 14000\n",
      "Error on line: 14444\n",
      "Error on line: 15012\n",
      "Error on line: 15311\n",
      "Error on line: 17569\n",
      "Error on line: 18056\n",
      "Error on line: 18318\n",
      "Error on line: 18325\n",
      "Error on line: 18366\n",
      "Error on line: 18961\n",
      "Error on line: 20161\n",
      "Error on line: 20485\n",
      "Error on line: 20843\n",
      "Error on line: 21346\n",
      "Error on line: 21535\n",
      "Error on line: 22567\n",
      "Error on line: 22639\n",
      "Error on line: 22641\n",
      "Error on line: 24061\n",
      "Error on line: 25125\n",
      "Error on line: 25774\n",
      "Error on line: 25798\n",
      "Error on line: 29836\n",
      "Error on line: 31447\n",
      "Error on line: 31460\n",
      "Error on line: 31594\n",
      "Error on line: 31840\n",
      "Error on line: 32446\n",
      "Error on line: 32659\n",
      "Error on line: 35157\n",
      "Error on line: 36897\n",
      "Error on line: 38125\n",
      "Error on line: 38569\n",
      "Error on line: 39943\n",
      "Error on line: 40149\n",
      "Error on line: 41859\n",
      "Error on line: 42414\n",
      "Error on line: 42987\n",
      "Error on line: 43100\n",
      "Error on line: 44913\n",
      "Error on line: 46473\n",
      "Error on line: 46836\n",
      "Error on line: 46930\n",
      "Error on line: 47049\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "sys.path.append('../utils')\n",
    "from dataset_utils import parse_html_from_jsonl_gz\n",
    "\n",
    "in_loc = '/lfs/local/0/jdunnmon/data/memex-data/escorts/2015/05/08/0000.jsonl.gz'\n",
    "in_parts = in_loc.split('/')\n",
    "in_parts[in_parts.index('escorts')] = 'escorts_preproc'\n",
    "in_parts[-1] = in_parts[-1].split('.')[0]+'.tsv'\n",
    "out_loc = '/'.join(in_parts)\n",
    "term = r'([Ll]ocation:[\\w\\W]{1,200}</.{0,20}>|\\W[cC]ity:[\\w\\W]{1,200}</.{0,20}>|\\d\\dyo\\W|\\d\\d.{0,10}\\Wyo\\W|\\d\\d.{0,10}\\Wold\\W|\\d\\d.{0,10}\\Wyoung\\W|\\Wage\\W.{0,10}\\d\\d)'\n",
    "field_dict = {'id':'doc_id', 'uuid':'', 'memex_id':'doc_id', 'memex_doc_type':'type', 'memex_raw_content':'raw_content', 'memex_url':'url', 'url':'', 'extractions':'extractions'}\n",
    "content_field = 'memex_raw_content'\n",
    "file_data = (in_loc, out_loc, term, field_dict, content_field)\n",
    "parse_html_from_jsonl_gz(file_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SANDBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matcher(object):\n",
    "    \"\"\"\n",
    "    Applies a function f : c -> {True,False} to a generator of candidates,\n",
    "    returning only candidates _c_ s.t. _f(c) == True_,\n",
    "    where f can be compositionally defined.\n",
    "    \"\"\"\n",
    "    def __init__(self, *children, **opts):\n",
    "        self.children           = children\n",
    "        self.opts               = opts\n",
    "        self.longest_match_only = self.opts.get('longest_match_only', True)\n",
    "        self.init()\n",
    "        self._check_opts()\n",
    "\n",
    "    def init(self):\n",
    "        pass\n",
    "\n",
    "    def _check_opts(self):\n",
    "        \"\"\"\n",
    "        Checks for unsupported opts, throws error if found\n",
    "        NOTE: Must be called _after_ init()\n",
    "        \"\"\"\n",
    "        for opt in self.opts.keys():\n",
    "            if not opt in self.__dict__:\n",
    "                raise Exception(\"Unsupported option: %s\" % opt)\n",
    "\n",
    "    def _f(self, c):\n",
    "        \"\"\"The internal (non-composed) version of filter function f\"\"\"\n",
    "        return True\n",
    "\n",
    "    def f(self, c):\n",
    "        \"\"\"\n",
    "        The recursively composed version of filter function f\n",
    "        By default, returns logical **conjunction** of operator and single child operator\n",
    "        \"\"\"\n",
    "        if len(self.children) == 0:\n",
    "            return self._f(c)\n",
    "        elif len(self.children) == 1:\n",
    "            return self._f(c) and self.children[0].f(c)\n",
    "        else:\n",
    "            raise Exception(\"%s does not support more than one child Matcher\" % self.__name__)\n",
    "\n",
    "    def _is_subspan(self, c, span):\n",
    "        \"\"\"Tests if candidate c is subspan of span, where span is defined specific to candidate type\"\"\"\n",
    "        return False\n",
    "\n",
    "    def _get_span(self, c):\n",
    "        \"\"\"Gets a tuple that identifies a span for the specific candidate class that c belongs to\"\"\"\n",
    "        return c\n",
    "\n",
    "    def apply(self, candidates):\n",
    "        \"\"\"\n",
    "        Apply the Matcher to a **generator** of candidates\n",
    "        Optionally only takes the longest match (NOTE: assumes this is the *first* match)\n",
    "        \"\"\"\n",
    "        seen_spans = set()\n",
    "        for c in candidates:\n",
    "            if self.f(c) and (not self.longest_match_only or not any([self._is_subspan(c, s) for s in seen_spans])):\n",
    "                if self.longest_match_only:\n",
    "                    seen_spans.add(self._get_span(c))\n",
    "                yield c\n",
    "\n",
    "\n",
    "WORDS = 'words'\n",
    "\n",
    "class NgramMatcher(Matcher):\n",
    "    \"\"\"Matcher base class for Ngram objects\"\"\"\n",
    "    def _is_subspan(self, c, span):\n",
    "        \"\"\"Tests if candidate c is subspan of span, where span is defined specific to candidate type\"\"\"\n",
    "        return c.char_start >= span[0] and c.char_end <= span[1]\n",
    "\n",
    "    def _get_span(self, c):\n",
    "        \"\"\"Gets a tuple that identifies a span for the specific candidate class that c belongs to\"\"\"\n",
    "        return (c.char_start, c.char_end)\n",
    "\n",
    "class Union(NgramMatcher):\n",
    "    \"\"\"Takes the union of candidate sets returned by child operators\"\"\"\n",
    "    def f(self, c):\n",
    "       for child in self.children:\n",
    "           if child.f(c) > 0:\n",
    "               return True\n",
    "       return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
