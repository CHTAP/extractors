{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "import sys\n",
    "sys.path.append('../utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Environment and Spark SQL\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Snorkel MEMEX Preprocessing\") \\\n",
    "    .config(\"spark.cores.max\", \"96\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 952 ms, sys: 152 ms, total: 1.1 s\n",
      "Wall time: 24min 37s\n"
     ]
    }
   ],
   "source": [
    "# Defining parent data path\n",
    "from dataset_utils import retrieve_all_files\n",
    "#pth='/lfs/local/0/jdunnmon/data/memex-data/escorts/2015/05/08' #/0000.jsonl.gz\n",
    "root_pth = '/lfs/local/0/jdunnmon/data/memex-data'\n",
    "json_path = 'escorts/2015'\n",
    "pth = os.path.join(root_pth,json_path)\n",
    "pth_lst = retrieve_all_files(pth)\n",
    "%time test_df = spark.read.json(pth_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of attributes to extract\n",
    "attr_list = ['doc_id','type', 'raw_content','url',\\\n",
    "             'extractions.phonenumber.results',\n",
    "             'extractions.age.results',\n",
    "             'extractions.rate.results',\n",
    "             #'extractions.location.results',\n",
    "             'extractions.ethnicity.results',\n",
    "             'extractions.email.results',\n",
    "             'extractions.incall.results'\n",
    "            ]\n",
    "\n",
    "cols = ['doc_id','type', 'raw_content','url','extracted_phone','extracted_age',\n",
    "        'extracted_rate','extracted_ethnicity',\\\n",
    "        'extracted_email','extracted_incall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping samples with no relevant data\n",
    "test_df = test_df.na.drop(subset=['doc_id','raw_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47996610"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting examples\n",
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "sys.path.append('../utils')\n",
    "spark.sparkContext.addPyFile('../utils/dataset_utils.py')\n",
    "from dataset_utils import get_posting_html_fast, parse_url\n",
    "from pyspark.sql.functions import udf\n",
    "from functools import partial\n",
    "\n",
    "# Transforming raw content column\n",
    "term = r'([Ll]ocation:[\\w\\W]{1,200}</.{0,20}>|\\W[cC]ity:[\\w\\W]{1,200}</.{0,20}>|\\d\\dyo\\W|\\d\\d.{0,10}\\Wyo\\W|\\d\\d.{0,10}\\Wold\\W|\\d\\d.{0,10}\\Wyoung\\W|\\Wage\\W.{0,10}\\d\\d)'\n",
    "get_posting_html_fast_udf = udf(partial(get_posting_html_fast, search_term=term), StringType())\n",
    "parse_url_udf = udf(parse_url, StringType())\n",
    "\n",
    "# Testing on single row\n",
    "#test_row = test_df.where(test_df.doc_id == 'F1729609E6729B799FB4CC9B5EA5EE0743D8DE10741622DDA4D768443DB64242')\n",
    "#test_row = test_row.withColumn(\"raw_content_parsed\", get_posting_html_fast_udf(test_row.raw_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining query over entire df \n",
    "test_df = test_df.withColumn(\"raw_content_parsed\", get_posting_html_fast_udf(test_df.raw_content))\n",
    "test_df = test_df.withColumn(\"url_parsed\", parse_url_udf(test_df.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 60 ms, total: 60 ms\n",
      "Wall time: 72.4 ms\n",
      "+--------------------+-------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+---------------+----------------+--------------------+--------------------+\n",
      "|              doc_id|   type|         raw_content|                 url|     extracted_phone|extracted_age|      extracted_rate| extracted_ethnicity|extracted_email|extracted_incall|  raw_content_parsed|          url_parsed|\n",
      "+--------------------+-------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+---------------+----------------+--------------------+--------------------+\n",
      "|DBBF9D41F266C969A...|escorts|\n",
      "<!DOCTYPE html P...|http://www.erotic...|[286199710279, 31...|         [23]|           [20-0:15]|[white, latin, la...|           null|            null|Title 619-971-027...|Url Eroticmugshot...|\n",
      "|953E27336F4EF94D2...|escorts|\n",
      "<!DOCTYPE html P...|http://www.erotic...|[3126008628, 8329...|         [26]|[130-1:00, 50-1:0...|[white, latin, la...|           null|            null|Title 678-378-783...|Url Eroticmugshot...|\n",
      "|4F325D0CAAF15FBB9...|escorts|\n",
      "<!DOCTYPE html P...|http://www.erotic...|[229175899361, 31...|         [26]|           [20-0:15]|[white, latin, la...|           null|            null|Title 917-589-936...|Url Eroticmugshot...|\n",
      "|70048F4FAAC62076A...|escorts|\n",
      "<!DOCTYPE html ...|http://416-723-66...|[26201541672366, ...|         [22]|                null|             [latin]|           null|            null|Title 416-723-666...|Url 416 723 6666 ...|\n",
      "|26BB32CEAFF5F1EE9...|escorts|\n",
      "<!DOCTYPE html P...|http://escortads....|        [7742126061]|     [27, 27]|                null|                null|           null|            null|Title 774-212-606...|Url Escortads Xxx...|\n",
      "|0079D9F5AAED9D06E...|escorts|<!DOCTYPE html>\n",
      "<...|http://liveescort...|[650984061740, 51...|         null|                null|                null|           null|            null|Title Live Escort...|Url Liveescortrev...|\n",
      "|8629243A0E4432973...|escorts|\n",
      "<!DOCTYPE html P...|http://www.erotic...|[3126008628, 2660...|         [22]|           [20-0:15]|[white, latin, la...|           null|            null|Title 602-228-419...|Url Eroticmugshot...|\n",
      "|43EC35C1B58926B1F...|escorts|\n",
      "<!DOCTYPE html P...|http://www.erotic...|[229175899361, 31...|         [25]|           [20-0:15]|[white, latin, la...|           null|            null|Title 917-589-936...|Url Eroticmugshot...|\n",
      "|8FC1B98E036C562D3...|escorts|<!DOCTYPE html>\n",
      "<...|http://liveescort...|[152015527, 40588...|         null|                null|                null|           null|            null|Title Live Escort...|Url Liveescortrev...|\n",
      "|78F5FBAF8540292F7...|escorts|<!DOCTYPE html>\n",
      "<...|http://liveescort...|[9546734691, 9546...|         null|                null|                null|           null|            null|Title Live Escort...|Url Liveescortrev...|\n",
      "|A5FC2DB0960C788AE...|escorts|\n",
      "<!DOCTYPE html P...|http://www.erotic...|[229175899361, 31...|         [21]|  [50-1:00, 20-0:15]|[white, latin, la...|           null|            null|Title 917-589-936...|Url Eroticmugshot...|\n",
      "|ADEA65857117F1B33...|escorts|<!DOCTYPE html>\n",
      "<...|http://liveescort...|[7183043545, 8628...|         null|                null|                null|           null|            null|Title Live Escort...|Url Liveescortrev...|\n",
      "|7B36DE208D32A8B89...|escorts|<!DOCTYPE html>\n",
      "<...|http://liveescort...|[714686809025, 94...|         null|                null|                null|           null|            null|Title Live Escort...|Url Liveescortrev...|\n",
      "|757AC7329FB807226...|escorts|\n",
      "<!DOCTYPE html P...|http://escortads....|        [7193109971]|     [24, 24]|                null|                null|           null|            null|Title 719-310-997...|Url Escortads Xxx...|\n",
      "|B7B74E42E31C25BF2...|escorts|\n",
      "<!DOCTYPE html ...|http://210-315-81...|        [2103158148]|         [20]|                null|                null|           null|            null|Title 210-315-814...|Url 210 315 8148 ...|\n",
      "|84753E9B2F45DA6BC...|escorts|<!DOCTYPE html>\n",
      "<...|http://liveescort...|[212015812, 70435...|         null|                null|                null|           null|            null|Title Live Escort...|Url Liveescortrev...|\n",
      "|026C47AD8FA5D0359...|escorts|\n",
      "<!DOCTYPE html ...|http://929-281-50...|[04201592928150, ...|         [20]|                null|             [asian]|           null|            null|Title 929-281-503...|Url 929 281 5033 ...|\n",
      "|E4288A72F907E4116...|escorts|\n",
      "<!DOCTYPE html P...|http://www.erotic...|[246154504856, 31...|         [22]|           [20-0:15]|[white, latin, la...|           null|            null|Title 615-450-485...|Url Eroticmugshot...|\n",
      "|28E9C0E0A22A5CE9B...|escorts|\n",
      "<!DOCTYPE html P...|http://www.erotic...|[286199710279, 31...|         [21]|           [20-0:15]|[white, latin, la...|           null|            null|Title 619-971-027...|Url Eroticmugshot...|\n",
      "|BCED8D51FD2F1432D...|escorts|\n",
      "<!DOCTYPE html P...|http://www.erotic...|[3126008628, 2278...|         [23]| [140-1:00, 20-0:15]|[white, latin, la...|           null|            null|Title 783-787-831...|Url Eroticmugshot...|\n",
      "+--------------------+-------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+---------------+----------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Executing and showing entire df\n",
    "%time test_df = test_df.select(attr_list+['raw_content_parsed', 'url_parsed'])\n",
    "cols = cols +['raw_content_parsed', 'url_parsed']\n",
    "test_df = test_df.toDF(*cols)\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    if 'extracted' in col:\n",
    "        test_df = test_df.withColumn(col, concat_ws(',',col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc_id',\n",
       " 'type',\n",
       " 'raw_content',\n",
       " 'url',\n",
       " 'extracted_phone',\n",
       " 'extracted_age',\n",
       " 'extracted_rate',\n",
       " 'extracted_ethnicity',\n",
       " 'extracted_email',\n",
       " 'extracted_incall',\n",
       " 'raw_content_parsed',\n",
       " 'url_parsed']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- doc_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- raw_content: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- extracted_phone: string (nullable = false)\n",
      " |-- extracted_age: string (nullable = false)\n",
      " |-- extracted_rate: string (nullable = false)\n",
      " |-- extracted_ethnicity: string (nullable = false)\n",
      " |-- extracted_email: string (nullable = false)\n",
      " |-- extracted_incall: string (nullable = false)\n",
      " |-- raw_content_parsed: string (nullable = true)\n",
      " |-- url_parsed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing single example\n",
    "#%time test_df_row = test_df.where(test_df.doc_id == 'F1729609E6729B799FB4CC9B5EA5EE0743D8DE10741622DDA4D768443DB64242').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o155.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job 5 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:837)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:835)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:835)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1841)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1754)\n\tat org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1931)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1360)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1930)\n\tat org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:573)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:194)\n\t... 31 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch1/jdunnmon/repos/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping)\u001b[0m\n\u001b[1;32m    883\u001b[0m                        \u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m                        charToEscapeQuoteEscaping=charToEscapeQuoteEscaping)\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch1/jdunnmon/repos/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch1/jdunnmon/repos/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch1/jdunnmon/repos/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o155.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job 5 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:837)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:835)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:835)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1841)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1754)\n\tat org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1931)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1360)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1930)\n\tat org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:573)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:194)\n\t... 31 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 33474)\n",
      "Traceback (most recent call last):\n",
      "  File \"/lfs/local/0/jdunnmon/repos/anaconda3/envs/snorkel/lib/python3.6/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/lfs/local/0/jdunnmon/repos/anaconda3/envs/snorkel/lib/python3.6/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/lfs/local/0/jdunnmon/repos/anaconda3/envs/snorkel/lib/python3.6/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/lfs/local/0/jdunnmon/repos/anaconda3/envs/snorkel/lib/python3.6/socketserver.py\", line 696, in __init__\n",
      "    self.handle()\n",
      "  File \"/dfs/scratch1/jdunnmon/repos/spark-2.3.1-bin-hadoop2.7/python/pyspark/accumulators.py\", line 235, in handle\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/dfs/scratch1/jdunnmon/repos/spark-2.3.1-bin-hadoop2.7/python/pyspark/serializers.py\", line 685, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cols_to_write = ['doc_id','type', 'url','url_parsed','raw_content_parsed','extracted_phone','extracted_age',\n",
    "        'extracted_rate','extracted_ethnicity',\\\n",
    "        'extracted_email','extracted_incall']\n",
    "write_path = os.path.join(root_pth,'escorts_preproc/spark_test/2015')\n",
    "%time test_df.select(cols_to_write).write.option(\"maxRecordsPerFile\", 1000000).csv(write_path,header = 'true',sep='\\t',mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.write.csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing schema\n",
    "#test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import Context, Candidate\n",
    "from snorkel.contrib.models.text import RawText\n",
    "\n",
    "# Make sure DB is cleared\n",
    "session.query(Context).delete()\n",
    "session.query(Candidate).delete()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running phone extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import set_preprocessor\n",
    "from snorkel.parser import CorpusParser\n",
    "from parser_utils import SimpleTokenizer\n",
    "from snorkel.models import Document, Sentence\n",
    "from snorkel.parser import HTMLDocPreprocessor as pp\n",
    "\n",
    "dbname = \"\".join(in_loc.split('/'))\n",
    "postgres_location = 'postgresql:///'\n",
    "os.system(f'../utils/kill_db.sh {dbname}')\n",
    "os.system(f'createdb {dbname}')\n",
    "os.environ['SNORKELDB'] = os.path.join(postgres_location, db_name)\n",
    "\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Now we create the candidates with a simple loop\n",
    "candidate_docs = test_df.select(attr_list+['raw_content_parsed']).orderBy(\"doc_id\").distinct()\n",
    "\n",
    "# export SNORKELDB=postgresql:///${DBNAME}; cd /dfs/scratch1/jdunnmon/repos/extractors/src/shell; \n",
    "#sh ../utils/kill_db.sh ${DBNAME}; source activate snorkel; python create_dbs.py -f ${FOLDERS[$iter]}; bash -l\n",
    "\n",
    "# Using Spark to add candidates\n",
    "def spark_preprocessor(cd, verbose=False, max_doc_length=2000):\n",
    "    # Fields to add\n",
    "    text_fields = ['raw_content_parsed','url']\n",
    "    doc_text = cd['raw_content_parsed']+\" \"+cd['url_parsed']\n",
    "    doc_text = doc_text.strip()\n",
    "    doc_name = cd['doc_id']\n",
    "    extractions = cd['extractions']\n",
    "\n",
    "    # Short documents are usually parsing errors...\n",
    "    if len(doc_text) < 10:\n",
    "        if verbose:\n",
    "            print('Short Doc!')\n",
    "        continue\n",
    "\n",
    "    if max_doc_length and len(doc_text) > max_doc_length:\n",
    "        if verbose:\n",
    "            print('Long document')\n",
    "        continue\n",
    "\n",
    "    stable_id = pp.get_stable_id(doc_name)\n",
    "\n",
    "    # Yielding results, adding useful info to metadata\n",
    "    doc = Document(\n",
    "        name=doc_name, stable_id=stable_id,\n",
    "        meta={'extractions':extractions,\n",
    "              'url':url}\n",
    "    )\n",
    "    yield doc, doc_text\n",
    "    \n",
    "# Setting parser and applying corpus preprocessor\n",
    "parser=SimpleTokenizer(delim='<|>')\n",
    "corpus_parser = CorpusParser(parser=parser)\n",
    "corpus_parser.apply(list(doc_preprocessor), parallelism=parallelism, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import create_candidate_class\n",
    "extraction_type = 'phone'\n",
    "\n",
    "# Creating candidate class\n",
    "candidate_class, candidate_class_name = create_candidate_class(extraction_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_bodies = candidate_labeled_tweets \\\n",
    "    .select(\"tweet_id\", \"tweet_body\") \\\n",
    "    .orderBy(\"tweet_id\") \\\n",
    "    .distinct()\n",
    "\n",
    "# Generate and store the tweet candidates to be classified\n",
    "# Note: We split the tweets in two sets: one for which the crowd \n",
    "# labels are not available to Snorkel (test, 10%) and one for which we assume\n",
    "# crowd labels are obtained (to be used for training, 90%)\n",
    "total_tweets = tweet_bodies.count()\n",
    "test_split = total_tweets*0.1\n",
    "for i, t in enumerate(tweet_bodies.collect()):\n",
    "    split = 1 if i <= test_split else 0\n",
    "    raw_text = RawText(stable_id=t.tweet_id, name=t.tweet_id, text=t.tweet_body)\n",
    "    tweet = Tweet(tweet=raw_text, split=split)\n",
    "    session.add(tweet)\n",
    "session.commit()\n",
    "\n",
    "\n",
    "# Getting gold label for each doc\n",
    "print(\"Running regex extractor...\")\n",
    "doc_extractions = {}\n",
    "for ii, _ in enumerate(eval_cands):\n",
    "    doc_extractions[doc.name] = {}\n",
    "    if ii % 1000 == 0:\n",
    "        print(f'Extracting regexes from doc {ii} out of {len(eval_cands)}')\n",
    "    doc_extractions[doc.name]['phone'] = regex_matcher(doc, mode='phonenumbers')\n",
    "\n",
    "# Setting filename\n",
    "out_filename = \"phone_extraction_\"+postgres_db_name+\".jsonl\"\n",
    "out_folder = os.path.join(config['output_dir'], 'phone')\n",
    "out_path = os.path.join(out_folder, out_filename)\n",
    "\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "                          \n",
    "# Saving file to jsonl in extractions format\n",
    "print(f\"Saving output to {out_path}\")\n",
    "with open(out_path, 'w') as outfile:\n",
    "    for k,v in doc_extractions.items():\n",
    "        v['id'] = k\n",
    "        v['phone'] = list(v['phone'])\n",
    "        print(json.dumps(v), file=outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SANDBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"export SNORKELDB=postgresql://jdunnmon:123@localhost:5432/${DBNAME}; export CUDA_VISIBLE_DEVICES=0; \n",
    "#cd /dfs/scratch1/jdunnmon/repos/extractors/src/shell; source activate snorkel; \n",
    "#echo 'Evaluating phone extractor for node ${NODES[NODE_INDEX]} with database ${DBNAME} ...';\n",
    "#python evaluate_phone_extractor.py -f ${FOLDERS[$iter]}; bash -l\" &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing if PySpark works\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting correct column...\n",
      "Processing in parallel\n",
      "Distributing data...\n",
      "Parsing distributed data...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys, os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "#from multiprocessing import Pool\n",
    "\n",
    "# Getting config\n",
    "with open('/dfs/scratch1/jdunnmon/data/memex-data/config/config_spark.json') as fl:\n",
    "    config = json.load(fl)\n",
    "\n",
    "# Adding path for utils\n",
    "sys.path.append('/dfs/scratch1/jdunnmon/repos/extractors/src/utils')\n",
    "\n",
    "# Setting random seed\n",
    "seed = config['seed']\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# If memex_raw_content is a content_field, uses term as a regex in raw data in addition to getting title and body\n",
    "#term = r'\\b[Ll]ocation:|\\b[cC]ity:'\n",
    "#term = r'([Ll]ocation:.{0,100}|[cC]ity:.{0,100}|\\d\\dyo\\W|\\d\\d.{0,10}\\Wyo\\W|\\d\\d.{0,10}\\Wold\\W|\\d\\d.{0,10}\\Wyoung\\W|\\Wage\\W.{0,10}\\d\\d)'\n",
    "term = r'([Ll]ocation:[\\w\\W]{1,200}</.{0,20}>|\\W[cC]ity:[\\w\\W]{1,200}</.{0,20}>|\\d\\dyo\\W|\\d\\d.{0,10}\\Wyo\\W|\\d\\d.{0,10}\\Wold\\W|\\d\\d.{0,10}\\Wyoung\\W|\\Wage\\W.{0,10}\\d\\d)'\n",
    "\n",
    "# Setting up arguments (can also get from argparse)\n",
    "args = {}\n",
    "args['data_loc'] = '/dfs/scratch1/jdunnmon/data/memex-data/tsvs/output_all_slicetest'\n",
    "# Getting raw_content column\n",
    "print('Getting correct column...')\n",
    "files = os.listdir(args['data_loc'])\n",
    "df = pd.read_csv(os.path.join(args['data_loc'],files[0]),sep='\\t',nrows=10)\n",
    "col = df.columns.get_loc(\"memex_raw_content\")\n",
    "\n",
    "# Getting path and setting up data\n",
    "path = args['data_loc']\n",
    "file_list = os.listdir(path)\n",
    "path_list = [os.path.join(path, file) for file in file_list]\n",
    "file_data = [(path, term, col) for path in path_list if path.endswith('tsv')]\n",
    "\n",
    "out_dir = path + '/parsed/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "print('Processing in parallel')\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"parse_html\")\n",
    "sc.addPyFile('/dfs/scratch1/jdunnmon/repos/extractors/src/utils/dataset_utils.py')\n",
    "from dataset_utils import parse_html\n",
    "print('Distributing data...')\n",
    "distData = sc.parallelize(file_data)\n",
    "print('Parsing distributed data...')\n",
    "distData.foreach(parse_html)\n",
    "sc.stop()\n",
    "#threads = 30\n",
    "#pool = Pool(threads) \n",
    "#results = pool.map(parse_html, file_data)\n",
    "#pool.close()\n",
    "#pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "pth='/lfs/local/0/jdunnmon/data/memex-data/escorts/2015/05/08/0000.jsonl.gz'\n",
    "with gzip.GzipFile(pth, 'r') as in_file:\n",
    "        \n",
    "        # Getting file objects\n",
    "        json_reader = in_file.read()\n",
    "        \n",
    "        for ii, chunk in enumerate(json_reader.splitlines()):\n",
    "            data = json.loads(chunk)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['type', 'crawl_data', 'url', 'timestamp', 'extractions', 'raw_content', 'extracted_metadata', 'version', 'extracted_text', 'content_type', 'team', 'doc_id', 'crawler'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_dict = {'id':'doc_id', 'uuid':'', 'memex_id':'doc_id', 'memex_doc_type':'type', 'memex_raw_content':'raw_content', 'memex_url':'url', 'url':'', 'extractions':'extractions'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'uuid', 'memex_id', 'memex_doc_type', 'memex_raw_content', 'memex_url', 'url', 'extractions'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on line: 293\n",
      "Error on line: 2759\n",
      "Error on line: 4247\n",
      "Error on line: 6240\n",
      "Error on line: 8249\n",
      "Error on line: 8801\n",
      "Error on line: 10022\n",
      "Error on line: 10517\n",
      "Error on line: 10518\n",
      "Error on line: 10954\n",
      "Error on line: 11458\n",
      "Error on line: 11705\n",
      "Error on line: 13726\n",
      "Error on line: 14000\n",
      "Error on line: 14444\n",
      "Error on line: 15012\n",
      "Error on line: 15311\n",
      "Error on line: 17569\n",
      "Error on line: 18056\n",
      "Error on line: 18318\n",
      "Error on line: 18325\n",
      "Error on line: 18366\n",
      "Error on line: 18961\n",
      "Error on line: 20161\n",
      "Error on line: 20485\n",
      "Error on line: 20843\n",
      "Error on line: 21346\n",
      "Error on line: 21535\n",
      "Error on line: 22567\n",
      "Error on line: 22639\n",
      "Error on line: 22641\n",
      "Error on line: 24061\n",
      "Error on line: 25125\n",
      "Error on line: 25774\n",
      "Error on line: 25798\n",
      "Error on line: 29836\n",
      "Error on line: 31447\n",
      "Error on line: 31460\n",
      "Error on line: 31594\n",
      "Error on line: 31840\n",
      "Error on line: 32446\n",
      "Error on line: 32659\n",
      "Error on line: 35157\n",
      "Error on line: 36897\n",
      "Error on line: 38125\n",
      "Error on line: 38569\n",
      "Error on line: 39943\n",
      "Error on line: 40149\n",
      "Error on line: 41859\n",
      "Error on line: 42414\n",
      "Error on line: 42987\n",
      "Error on line: 43100\n",
      "Error on line: 44913\n",
      "Error on line: 46473\n",
      "Error on line: 46836\n",
      "Error on line: 46930\n",
      "Error on line: 47049\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "sys.path.append('../utils')\n",
    "from dataset_utils import parse_html_from_jsonl_gz\n",
    "\n",
    "in_loc = '/lfs/local/0/jdunnmon/data/memex-data/escorts/2015/05/08/0000.jsonl.gz'\n",
    "in_parts = in_loc.split('/')\n",
    "in_parts[in_parts.index('escorts')] = 'escorts_preproc'\n",
    "in_parts[-1] = in_parts[-1].split('.')[0]+'.tsv'\n",
    "out_loc = '/'.join(in_parts)\n",
    "term = r'([Ll]ocation:[\\w\\W]{1,200}</.{0,20}>|\\W[cC]ity:[\\w\\W]{1,200}</.{0,20}>|\\d\\dyo\\W|\\d\\d.{0,10}\\Wyo\\W|\\d\\d.{0,10}\\Wold\\W|\\d\\d.{0,10}\\Wyoung\\W|\\Wage\\W.{0,10}\\d\\d)'\n",
    "field_dict = {'id':'doc_id', 'uuid':'', 'memex_id':'doc_id', 'memex_doc_type':'type', 'memex_raw_content':'raw_content', 'memex_url':'url', 'url':'', 'extractions':'extractions'}\n",
    "content_field = 'memex_raw_content'\n",
    "file_data = (in_loc, out_loc, term, field_dict, content_field)\n",
    "parse_html_from_jsonl_gz(file_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SANDBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matcher(object):\n",
    "    \"\"\"\n",
    "    Applies a function f : c -> {True,False} to a generator of candidates,\n",
    "    returning only candidates _c_ s.t. _f(c) == True_,\n",
    "    where f can be compositionally defined.\n",
    "    \"\"\"\n",
    "    def __init__(self, *children, **opts):\n",
    "        self.children           = children\n",
    "        self.opts               = opts\n",
    "        self.longest_match_only = self.opts.get('longest_match_only', True)\n",
    "        self.init()\n",
    "        self._check_opts()\n",
    "\n",
    "    def init(self):\n",
    "        pass\n",
    "\n",
    "    def _check_opts(self):\n",
    "        \"\"\"\n",
    "        Checks for unsupported opts, throws error if found\n",
    "        NOTE: Must be called _after_ init()\n",
    "        \"\"\"\n",
    "        for opt in self.opts.keys():\n",
    "            if not opt in self.__dict__:\n",
    "                raise Exception(\"Unsupported option: %s\" % opt)\n",
    "\n",
    "    def _f(self, c):\n",
    "        \"\"\"The internal (non-composed) version of filter function f\"\"\"\n",
    "        return True\n",
    "\n",
    "    def f(self, c):\n",
    "        \"\"\"\n",
    "        The recursively composed version of filter function f\n",
    "        By default, returns logical **conjunction** of operator and single child operator\n",
    "        \"\"\"\n",
    "        if len(self.children) == 0:\n",
    "            return self._f(c)\n",
    "        elif len(self.children) == 1:\n",
    "            return self._f(c) and self.children[0].f(c)\n",
    "        else:\n",
    "            raise Exception(\"%s does not support more than one child Matcher\" % self.__name__)\n",
    "\n",
    "    def _is_subspan(self, c, span):\n",
    "        \"\"\"Tests if candidate c is subspan of span, where span is defined specific to candidate type\"\"\"\n",
    "        return False\n",
    "\n",
    "    def _get_span(self, c):\n",
    "        \"\"\"Gets a tuple that identifies a span for the specific candidate class that c belongs to\"\"\"\n",
    "        return c\n",
    "\n",
    "    def apply(self, candidates):\n",
    "        \"\"\"\n",
    "        Apply the Matcher to a **generator** of candidates\n",
    "        Optionally only takes the longest match (NOTE: assumes this is the *first* match)\n",
    "        \"\"\"\n",
    "        seen_spans = set()\n",
    "        for c in candidates:\n",
    "            if self.f(c) and (not self.longest_match_only or not any([self._is_subspan(c, s) for s in seen_spans])):\n",
    "                if self.longest_match_only:\n",
    "                    seen_spans.add(self._get_span(c))\n",
    "                yield c\n",
    "\n",
    "\n",
    "WORDS = 'words'\n",
    "\n",
    "class NgramMatcher(Matcher):\n",
    "    \"\"\"Matcher base class for Ngram objects\"\"\"\n",
    "    def _is_subspan(self, c, span):\n",
    "        \"\"\"Tests if candidate c is subspan of span, where span is defined specific to candidate type\"\"\"\n",
    "        return c.char_start >= span[0] and c.char_end <= span[1]\n",
    "\n",
    "    def _get_span(self, c):\n",
    "        \"\"\"Gets a tuple that identifies a span for the specific candidate class that c belongs to\"\"\"\n",
    "        return (c.char_start, c.char_end)\n",
    "\n",
    "class Union(NgramMatcher):\n",
    "    \"\"\"Takes the union of candidate sets returned by child operators\"\"\"\n",
    "    def f(self, c):\n",
    "       for child in self.children:\n",
    "           if child.f(c) > 0:\n",
    "               return True\n",
    "       return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
