{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Parsing Files, Adding Candidates and Labels to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saeideh.shahrokh/projects/memex/test_extractors/temp/.venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://saeideh.shahrokh:saeede7696@localhost:5432/aht_db_ver1\n",
      "[INFO] fonduer.models.meta - Initializing the storage schema\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from os.path import join\n",
    "# Loading config\n",
    "with open(\"run_config_ver2.json\") as fl:\n",
    "    cfg = json.load(fl)\n",
    "cfg_params = cfg['parameters']\n",
    "\n",
    "# Setting snorkel path and output root\n",
    "\n",
    "output_root = join(cfg_params['output_path'],cfg_params['experiment_name'])\n",
    "\n",
    "# Old import grammar\n",
    "os.environ['FONDUERDBNAME'] = cfg['postgres_db_name']\n",
    "os.environ['SNORKELDB'] = join(cfg['postgres_location'],os.environ['FONDUERDBNAME'])\n",
    "PARALLEL = 4 \n",
    "# For loading input files\n",
    "import pandas as pd\n",
    "from fonduer import Meta\n",
    "\n",
    "print(os.environ['SNORKELDB'])\n",
    "\n",
    "# Configure logging for Fonduer\n",
    "logging.basicConfig(stream=sys.stdout, format='[%(levelname)s] %(name)s - %(message)s')\n",
    "log = logging.getLogger('fonduer')\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "session = Meta.init(os.environ['SNORKELDB']).Session()\n",
    "# For running Snorkel\n",
    "#from fonduer import SnorkelSession\n",
    "from fonduer.models import candidate_subclass\n",
    "from fonduer import HTMLPreprocessor, OmniParser\n",
    "#from fonduer import Meta\n",
    "from utils import MEMEXJsonPreprocessor, HTMLListPreprocessor\n",
    "\n",
    "#old snorkel imports\n",
    "#from snorkel.contrib.fonduer import SnorkelSession\n",
    "#from snorkel.contrib.fonduer.models import candidate_subclass\n",
    "#from snorkel.contrib.fonduer import HTMLPreprocessor, OmniParser\n",
    "#from utils import HTMLListPreprocessor, MEMEXJsonPreprocessor\n",
    "\n",
    "#from sqlalchemy import create_engine\n",
    "#snorkeldb = create_engine(os.environ['SNORKELDB'], isolation_level=\"AUTOCOMMIT\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(os.environ['SNORKELDB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading gold_files.csv\n",
    "\n",
    "create a dictionary for gold_locations: keys: \"urls\" , values: \"gold_location\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "### function for cleaning gold labels\n",
    "def cleaning_gold_value(value):\n",
    "    y= value.replace('\"',\"\")  \n",
    "    z =y.replace(\"'\",\"\")\n",
    "    r = z.split(\", \")\n",
    "    p_1 = re.compile(\"^\\s+[\\w]\")\n",
    "    \n",
    "    st_new =[]\n",
    "    for s in r:\n",
    "        #finding whitespaces\n",
    "        if p_1.match(s):\n",
    "        \n",
    "        #counting whitespaces before a word\n",
    "            l = len(s)-len(s.lstrip(' '))\n",
    "        \n",
    "            st_new.append(s[(l):])        \n",
    "        else:\n",
    "            st_new.append(s)    \n",
    "    return st_new\n",
    "\n",
    "\"\"\" create a dictionary, key: url, value: list of all gold_location \"\"\"\n",
    "def load_gold_df (filename,doc_name,ex_target):\n",
    "    gold_df = pd.read_csv(filename,names=['number_1','index','website_name',\\\n",
    "        'add_type',doc_name,'text','info_extracted',ex_target])\n",
    "    return gold_df\n",
    "    \n",
    "df_gold = load_gold_df ('../../data/gold_files.csv',\"url\",\"gold_location\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gold_dict_(gold_df,doc_name,ex_target):\n",
    "    \n",
    "    gold_df = gold_df.dropna(subset=[ex_target])\n",
    "    gold_df[\"new_loc\"] = gold_df[ex_target].apply(cleaning_gold_value)\n",
    "    locs_ = gold_df[\"new_loc\"].values\n",
    "    urls_ = gold_df[doc_name].values\n",
    "    return dict(zip(urls_,locs_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_dict_ = make_gold_dict_(df_gold, \"url\",\"gold_location\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_gold_dict(gold_df,doc_name,ex_target):\n",
    "#     gold_dict={}\n",
    "#     ###\n",
    "# #     gold_df = pd.read_csv(filename,names=['number_1','index','website_name',\\\n",
    "# #         'add_type',doc_name,'text','info_extracted',ex_target])\n",
    "#     gold_df = gold_df.dropna(subset=[ex_target])\n",
    "#     gold_df[\"new_loc\"] = gold_df[ex_target].apply(cleaning_gold_value)\n",
    "#     for ind,row in gold_df.iterrows():\n",
    "        \n",
    "#         gold_dict[row[doc_name]] = row[\"new_loc\"]#( i for i in cleaning_gold_value(df[ex_target].tolist))\n",
    "#     return gold_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading raw files -- Json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/saeideh.shahrokh/projects/memex/test_extractors/ext_3_04_23_18/s3_sync/test-full-content/temp_1.jsonl']\n"
     ]
    }
   ],
   "source": [
    "# #Load html data from json files\n",
    "\n",
    "#Load html data from json files\n",
    "path_unlabeled = \"/Users/saeideh.shahrokh/projects/memex/test_extractors/ext_3_04_23_18/s3_sync/\"#cfg['unlabeled_data_path']\n",
    "fl_unlabeled = 'test-full-content'#cfg['unlabeled_data_file']\n",
    "\n",
    "def retrieve_all_files(dr):\n",
    "    lst = []\n",
    "    for root, directories, filenames in os.walk(dr):\n",
    "        \n",
    "         for filename in filenames: \n",
    "                #print (filename)\n",
    "                lst.append(os.path.join(root,filename))\n",
    "    return lst\n",
    "    \n",
    "#getting all files recursively\n",
    "data_loc = os.path.join(path_unlabeled,fl_unlabeled) \n",
    "path_list = retrieve_all_files(data_loc)\n",
    "\n",
    "print(path_list[1:])\n",
    "# Start snorkel session and creating location subclass\n",
    "\n",
    "Location_Extraction = candidate_subclass('location_extraction',\\\n",
    "                          [\"location\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice, chain\n",
    "from fonduer.utils import ProgressBar #fonduer.snorkel.utils\n",
    "from fonduer.models import GoldLabel, GoldLabelKey, Document\n",
    "\n",
    "class MEMEXJsonLPreprocessor(HTMLListPreprocessor):\n",
    "    \n",
    "    def __init__(self, path, file_list, encoding=\"utf-8\", max_docs=float('inf'), lines_per_entry=6, verbose=False):\n",
    "        self.path = path\n",
    "        self.encoding = encoding\n",
    "        self.max_docs = max_docs\n",
    "        self.file_list = file_list\n",
    "        self.lines_per_entry = lines_per_entry\n",
    "        self.verbose=verbose\n",
    "        \n",
    "    def _get_files(self,path_list):\n",
    "        fpaths = [fl for fl in path_list]\n",
    "        return fpaths\n",
    "    \n",
    "    def _can_read(self, fpath):\n",
    "        return fpath.endswith('jsonl')  \n",
    "    \n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Parses a file or directory of files into a set of Document objects.\n",
    "        \"\"\"\n",
    "        doc_count = 0\n",
    "        for file_name in self._get_files(self.file_list):\n",
    "            if self._can_read(file_name):\n",
    "                for doc, text in self.parse_file(file_name):\n",
    "                    yield doc, text\n",
    "                    #print(doc.name)\n",
    "                    #print(text)\n",
    "                    doc_count += 1\n",
    "                    if self.verbose:\n",
    "                        print(f'Parsed {doc_count} docs...')\n",
    "                    if doc_count >= self.max_docs:\n",
    "                        return\n",
    "                    \n",
    "    def _lines_per_n(self, f, n):\n",
    "        for line in f:\n",
    "            yield ''.join(chain([line], islice(f, n - 1)))\n",
    "        \n",
    "    def _read_content_file(self, fl):\n",
    "        json_lst = []\n",
    "        #with codecs.open(fl, encoding=self.encoding) as f:\n",
    "        with open(fl) as f:\n",
    "            for chunk in self._lines_per_n(f, self.lines_per_entry):\n",
    "                jfile = json.loads(chunk)\n",
    "                json_lst.append(jfile)\n",
    "        json_pd = pd.DataFrame(json_lst)\n",
    "       # json_pd = pd.DataFrame(json_lst).dropna()\n",
    "        return json_pd\n",
    "    \n",
    "    def parse_file(self, file_name):\n",
    "        df = self._read_content_file(file_name)\n",
    "        for index, row in df.iterrows():\n",
    "            name = row.url\n",
    "            stable_id = self.get_stable_id(name)\n",
    "            text = ' '.join(row.raw_content[1:-1].replace('<br>', '').split())\n",
    "            yield Document(name=name, stable_id=stable_id, text=str(text),\n",
    "                               meta={'file_name' : file_name}), str(text)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting parameter for max number of docs to load from labeled/unlabeled\n",
    "#max_docs = cfg['max_docs']\n",
    "max_docs = 100\n",
    "\n",
    "# Setting jsons to load\n",
    "path_list = path_list[1:]\n",
    "# pth_list_test = \n",
    "# Preprocessing documents from path_list\n",
    "doc_preprocessor = MEMEXJsonLPreprocessor(data_loc,\\\n",
    "                                file_list=path_list,encoding='utf-8',lines_per_entry=1, verbose=True,max_docs=max_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fonduer.udf - Clearing existing...\n",
      "[INFO] fonduer.udf - Running UDF...\n",
      "Parsed 1 docs...\n",
      "Parsed 2 docs...\n",
      "Parsed 3 docs...\n",
      "Parsed 4 docs...\n",
      "Parsed 5 docs...\n",
      "Parsed 6 docs...\n",
      "Parsed 7 docs...\n",
      "Parsed 8 docs...\n",
      "Parsed 9 docs...\n",
      "Parsed 10 docs...\n",
      "Parsed 11 docs...\n",
      "Parsed 12 docs...\n",
      "Parsed 13 docs...\n",
      "Parsed 14 docs...\n",
      "Parsed 15 docs...\n",
      "Parsed 16 docs...\n",
      "Parsed 17 docs...\n",
      "Parsed 18 docs...\n",
      "Parsed 19 docs...\n",
      "Parsed 20 docs...\n",
      "Parsed 21 docs...\n",
      "Parsed 22 docs...\n",
      "Parsed 23 docs...\n",
      "Parsed 24 docs...\n",
      "Parsed 25 docs...\n",
      "Parsed 26 docs...\n",
      "Parsed 27 docs...\n",
      "Parsed 28 docs...\n",
      "Parsed 29 docs...\n",
      "Parsed 30 docs...\n",
      "Parsed 31 docs...\n",
      "Parsed 32 docs...\n",
      "Parsed 33 docs...\n",
      "Parsed 34 docs...\n",
      "Parsed 35 docs...\n",
      "Parsed 36 docs...\n",
      "Parsed 37 docs...\n",
      "Parsed 38 docs...\n",
      "Parsed 39 docs...\n",
      "Parsed 40 docs...\n",
      "Parsed 41 docs...\n",
      "Parsed 42 docs...\n",
      "Parsed 43 docs...\n",
      "Parsed 44 docs...\n",
      "Parsed 45 docs...\n",
      "Parsed 46 docs...\n",
      "Parsed 47 docs...\n",
      "Parsed 48 docs...\n",
      "Parsed 49 docs...\n",
      "Parsed 50 docs...\n",
      "Parsed 51 docs...\n",
      "Parsed 52 docs...\n",
      "Parsed 53 docs...\n",
      "Parsed 54 docs...\n",
      "Parsed 55 docs...\n",
      "Parsed 56 docs...\n",
      "Parsed 57 docs...\n",
      "Parsed 58 docs...\n",
      "Parsed 59 docs...\n",
      "Parsed 60 docs...\n",
      "Parsed 61 docs...\n",
      "Parsed 62 docs...\n",
      "Parsed 63 docs...\n",
      "Parsed 64 docs...\n",
      "Parsed 65 docs...\n",
      "Parsed 66 docs...\n",
      "Parsed 67 docs...\n",
      "Parsed 68 docs...\n",
      "Parsed 69 docs...\n",
      "Parsed 70 docs...\n",
      "Parsed 71 docs...\n",
      "Parsed 72 docs...\n",
      "Parsed 73 docs...\n",
      "Parsed 74 docs...\n",
      "Parsed 75 docs...\n",
      "Parsed 76 docs...\n",
      "Parsed 77 docs...\n",
      "Parsed 78 docs...\n",
      "Parsed 79 docs...\n",
      "Parsed 80 docs...\n",
      "Parsed 81 docs...\n",
      "Parsed 82 docs...\n",
      "Parsed 83 docs...\n",
      "Parsed 84 docs...\n",
      "Parsed 85 docs...\n",
      "Parsed 86 docs...\n",
      "Parsed 87 docs...\n",
      "Parsed 88 docs...\n",
      "Parsed 89 docs...\n",
      "Parsed 90 docs...\n",
      "Parsed 91 docs...\n",
      "Parsed 92 docs...\n",
      "Parsed 93 docs...\n",
      "Parsed 94 docs...\n",
      "Parsed 95 docs...\n",
      "Parsed 96 docs...\n",
      "Parsed 97 docs...\n",
      "Parsed 98 docs...\n",
      "Parsed 99 docs...\n",
      "Parsed 100 docs...\n",
      "CPU times: user 1min 11s, sys: 4.98 s, total: 1min 16s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "# Ingest data into Fonduer via parser\n",
    "corpus_parser = OmniParser(structural=True, lingual=True, visual=False)\n",
    "%time corpus_parser.apply((doc_preprocessor), parallelism=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 100\n",
      "Phrases: 8702\n"
     ]
    }
   ],
   "source": [
    "# Ingest data into Fonduer via parser\n",
    "from fonduer.models import Document, Phrase\n",
    "\n",
    "# Checking database contents\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Phrases:\", session.query(Phrase).count())\n",
    "\n",
    "# corpus_parser = OmniParser(structural=True, lingual=True, visual=False)\n",
    "# %time corpus_parser.apply(doc_preprocessor)#, parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Dividing into Test/Train, Extracting Features, Throttling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 80\n",
      "dev: 10\n",
      "test: 10\n",
      "['http://phoenix.backpage.com/FemaleEscorts/your-sexy-little-secret-young-sweet-petite-20/26284112',\n",
      " 'http://toronto.backpage.com/FemaleEscorts/class-is-inn-session-20/24318467',\n",
      " 'http://boston.backpage.com/FemaleEscorts/my-stp-dad-didnt-tuck-me-in-like-he-promised-_-so-now-i-cant-sleep_-hot-dirty-blonde-girl-22/27609932',\n",
      " 'http://sandiego.backpage.com/FemaleEscorts/classy-and-sexy-will-leave-u-wanting-more-im-here-to-please-u-24/17308287',\n",
      " 'http://newyork.backpage.com/FemaleEscorts/ss-ss-and-more-ss_-phat-booty-on-deck_-50-spec-347-645-7057-23/49285066',\n",
      " 'http://michigan.backpage.com/FemaleEscorts/best-kept-secret-in-ann-arbor60-specials-21/17148900',\n",
      " 'http://toronto.backpage.com/FemaleEscorts/150-hrnew-upscale-puterican-and-egyptian-mix-ebony-in-town-looking-2-get-knky-19/10837807',\n",
      " 'http://phoenix.backpage.com/FemaleEscorts/incall-specials-beautiful-gorgeouse-puerto-rican-21/29872763',\n",
      " 'http://beaumont.backpage.com/FemaleEscorts/wed-oct-2-2014fetishes-and-toyscuddles-and-kali-outcalls-409-683-2721all-areas-29/23039224',\n",
      " 'http://sanantonio.backpage.com/BodyRubs/sexy-latina-bombshell-early-bird-special-60-40/7488533',\n",
      " 'http://miami.backpage.com/FemaleEscorts/latinas-ricas-y-calenticahot-and-spicy-andrea-and-yvonne-newavailable-now305-508-8037-21/28005306',\n",
      " 'http://wichitafalls.backpage.com/FemaleEscorts/im-backlets-playspecials-call-now-23/29559541',\n",
      " 'http://austin.backpage.com/FemaleEscorts/real-pics-great-skills-good-attitude-worth-every-dime-22/10336341',\n",
      " 'http://washingtondc.backpage.com/FemaleEscorts/curvaceous-bombshell-22/15757830',\n",
      " 'http://newyork.backpage.com/FemaleEscorts/new-york-asian-escorts-available-now-18/52777675',\n",
      " 'http://washington.backpage.com/BodyRubs/gorgeous-ebony-playmate-100-in-special-sexi-and-petite-36/15371637',\n",
      " 'http://connecticut.backpage.com/FemaleEscorts/6ftsexysensual-cougar-no-rushing-allowed-46/7168120',\n",
      " 'http://britishcolumbia.backpage.com/FemaleEscorts/coming-to-grand-prairie-classy-curvy-cougar-come-play-47/8161904',\n",
      " 'http://newyork.backpage.com/BodyRubs/grand-open-40st-7-and-8-ave-212-730-2113-22/52774436',\n",
      " 'http://newjersey.backpage.com/FemaleEscorts/incalls-arabicspanish-feminine-soft-skin-clean-smell-good-independent-201-972-0110-25/20022559',\n",
      " 'http://westpalmbeach.backpage.com/FemaleEscorts/businessmen-you-will-love-your-vip-time-with-julia-blue-football-bachelor-poker-parties-too-42/27975372',\n",
      " 'http://ontario.backpage.com/FemaleEscorts/new-open-0913-5-days-only-full-cosplay-36d-sexy-and-busty-hot-3-just-off-plane-girls-19/21859894',\n",
      " 'http://montreal.backpage.com/FemaleEscorts/the-real-naughty-alysha-available-now-incall-and-outcall-438-827-5148-19/4198550',\n",
      " 'http://alberta.backpage.com/FemaleEscorts/back-for-few-dayssexy-french-katrinabig-bootypretty-facetan-and-tightall-that-you-needreal-19/8913767',\n",
      " 'http://ontario.backpage.com/FemaleEscorts/gorgeous-native-calista-yongecollege-you-cant-go-wrong-with-21/24314728',\n",
      " 'http://hampton.backpage.com/FemaleEscorts/n-naughtyhoney-wraps-her-soft-lips-around-your-banana-40qv-incall-25/12231721',\n",
      " 'http://calgary.backpage.com/FemaleEscorts/sweetest-girl-in-town-very-fun-and-great-attitude-27/8915873',\n",
      " 'http://ohio.backpage.com/FemaleEscorts/1-hottest-in-townon-duty-let-me-be-your-stress-reliever-24/11348043',\n",
      " 'http://ventura.backpage.com/FemaleEscorts/new-arrival-exotic-mixed-gorgeous-22/45788482',\n",
      " 'http://manhattan.backpage.com/FemaleEscorts/outcalls-sexy-white-veronica-baby-347570-2002-22/51369692',\n",
      " 'http://minot.backpage.com/FemaleEscorts/sinfully-sweet-aim-to-please-guaranteed-satisfaction-24/3435931',\n",
      " 'http://calgary.backpage.com/FemaleEscorts/c-u-b-a-n-and-j-a-m-a-i-c-a-n-24hrs-always-ready-to-please-great-service-20/8913778',\n",
      " 'http://seoul.backpage.com/FemaleEscorts/focus-forgot-the-world-ready-to-serve-young-glamour-and-hight-society-available-for-you-in-seoul-24/2774898',\n",
      " 'http://northjersey.backpage.com/FemaleEscorts/hot-colombian-doll-available-now-4-u-outcall-only-lets-have-fun-tonightask-about-specials-23/20022906',\n",
      " 'http://halifax.backpage.com/FemaleEscorts/sexy-chick-new-to-halifax-in-and-outcalls-avail-all-night-long-25/1979455',\n",
      " 'http://minnesota.backpage.com/FemaleEscorts/im-the-best-hands-down-choose-me-new-skin-upscale-incall-22/12993915',\n",
      " 'http://southjersey.backpage.com/FemaleEscorts/100-real-hot-brunette-ready-to-play-23/20019984',\n",
      " 'http://sanjose.backpage.com/FemaleEscorts/back-in-town-ready-too-play-60-special-call-me-22/19824366',\n",
      " 'http://denver.classivox.com/t/female-escorts/-httpwww.highclasslist.comguideindex.php/418187/',\n",
      " 'http://northcarolina.backpage.com/FemaleEscorts/petetite-sexybombshell-meeya-20/18370732',\n",
      " 'http://southcoast.backpage.com/FemaleEscorts/lt-m-mk-ll-ur-fantaz-0m-2-f-x0x-7742971401-22/27619468',\n",
      " 'http://denver.backpage.com/FemaleEscorts/hot-hot-azn-sensation-right-here-guys-playful-and-ready-for-you-20/16538395',\n",
      " 'http://sandiego.backpage.com/FemaleEscorts/s-ey-naughtyupscale-hottie-100-real-exotiic-priincess-22/17309326',\n",
      " 'http://wyoming.backpage.com/BodyRubs/full-body-massageclosed-on-oct-2nd-39/2427843',\n",
      " 'http://philadelphia.backpage.com/FemaleEscorts/spanish-barbie-waiting-on-you-23/21060917',\n",
      " 'http://atlanta.backpage.com/BodyRubs/forget-therestcall-thebest-sexy-23/25261070',\n",
      " 'http://losangeles.backpage.com/FemaleEscorts/fun-with-freaky-kinky-kali-k-60sp-100hhr-140hr-21/42620021',\n",
      " 'http://dc.backpage.com/BodyRubs/sensual-nude-body-on-body-by-erotic-blonde-bombshell-outcall-only-26/15202170',\n",
      " 'http://albuquerque.backpage.com/BodyRubs/100hr-100hr-incalls-only-therapeutic-special-9pm-12am-specials-100hr-100hr-100hr-25/5948529',\n",
      " 'http://dallas.backpage.com/FemaleEscorts/sexii-spinner-angel-with-cute-petite-body-n-amazing-green-eyes-26/29527718',\n",
      " 'http://massachusetts.backpage.com/FemaleEscorts/lets-play-20/27207954',\n",
      " 'http://tennessee.backpage.com/BodyRubs/hot-sexy-ebony-topless-or-nude-bodyrubs-mutual-touching-prostate-pleasures-27/6325058',\n",
      " 'http://pennsylvania.backpage.com/BodyRubs/heather-the-39-year-old-blonde-playmate-doesnt-deliver-what-she-says-52/21059557',\n",
      " 'http://georgia.backpage.com/FemaleEscorts/50-juicy-booty-playmate-it-100better-n-person-22/24894188',\n",
      " 'http://tampa.backpage.com/FemaleEscorts/super-sexy-smack-it-grab-it-pull-my-hair-come-get-this-friday-morning-delightpinellas-park-27/13357572',\n",
      " 'http://connecticut.backpage.com/FemaleEscorts/trulygorgeous-extremelybusty-just-what-youve-been-looking-for-21/8476707',\n",
      " 'http://atlanta.backpage.com/FemaleEscorts/247-50-well-reviewed-upscale-doll-face-spinner-w-perfect-rack-full-hr-special-21/23697163',\n",
      " 'http://ontario.backpage.com/FemaleEscorts/outcalls-cici-monroe-ddd-busty-wt-and-full-kissable-lips-21/20399339',\n",
      " 'http://minneapolis.backpage.com/FemaleEscorts/andgtandgt-visiting-andltandlt-hot-new-girls-back-in-town-omg-call-us-now-21/12995155',\n",
      " 'http://everett.backpage.com/FemaleEscorts/edmonds-leaving-fri-noonapple-bottom-sexy-voluptuous-sensual-blueeyes-38ds-open-minded-35/18637433',\n",
      " 'http://southjersey.backpage.com/FemaleEscorts/exotic-italian-sexy-but-sweet-pre-game-for-your-weekend-the-rite-way-24/20023059',\n",
      " 'http://ontario.backpage.com/FemaleEscorts/new-to-ottawa-statuesque-swedish-bombshell-visiting-until-sat-book-now-15304560-mins-available-34/24249886',\n",
      " 'http://alberta.backpage.com/FemaleEscorts/ultimate-pleasure-im-here-to-please-you-come-play-indulge-20/8916325',\n",
      " 'http://providence.backpage.com/FemaleEscorts/100-specials-every-boys-dream-every-mans-fantasy-reward-yourself-and-see-me-19/4429342',\n",
      " 'http://southjersey.backpage.com/FemaleEscorts/andgt-andgt-andgt-andgt-andgt-andgt-andgt-omg-wow-hot-no-rush-petite-24-hours-24-hours-20/20023362',\n",
      " 'http://kansas.backpage.com/FemaleEscorts/petiete-blonde-fun-loving-open-mindedblondes-do-have-more-fun-44/7251779',\n",
      " 'http://pittsburgh.backpage.com/FemaleEscorts/young-petite-eboney-spinner-24/21063123',\n",
      " 'http://edmonton.backpage.com/FemaleEscorts/sweet-bbw-cuban-n-jamaican-ready-to-play-my-spot-north-by-century-casino-34/8915145',\n",
      " 'http://alberta.backpage.com/FemaleEscorts/hot-hot-hot-you-dont-want-to-miss-out-sensual-seductive-satisfying-250special-25/8807069',\n",
      " 'http://inlandempire.backpage.com/FemaleEscorts/look-at-my-video-brazillianstar-blck-eye-candy-pecials-4080120-beauty-from-brazil-22/45273307',\n",
      " 'http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716',\n",
      " 'http://vancouver.backpage.com/FemaleEscorts/specials-langley-ru-rutt-22/8146609',\n",
      " 'http://edmonton.backpage.com/FemaleEscorts/double-the-trouble-double-the-fun-let-us-make-u-1-boo-24/8915710',\n",
      " 'http://michigan.backpage.com/FemaleEscorts/sweet-n-sexy-available-now-incalloutcall-25/13888687',\n",
      " 'http://providence.backpage.com/FemaleEscorts/im-the-one-you-missing-and-im-here-to-play-22/4429157',\n",
      " 'http://wisconsin.backpage.com/FemaleEscorts/brunette-bombshell-30/8966929',\n",
      " 'http://illinois.backpage.com/FemaleEscorts/new-menu-l-yd-italian-perky-and-petite-caucasian-treat-new-menu-spcls-22/23512826',\n",
      " 'http://eastbay.backpage.com/FemaleEscorts/incall-amazing-best-service-hot-asian-girl-new-arrived-103-now-5o7-55o-23/19823839',\n",
      " 'http://raleigh.backpage.com/FemaleEscorts/36d-sexy-sweetheart-30/3617016',\n",
      " 'http://idaho.backpage.com/FemaleEscorts/sexxybrunettebest-night-ever-20/5485238']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Getting all documents parsed by Fonduer\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "from fonduer.models import Document, Phrase\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "dev_set_sz = np.round(ld*0.1)\n",
    "test_set_sz = np.round(ld*0.1)\n",
    "train_set_sz = ld - dev_set_sz - test_set_sz\n",
    "\n",
    "# Setting up train, dev, and test sets\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "# Setting up test, dev dictionaries\n",
    "dev_gold_dict={}\n",
    "test_gold_dict ={}\n",
    "# Creating list of (document name, Fonduer document object) tuples\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "shuffle(data)\n",
    "\n",
    "cnt= dev_set_sz\n",
    "\n",
    "# Adding unlabeled data to train set, \n",
    "# labaled data to dev/test sets in alternating fashion\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    " \n",
    "    try:\n",
    "        loc = gold_dict_[doc_name]\n",
    "        \n",
    "        if cnt>0:\n",
    "                \n",
    "            dev_docs.add(doc)\n",
    "            dev_gold_dict[doc_name] = loc\n",
    "            cnt = cnt-1\n",
    "                \n",
    "        elif cnt> -(test_set_sz) and cnt<=0  :\n",
    "                \n",
    "            test_docs.add(doc)\n",
    "            test_gold_dict[doc_name] = loc\n",
    "            cnt = cnt-1\n",
    "        else:\n",
    "            train_docs.add(doc)       \n",
    "    except:\n",
    "        train_docs.add(doc)\n",
    "    \n",
    "#Printing length of train/test/dev sets\n",
    "print(\"train:\",len(train_docs))\n",
    "print(\"dev:\" ,len(dev_docs))\n",
    "print(\"test:\",len(test_docs))\n",
    "\n",
    "#Printing some filenames \n",
    "from pprint import pprint\n",
    "pprint([x.name for x in train_docs])\n",
    "dev_list=len(set([x.name for x in dev_docs]))\n",
    "test_list=len(set([x.name for x in test_docs]))\n",
    "#pprint([x.name for x in test_docs])\n",
    "# url_int = labeled_url_set & test_list\n",
    "print (test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing matchers module and defining LocationMatchers\n",
    "from fonduer.matchers import *\n",
    "class LocationMatcher(RegexMatchEach):\n",
    "    \"\"\"\n",
    "    Matches Spans that are the names of locations, as identified by spaCy.\n",
    "    A convenience class for setting up a RegexMatchEach to match spans\n",
    "    for which each token was tagged as a location.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *children, **kwargs):\n",
    "        kwargs['attrib'] = 'ner_tags'\n",
    "        kwargs['rgx'] = 'GPE|LOC'\n",
    "        super(LocationMatcher, self).__init__(*children, **kwargs)\n",
    "location_matcher_1 = LocationMatcher(longest_match_only=True) \n",
    "######################################## these are sths to think about for future #################################\n",
    "def filter_lookup_city(span_input):\n",
    "    span_input = span_input.get_span()\n",
    "    span_input = span.upper()\n",
    "    places = GeoText(u)\n",
    "    lst = places.cities\n",
    "    if len(lst)!=0:\n",
    "        \n",
    "        return True    \n",
    "    else:\n",
    "        return False\n",
    "def lookup_cardinal_direction(span_input):\n",
    "    cardinal_loc= [\"north\",\"west\",\"east\",\"south\",\"northeast\",\"NE\",\\\n",
    "                   \"southeast\",\"SE\", \"southwest\",\"SW\",\"northwest\",\"NW\"]\n",
    "    span_input = span_input.get_span()\n",
    "    splitted_span= span_input.split()\n",
    "    for s in splitted_span:\n",
    "        if s in cardinal_loc:\n",
    "                 \n",
    "            return True    \n",
    "        else:\n",
    "            return False\n",
    "def filter_lookup_country_name(span_input):\n",
    "    span_input = span_input.get_span()\n",
    "    try:\n",
    "        out = pycountry.countries.lookup(span).name\n",
    "    except:\n",
    "        out = 'no country'\n",
    "    if out != 'no country':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def filter_lookup_state_name(span_input):\n",
    "    span = span_input.get_span()\n",
    "    try:\n",
    "        out = us.states.lookup(span).name\n",
    "    except:\n",
    "        out = 'no state'\n",
    "    if out != 'no country':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "city_lambda_matcher =LambdaFunctionMatcher(func=filter_lookup_city)\n",
    "cardinal_direction_lambda_matcher =LambdaFunctionMatcher(func=lookup_cardinal_direction)\n",
    "country_lambda_matcher =LambdaFunctionMatcher(func=filter_lookup_country_name)\n",
    "state_lambda_matcher =LambdaFunctionMatcher(func=filter_lookup_state_name)\n",
    "\n",
    "#location_matcher_ = Union(city_lambda_matcher,cardinal_direction_lambda_matcher,\\\n",
    "#                         country_lambda_matcher,state_lambda_matcher,location_matcher_1)\n",
    "\n",
    "#importing NGrams and defining location_ngrams \n",
    "from fonduer.candidates import OmniNgrams\n",
    "location_ngrams = OmniNgrams(n_max=10, split_tokens=[])\n",
    "location_matcher =location_matcher_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import range\n",
    "import csv\n",
    "import codecs\n",
    "import pycountry\n",
    "import us\n",
    "import editdistance\n",
    "\n",
    "from fonduer.utils import ProgressBar\n",
    "from fonduer.models import GoldLabel, GoldLabelKey\n",
    "\n",
    "# Defining function for getting gold labels\n",
    "# Could go in utils file later!\n",
    "\n",
    "def filter_lookup_country_name(cn):\n",
    "    try:\n",
    "        out = pycountry.countries.lookup(cn).name\n",
    "    except:\n",
    "        out = 'no country'\n",
    "    if out != 'no country':\n",
    "        return out\n",
    "        \n",
    "\n",
    "def lookup_country_alpha3(cn):\n",
    "    try:\n",
    "        out = pycountry.countries.lookup(cn).alpha_3\n",
    "    except:\n",
    "        out = 'no country'\n",
    "    return out\n",
    "\n",
    "def lookup_country_alpha2(cn):\n",
    "    try:\n",
    "        out = pycountry.countries.lookup(cn).alpha_2\n",
    "    except:\n",
    "        out = 'no country'\n",
    "    return out\n",
    "\n",
    "def lookup_state_name(cn):\n",
    "    try:\n",
    "        out = us.states.lookup(cn).name\n",
    "    except:\n",
    "        out = 'no state'\n",
    "    return out\n",
    "\n",
    "def lookup_state_abbr(cn):\n",
    "    try:\n",
    "        out = us.states.lookup(val).abbr\n",
    "    except:\n",
    "        out = 'no state'\n",
    "    return out\n",
    "\n",
    "def check_editdistance(val,targets):\n",
    "    for tgt in targets:\n",
    "        if editdistance.eval(val,tgt)<=3:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining candidate Throttlers ( or LFs )\n",
    "\n",
    "Need to think about them for the future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.lf_helpers import *\n",
    "from fonduer import RegexMatchSpan, DictionaryMatch, LambdaFunctionMatcher, Intersect, Union\n",
    "import re\n",
    "from builtins import range\n",
    "import csv\n",
    "import codecs\n",
    "import pycountry\n",
    "import us\n",
    "import editdistance\n",
    "#from commonregex import CommonRegex\n",
    "# Creating filter to eliminate mentions of currency  \n",
    "def location_currencies_filter(location):\n",
    "    list_currencies = [ \"dollar\", \"dollars\", \"lira\",\"kwacha\",\"rials\",\"rial\",\"dong\",\"dongs\",\"fuerte\",\"euro\",\n",
    "                       \"euros\",\"vatu\",\"som\",\"peso\",\"sterling\",\"sterlings\",\"soms\",\"pestos\",\n",
    "                       \"pounds\", \n",
    "                  \"pound\",\"dirham\",\"dirhams\",\"hryvnia\",\"manat\",\"manats\",\"liras\",\"lira\",\n",
    "                       \"dinar\",\"dinars\",\"pa'anga\",\"franc\",\"baht\",\"schilling\",\n",
    "                  \"somoni\",\"krona\",\"lilangeni\",\"rupee\",\"rand\",\"shilling\",\"leone\",\"riyal\",\"dobra\",\n",
    "                  \"tala\",\"ruble\",\"zloty\",\"peso\",\"sol\",\"quarani\",\"kina\",\"guinean\",\"balboa\",\"krone\",\"naira\",\n",
    "                  \"cordoba\",\"kyat\",\"metical\",\"togrog\",\"leu\",\"ouguiya\",\"rufiyaa\",\"ringgit\",\"kwacha\",\n",
    "                  \"ariary\",\"denar\",\"litas\",\"loti\",\"lats\",\"kip\",\"som\",\"won\",\"tenge\",\"yen\",\"shekel\",\"rupiah\",\n",
    "                  \"forint\",\"lempira\",\"gourde\",\"quetzal\",\"cedi\",\"lari\",\"dalasi\",\"cfp\",\"birr\",\"kroon\",\"nakfa\",\n",
    "                  \"cfa\",\"Peso\",\"koruna\",\"croatian\",\"colon\",\"yuan\",\"escudo\",\"cape\",\"riel\",\"lev\",\"real\"\n",
    "                  ,\"real\",\"mark\",\"boliviano\",\"ngultrum\",\"taka\",\"manat\",\"dram\",\"kwanza\",\"lek\",\"afghani\",\"renminbi\"]\n",
    "\n",
    "    \n",
    "    cand_right_tokens = list(get_right_ngrams(location,window=2))\n",
    "    for cand in cand_right_tokens:\n",
    "        if cand not in list_currencies:\n",
    "            return True\n",
    "def filter_capital_words(c):\n",
    "    patern = re.compile(\"(?=.{1,20}$)[A-Z](\\s*?[A-Z])*$\")\n",
    "#     result = prog.match(string)\n",
    "    span = c[0].get_span()\n",
    "    result = patern.match(span)\n",
    "    if result:\n",
    "        return False\n",
    "\n",
    "def filter_characters(c):\n",
    "    patern_1 = re.compile(\"^[(+*\\?/\\-,@/$)]$\")\n",
    "    patern_2 = re.compile (\"^[(+*?/\\-)]\\s*[(A-Z)]*[(a-z)]*$\")\n",
    "    patern_3 = re.compile(\"@[a-z]*|[1-9]\")\n",
    "    patern_4 = re.compile(\"(?=.{1,20})[A-Z]*[(+*\\?/\\-&)]\")\n",
    "    span = c[0].get_span()\n",
    "    result_1 = patern_1.match(span)\n",
    "    result_2 = patern_2.match(span)\n",
    "    result_3 = patern_3.match(span)\n",
    "    result_4 = patern_4.match(span)\n",
    "    if result_1 or result_2 or result_3 or result_4:\n",
    "        return False\n",
    "\n",
    "def filter_numbers(c):\n",
    "    patern_1 = re.compile(\"^[1-9][(\\')][1-9][(\\\")]$\")\n",
    "    patern_2 = re.compile(\"^[1-9]$\")\n",
    "    span = c[0].get_span()\n",
    "    result_1 = patern_1.match(span)\n",
    "    result_2 = patern_2.match(span)\n",
    "    if result_1 or result_2:\n",
    "        return False\n",
    "\n",
    "def filter_(c):\n",
    "    if location_currencies_filter(c) == False:\n",
    "#         print(1)\n",
    "        return False\n",
    "    if filter_capital_words(c) == False:\n",
    "#         print(2)\n",
    "        return False\n",
    "    if filter_characters(c) == False:\n",
    "#         print(3)\n",
    "        return False\n",
    "    if filter_numbers(c) == False:\n",
    "#         print(4)\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "\n",
    "# import CommonRegex\n",
    "# # # txt =\"hi, this is saeideh. i leave in 1600 villa st.,\"\n",
    "# # # parser = CommonRegex()\n",
    "# # # parser.btc_addresses(txt)\n",
    "\n",
    "# # # address_ = re.compile(r'(street|st|avenue|ave|road|rd|highway|hwy|square|sq|trail|trl|drive|dr|court|ct|parkway|pkwy|circle|cir|boulevard|blvd)')\n",
    "# # # address_.search(txt).group()\n",
    "# #candidate_filter=filter_\n",
    "#candidate_filter = location_currencies_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying candidate extraction\n",
    "\n",
    "Still need to modify it. Each time that I have run the pipeline, it gives me diffrent numbers for the extracted candidates. \n",
    "Also instead of 80 docs for train set, matcher just finds candidates from 60-59 documents. We need to fix this issue as well, since we know that for all 80 documents we have gold_location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fonduer.udf - Clearing existing...\n",
      "[INFO] fonduer.udf - Running UDF...\n",
      "[========================================] 100%\n",
      "CPU times: user 3.18 s, sys: 89.7 ms, total: 3.27 s\n",
      "Wall time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "from fonduer.candidates import CandidateExtractor\n",
    "\n",
    "# Defining candidate extractor\n",
    "candidate_extractor = CandidateExtractor(Location_Extraction,[location_ngrams], [location_matcher])\n",
    "#                                          candidate_filter=candidate_filter)\n",
    "\n",
    "# Extracting candidates from each split\n",
    "%time candidate_extractor.apply(train_docs, split=0)#, parallelism=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[location_extraction(Span(\"b'East Valley'\", sentence=22637, chars=[10,20], words=[2,3])),\n",
       " location_extraction(Span(\"b'Boston'\", sentence=19991, chars=[10,15], words=[2,2])),\n",
       " location_extraction(Span(\"b'South Shore - Braintree / Quincy'\", sentence=20010, chars=[1,32], words=[1,6])),\n",
       " location_extraction(Span(\"b'South Shore - Braintree / Quincy'\", sentence=20002, chars=[1,32], words=[1,6])),\n",
       " location_extraction(Span(\"b'Braintree'\", sentence=20024, chars=[15,23], words=[5,5])),\n",
       " location_extraction(Span(\"b'South Shore - Braintree / Quincy'\", sentence=19997, chars=[1,32], words=[1,6])),\n",
       " location_extraction(Span(\"b'SERIOUS'\", sentence=21349, chars=[0,6], words=[0,0])),\n",
       " location_extraction(Span(\"b'OR'\", sentence=28588, chars=[20,21], words=[5,5])),\n",
       " location_extraction(Span(\"b'Ann Arbor$60'\", sentence=23885, chars=[3,14], words=[1,2])),\n",
       " location_extraction(Span(\"b'Ann Arbor'\", sentence=23909, chars=[0,8], words=[0,1])),\n",
       " location_extraction(Span(\"b'Ypsilanti'\", sentence=23909, chars=[11,19], words=[3,3])),\n",
       " location_extraction(Span(\"b'Ann Arbor$60'\", sentence=23858, chars=[3,14], words=[1,2])),\n",
       " location_extraction(Span(\"b'Independent'\", sentence=26406, chars=[25,35], words=[4,4])),\n",
       " location_extraction(Span(\"b'Brampton'\", sentence=26412, chars=[10,17], words=[2,2])),\n",
       " location_extraction(Span(\"b'Phoenix'\", sentence=28210, chars=[24,30], words=[5,5])),\n",
       " location_extraction(Span(\"b'North Phoenix'\", sentence=28210, chars=[33,45], words=[7,8])),\n",
       " location_extraction(Span(\"b'San Antonio'\", sentence=21519, chars=[10,20], words=[2,3])),\n",
       " location_extraction(Span(\"b'Miami'\", sentence=20114, chars=[10,14], words=[2,2])),\n",
       " location_extraction(Span(\"b'North AUSTIN'\", sentence=19513, chars=[18,29], words=[4,5])),\n",
       " location_extraction(Span(\"b'Austin'\", sentence=19513, chars=[10,15], words=[2,2])),\n",
       " location_extraction(Span(\"b'Gaithersburg'\", sentence=23271, chars=[40,51], words=[7,7])),\n",
       " location_extraction(Span(\"b'DC'\", sentence=23244, chars=[12,13], words=[2,2])),\n",
       " location_extraction(Span(\"b'Rockville'\", sentence=23271, chars=[29,37], words=[5,5])),\n",
       " location_extraction(Span(\"b'DC'\", sentence=23242, chars=[12,13], words=[2,2])),\n",
       " location_extraction(Span(\"b'Southern Maryland'\", sentence=23271, chars=[10,26], words=[2,3])),\n",
       " location_extraction(Span(\"b'DC'\", sentence=23221, chars=[14,15], words=[3,3])),\n",
       " location_extraction(Span(\"b'New York'\", sentence=22023, chars=[0,7], words=[0,1])),\n",
       " location_extraction(Span(\"b'New York'\", sentence=21993, chars=[0,7], words=[0,1])),\n",
       " location_extraction(Span(\"b'Earth'\", sentence=26744, chars=[81,85], words=[13,13])),\n",
       " location_extraction(Span(\"b'Seattle'\", sentence=26762, chars=[28,34], words=[6,6])),\n",
       " location_extraction(Span(\"b'Tacoma'\", sentence=26762, chars=[21,26], words=[4,4])),\n",
       " location_extraction(Span(\"b'WEST END'\", sentence=26608, chars=[31,38], words=[6,7])),\n",
       " location_extraction(Span(\"b'North Jersey'\", sentence=24876, chars=[10,21], words=[2,3])),\n",
       " location_extraction(Span(\"b'North Bergen'\", sentence=24876, chars=[24,35], words=[5,6])),\n",
       " location_extraction(Span(\"b'PALM CITY'\", sentence=19883, chars=[15,23], words=[4,5])),\n",
       " location_extraction(Span(\"b'PALM BEACH'\", sentence=19883, chars=[35,44], words=[9,10])),\n",
       " location_extraction(Span(\"b'Stuart'\", sentence=19895, chars=[27,32], words=[6,6])),\n",
       " location_extraction(Span(\"b'Jupiter'\", sentence=19895, chars=[34,40], words=[8,8])),\n",
       " location_extraction(Span(\"b'West Palm Beach'\", sentence=19895, chars=[10,24], words=[2,4])),\n",
       " location_extraction(Span(\"b'PBG'\", sentence=19883, chars=[47,49], words=[12,12])),\n",
       " location_extraction(Span(\"b'Alysha'\", sentence=24276, chars=[17,22], words=[3,3])),\n",
       " location_extraction(Span(\"b'Alysha'\", sentence=24300, chars=[17,22], words=[3,3])),\n",
       " location_extraction(Span(\"b'Montreal'\", sentence=24321, chars=[10,17], words=[2,2])),\n",
       " location_extraction(Span(\"b'Calgary'\", sentence=25579, chars=[10,16], words=[2,2])),\n",
       " location_extraction(Span(\"b'NEED?*REAL?'\", sentence=25605, chars=[88,98], words=[13,14])),\n",
       " location_extraction(Span(\"b'Calgary'\", sentence=25579, chars=[19,25], words=[4,4])),\n",
       " location_extraction(Span(\"b'NEED?*REAL?'\", sentence=25621, chars=[88,98], words=[13,14])),\n",
       " location_extraction(Span(\"b'Toronto'\", sentence=25250, chars=[18,24], words=[4,4])),\n",
       " location_extraction(Span(\"b'Calgary'\", sentence=20961, chars=[10,16], words=[2,2])),\n",
       " location_extraction(Span(\"b'Calgary'\", sentence=20961, chars=[19,25], words=[4,4])),\n",
       " location_extraction(Span(\"b'WeSt'\", sentence=22532, chars=[26,29], words=[6,6])),\n",
       " location_extraction(Span(\"b'Y.O.U'\", sentence=22513, chars=[27,31], words=[7,7])),\n",
       " location_extraction(Span(\"b'O.O'\", sentence=22517, chars=[5,7], words=[1,1])),\n",
       " location_extraction(Span(\"b'Cleveland'\", sentence=22532, chars=[10,18], words=[2,2])),\n",
       " location_extraction(Span(\"b'Thousand Oaks'\", sentence=19422, chars=[19,31], words=[4,5])),\n",
       " location_extraction(Span(\"b'Ventura'\", sentence=19422, chars=[10,16], words=[2,2])),\n",
       " location_extraction(Span(\"b'Manhattan'\", sentence=19734, chars=[1,9], words=[1,1])),\n",
       " location_extraction(Span(\"b'NEW YORK'\", sentence=19676, chars=[7,14], words=[2,3])),\n",
       " location_extraction(Span(\"b'CALGARY'\", sentence=26139, chars=[1,7], words=[1,1])),\n",
       " location_extraction(Span(\"b'NE'\", sentence=26118, chars=[11,12], words=[3,3])),\n",
       " location_extraction(Span(\"b'Aliyah'\", sentence=26080, chars=[20,25], words=[6,6])),\n",
       " location_extraction(Span(\"b'Calgary'\", sentence=26093, chars=[10,16], words=[2,2])),\n",
       " location_extraction(Span(\"b'NE'\", sentence=26139, chars=[10,11], words=[3,3])),\n",
       " location_extraction(Span(\"b\"5'3\"\", sentence=26084, chars=[0,2], words=[0,0])),\n",
       " location_extraction(Span(\"b'CALGARY'\", sentence=26118, chars=[1,7], words=[1,1])),\n",
       " location_extraction(Span(\"b'Calgary'\", sentence=26093, chars=[19,25], words=[4,4])),\n",
       " location_extraction(Span(\"b'Seoul'\", sentence=21728, chars=[68,72], words=[12,12])),\n",
       " location_extraction(Span(\"b'Seoul'\", sentence=21757, chars=[0,4], words=[0,0])),\n",
       " location_extraction(Span(\"b'Seoul'\", sentence=21701, chars=[68,72], words=[12,12])),\n",
       " location_extraction(Span(\"b'Seoul'\", sentence=21773, chars=[10,14], words=[2,2])),\n",
       " location_extraction(Span(\"b'North jersey'\", sentence=19339, chars=[67,78], words=[12,13])),\n",
       " location_extraction(Span(\"b'Lyndhurst'\", sentence=19342, chars=[45,53], words=[11,11])),\n",
       " location_extraction(Span(\"b'Lodi'\", sentence=19342, chars=[32,35], words=[7,7])),\n",
       " location_extraction(Span(\"b'Hoboken'\", sentence=19342, chars=[37,43], words=[9,9])),\n",
       " location_extraction(Span(\"b'Passaic'\", sentence=19342, chars=[24,30], words=[5,5])),\n",
       " location_extraction(Span(\"b'North Jersey'\", sentence=19342, chars=[10,21], words=[2,3])),\n",
       " location_extraction(Span(\"b'Halifax'\", sentence=28044, chars=[10,16], words=[2,2])),\n",
       " location_extraction(Span(\"b'@#@3'\", sentence=28049, chars=[18,21], words=[4,4])),\n",
       " location_extraction(Span(\"b'South Jersey'\", sentence=20696, chars=[10,21], words=[2,3])),\n",
       " location_extraction(Span(\"b'Natalie'\", sentence=20680, chars=[0,6], words=[0,0])),\n",
       " location_extraction(Span(\"b'San Jose'\", sentence=20876, chars=[10,17], words=[2,3])),\n",
       " location_extraction(Span(\"b'Town'\", sentence=20827, chars=[8,11], words=[2,2])),\n",
       " location_extraction(Span(\"b'Town'\", sentence=20852, chars=[8,11], words=[2,2])),\n",
       " location_extraction(Span(\"b'Anonymously</button'\", sentence=27061, chars=[72,90], words=[17,17])),\n",
       " location_extraction(Span(\"b'Greensboro'\", sentence=24252, chars=[10,19], words=[2,2])),\n",
       " location_extraction(Span(\"b'Black'\", sentence=25134, chars=[132,136], words=[30,30])),\n",
       " location_extraction(Span(\"b'South Coast'\", sentence=25140, chars=[10,20], words=[2,3])),\n",
       " location_extraction(Span(\"b'Denver'\", sentence=28291, chars=[10,15], words=[2,2])),\n",
       " location_extraction(Span(\"b'Denver'\", sentence=28291, chars=[18,23], words=[4,4])),\n",
       " location_extraction(Span(\"b'Colorado'\", sentence=28291, chars=[26,33], words=[6,6])),\n",
       " location_extraction(Span(\"b'San Diego'\", sentence=26919, chars=[18,26], words=[4,5])),\n",
       " location_extraction(Span(\"b'Philadelphia'\", sentence=23471, chars=[10,21], words=[2,2])),\n",
       " location_extraction(Span(\"b'Atlanta'\", sentence=25703, chars=[10,16], words=[2,2])),\n",
       " location_extraction(Span(\"b'Costa Mesa'\", sentence=24790, chars=[1,10], words=[1,2])),\n",
       " location_extraction(Span(\"b'Town'\", sentence=24788, chars=[17,20], words=[5,5])),\n",
       " location_extraction(Span(\"b'District Of Columbia'\", sentence=23109, chars=[10,29], words=[2,4])),\n",
       " location_extraction(Span(\"b'Christina'\", sentence=24506, chars=[0,8], words=[0,0])),\n",
       " location_extraction(Span(\"b'Albuquerque'\", sentence=24511, chars=[0,10], words=[0,0])),\n",
       " location_extraction(Span(\"b'Dallas'\", sentence=25926, chars=[10,15], words=[2,2])),\n",
       " location_extraction(Span(\"b'SERIOUS'\", sentence=25921, chars=[0,6], words=[0,0])),\n",
       " location_extraction(Span(\"b'Dallas'\", sentence=25918, chars=[56,61], words=[11,11])),\n",
       " location_extraction(Span(\"b'Dallas'\", sentence=25916, chars=[6,11], words=[1,1])),\n",
       " location_extraction(Span(\"b'West Memphis'\", sentence=22360, chars=[31,42], words=[6,7])),\n",
       " location_extraction(Span(\"b'Memphis'\", sentence=22363, chars=[10,16], words=[2,2])),\n",
       " location_extraction(Span(\"b'Covington'\", sentence=22360, chars=[84,92], words=[17,17])),\n",
       " location_extraction(Span(\"b'Arlington'\", sentence=22360, chars=[74,82], words=[15,15])),\n",
       " location_extraction(Span(\"b'Collierville'\", sentence=22360, chars=[61,72], words=[13,13])),\n",
       " location_extraction(Span(\"b'Peachtree City'\", sentence=23545, chars=[21,34], words=[3,4])),\n",
       " location_extraction(Span(\"b'PINELLAS PARK AREA'\", sentence=20501, chars=[11,28], words=[2,4])),\n",
       " location_extraction(Span(\"b'TIME&COMPANIONSHIP'\", sentence=20589, chars=[20,37], words=[4,4])),\n",
       " location_extraction(Span(\"b'New Haven'\", sentence=20592, chars=[10,18], words=[2,3])),\n",
       " location_extraction(Span(\"b'Atlanta'\", sentence=20802, chars=[10,16], words=[2,2])),\n",
       " location_extraction(Span(\"b\"5'3\"\", sentence=20791, chars=[0,2], words=[0,0])),\n",
       " location_extraction(Span(\"b'Ottawa'\", sentence=25771, chars=[10,15], words=[2,2])),\n",
       " location_extraction(Span(\"b\"5'4\"\", sentence=25768, chars=[70,72], words=[12,12])),\n",
       " location_extraction(Span(\"b'Ivory'\", sentence=25327, chars=[28,32], words=[6,6])),\n",
       " location_extraction(Span(\"b'Web'\", sentence=22115, chars=[13,15], words=[3,3])),\n",
       " location_extraction(Span(\"b'Sophie'\", sentence=24400, chars=[13,18], words=[2,2])),\n",
       " location_extraction(Span(\"b'Sophie'\", sentence=24411, chars=[13,18], words=[2,2])),\n",
       " location_extraction(Span(\"b'Atlantic City'\", sentence=24394, chars=[24,36], words=[5,6])),\n",
       " location_extraction(Span(\"b'South Jersey'\", sentence=24418, chars=[1,12], words=[1,2])),\n",
       " location_extraction(Span(\"b'South Jersey'\", sentence=24394, chars=[10,21], words=[2,3])),\n",
       " location_extraction(Span(\"b'South Jersey'\", sentence=24412, chars=[1,12], words=[1,2])),\n",
       " location_extraction(Span(\"b'Ottawa'\", sentence=27337, chars=[7,12], words=[2,2])),\n",
       " location_extraction(Span(\"b'Ottawa'\", sentence=27364, chars=[7,12], words=[2,2])),\n",
       " location_extraction(Span(\"b'Ottawa'\", sentence=27394, chars=[10,15], words=[2,2])),\n",
       " location_extraction(Span(\"b'Statuesque'\", sentence=27338, chars=[0,9], words=[0,0])),\n",
       " location_extraction(Span(\"b'Statuesque'\", sentence=27365, chars=[0,9], words=[0,0])),\n",
       " location_extraction(Span(\"b'FETISHES'\", sentence=26835, chars=[9,16], words=[2,2])),\n",
       " location_extraction(Span(\"b'Serious'\", sentence=26836, chars=[5,11], words=[1,1])),\n",
       " location_extraction(Span(\"b'Calgary'\", sentence=26843, chars=[10,16], words=[2,2])),\n",
       " location_extraction(Span(\"b'Atlantic City'\", sentence=24693, chars=[24,36], words=[5,6])),\n",
       " location_extraction(Span(\"b'South Jersey'\", sentence=24693, chars=[10,21], words=[2,3])),\n",
       " location_extraction(Span(\"b'Nice'\", sentence=24687, chars=[22,25], words=[5,5])),\n",
       " location_extraction(Span(\"b'Sweet'\", sentence=24684, chars=[13,17], words=[4,4])),\n",
       " location_extraction(Span(\"b'Wichita'\", sentence=22893, chars=[10,16], words=[2,2])),\n",
       " location_extraction(Span(\"b'GG'\", sentence=22208, chars=[43,44], words=[8,8])),\n",
       " location_extraction(Span(\"b'Pittsburgh'\", sentence=22212, chars=[12,21], words=[3,3])),\n",
       " location_extraction(Span(\"b'Los Angeles'\", sentence=27624, chars=[95,105], words=[19,20])),\n",
       " location_extraction(Span(\"b'San Mateo'\", sentence=27624, chars=[353,361], words=[76,77])),\n",
       " location_extraction(Span(\"b'Santa Rosa'\", sentence=27624, chars=[417,426], words=[91,92])),\n",
       " location_extraction(Span(\"b'Palmdale'\", sentence=27624, chars=[226,233], words=[48,48])),\n",
       " location_extraction(Span(\"b'San Francisco'\", sentence=27624, chars=[291,303], words=[62,63])),\n",
       " location_extraction(Span(\"b'Fresno'\", sentence=27624, chars=[41,46], words=[8,8])),\n",
       " location_extraction(Span(\"b'California'\", sentence=27568, chars=[0,9], words=[0,0])),\n",
       " location_extraction(Span(\"b'Napa'\", sentence=27624, chars=[171,174], words=[35,35])),\n",
       " location_extraction(Span(\"b'california'\", sentence=27618, chars=[16,25], words=[3,3])),\n",
       " location_extraction(Span(\"b'Los Gatos'\", sentence=27624, chars=[108,116], words=[22,23])),\n",
       " location_extraction(Span(\"b'Palo Alto'\", sentence=27624, chars=[236,244], words=[50,51])),\n",
       " location_extraction(Span(\"b'Santa Barbara'\", sentence=27624, chars=[364,376], words=[79,80])),\n",
       " location_extraction(Span(\"b'North Bay'\", sentence=27624, chars=[177,185], words=[37,38])),\n",
       " location_extraction(Span(\"b'Humboldt County'\", sentence=27624, chars=[49,63], words=[10,11])),\n",
       " location_extraction(Span(\"b'Saratoga, South Lake Tahoe'\", sentence=27624, chars=[429,454], words=[94,98])),\n",
       " location_extraction(Span(\"b'San Gabriel Valley'\", sentence=27624, chars=[306,323], words=[65,67])),\n",
       " location_extraction(Span(\"b'Sacramento'\", sentence=27624, chars=[247,256], words=[53,53])),\n",
       " location_extraction(Span(\"b'Videos'\", sentence=27516, chars=[0,5], words=[0,0])),\n",
       " location_extraction(Span(\"b'California'\", sentence=27622, chars=[16,25], words=[3,3])),\n",
       " location_extraction(Span(\"b'Mendocino County'\", sentence=27624, chars=[119,134], words=[25,26])),\n",
       " location_extraction(Span(\"b'Oakland'\", sentence=27624, chars=[188,194], words=[40,40])),\n",
       " location_extraction(Span(\"b'california'\", sentence=27613, chars=[0,9], words=[0,0])),\n",
       " location_extraction(Span(\"b'Santa Clara'\", sentence=27624, chars=[379,389], words=[82,83])),\n",
       " location_extraction(Span(\"b'Ukrainian'\", sentence=27561, chars=[0,8], words=[0,0])),\n",
       " location_extraction(Span(\"b'Bakersfield'\", sentence=27624, chars=[0,10], words=[0,0])),\n",
       " location_extraction(Span(\"b'Cupertino'\", sentence=27624, chars=[30,38], words=[6,6])),\n",
       " location_extraction(Span(\"b'San Diego'\", sentence=27624, chars=[259,267], words=[55,56])),\n",
       " location_extraction(Span(\"b'Imperial County'\", sentence=27624, chars=[66,80], words=[13,14])),\n",
       " location_extraction(Span(\"b'San Jose'\", sentence=27624, chars=[326,333], words=[69,70])),\n",
       " location_extraction(Span(\"b'california'\", sentence=27662, chars=[0,9], words=[0,0])),\n",
       " location_extraction(Span(\"b'Orange County'\", sentence=27624, chars=[197,209], words=[42,43])),\n",
       " location_extraction(Span(\"b'Welsh'\", sentence=27564, chars=[0,4], words=[0,0])),\n",
       " location_extraction(Span(\"b'Modesto'\", sentence=27624, chars=[137,143], words=[28,28])),\n",
       " location_extraction(Span(\"b'Santa Cruz'\", sentence=27624, chars=[392,401], words=[85,86])),\n",
       " location_extraction(Span(\"b'Sunnyvale'\", sentence=27624, chars=[457,465], words=[100,100])),\n",
       " location_extraction(Span(\"b'San Fernando Valley'\", sentence=27624, chars=[270,288], words=[58,60])),\n",
       " location_extraction(Span(\"b'San Luis Obispo'\", sentence=27624, chars=[336,350], words=[72,74])),\n",
       " location_extraction(Span(\"b'Long Beach'\", sentence=27624, chars=[83,92], words=[16,17])),\n",
       " location_extraction(Span(\"b'Ventura'\", sentence=27624, chars=[468,474], words=[102,102])),\n",
       " location_extraction(Span(\"b'Santa Maria'\", sentence=27624, chars=[404,414], words=[88,89])),\n",
       " location_extraction(Span(\"b'Palm Springs'\", sentence=27624, chars=[212,223], words=[45,46])),\n",
       " location_extraction(Span(\"b'Chico'\", sentence=27624, chars=[23,27], words=[4,4])),\n",
       " location_extraction(Span(\"b'address.\",\"systemErrorMessage\":\"\",\"hasValidationError\"'\", sentence=27724, chars=[199,252], words=[6,6])),\n",
       " location_extraction(Span(\"b'APPOINTMENT'\", sentence=27840, chars=[7,17], words=[2,2])),\n",
       " location_extraction(Span(\"b'Visalia'\", sentence=27624, chars=[477,483], words=[104,104])),\n",
       " location_extraction(Span(\"b'Romanian'\", sentence=27553, chars=[0,7], words=[0,0])),\n",
       " location_extraction(Span(\"b'Mountain View'\", sentence=27624, chars=[156,168], words=[32,33])),\n",
       " location_extraction(Span(\"b'Vancouver'\", sentence=26317, chars=[10,18], words=[2,2])),\n",
       " location_extraction(Span(\"b'Vicki'\", sentence=22272, chars=[35,39], words=[7,7])),\n",
       " location_extraction(Span(\"b'Grand Rapids'\", sentence=22442, chars=[10,21], words=[2,3])),\n",
       " location_extraction(Span(\"b'PROVIDENCE'\", sentence=21436, chars=[14,23], words=[4,4])),\n",
       " location_extraction(Span(\"b'happy!I'\", sentence=25843, chars=[193,199], words=[37,37]))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cands = session.query(Location_Extraction).filter(Location_Extraction.split == 0).all()\n",
    "# # train_links = ( cc[0].sentence.document.name for cc in train_cands )\n",
    "# print(train_cands[14][0].get_parent())\n",
    "# print(train_cands[14])\n",
    "# print(train_cands[14].get_parent()._asdict()['ner_tags'])\n",
    "print(len(train_cands))\n",
    "train_cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "train_links = ( cc[0].sentence.document.name for cc in train_cands )\n",
    "print (len(set(list(train_links))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://phoenix.backpage.com/FemaleEscorts/your-sexy-little-secret-young-sweet-petite-20/26284112\n",
      "East Valley\n",
      "http://boston.backpage.com/FemaleEscorts/my-stp-dad-didnt-tuck-me-in-like-he-promised-_-so-now-i-cant-sleep_-hot-dirty-blonde-girl-22/27609932\n",
      "Boston\n",
      "http://boston.backpage.com/FemaleEscorts/my-stp-dad-didnt-tuck-me-in-like-he-promised-_-so-now-i-cant-sleep_-hot-dirty-blonde-girl-22/27609932\n",
      "South Shore - Braintree / Quincy\n",
      "http://boston.backpage.com/FemaleEscorts/my-stp-dad-didnt-tuck-me-in-like-he-promised-_-so-now-i-cant-sleep_-hot-dirty-blonde-girl-22/27609932\n",
      "South Shore - Braintree / Quincy\n",
      "http://boston.backpage.com/FemaleEscorts/my-stp-dad-didnt-tuck-me-in-like-he-promised-_-so-now-i-cant-sleep_-hot-dirty-blonde-girl-22/27609932\n",
      "Braintree\n",
      "http://boston.backpage.com/FemaleEscorts/my-stp-dad-didnt-tuck-me-in-like-he-promised-_-so-now-i-cant-sleep_-hot-dirty-blonde-girl-22/27609932\n",
      "South Shore - Braintree / Quincy\n",
      "http://sandiego.backpage.com/FemaleEscorts/classy-and-sexy-will-leave-u-wanting-more-im-here-to-please-u-24/17308287\n",
      "SERIOUS\n",
      "http://newyork.backpage.com/FemaleEscorts/ss-ss-and-more-ss_-phat-booty-on-deck_-50-spec-347-645-7057-23/49285066\n",
      "OR\n",
      "http://michigan.backpage.com/FemaleEscorts/best-kept-secret-in-ann-arbor60-specials-21/17148900\n",
      "Ann Arbor$60\n",
      "http://michigan.backpage.com/FemaleEscorts/best-kept-secret-in-ann-arbor60-specials-21/17148900\n",
      "Ann Arbor\n",
      "http://michigan.backpage.com/FemaleEscorts/best-kept-secret-in-ann-arbor60-specials-21/17148900\n",
      "Ypsilanti\n",
      "http://michigan.backpage.com/FemaleEscorts/best-kept-secret-in-ann-arbor60-specials-21/17148900\n",
      "Ann Arbor$60\n",
      "http://toronto.backpage.com/FemaleEscorts/150-hrnew-upscale-puterican-and-egyptian-mix-ebony-in-town-looking-2-get-knky-19/10837807\n",
      "Independent\n",
      "http://toronto.backpage.com/FemaleEscorts/150-hrnew-upscale-puterican-and-egyptian-mix-ebony-in-town-looking-2-get-knky-19/10837807\n",
      "Brampton\n",
      "http://phoenix.backpage.com/FemaleEscorts/incall-specials-beautiful-gorgeouse-puerto-rican-21/29872763\n",
      "Phoenix\n",
      "http://phoenix.backpage.com/FemaleEscorts/incall-specials-beautiful-gorgeouse-puerto-rican-21/29872763\n",
      "North Phoenix\n",
      "http://sanantonio.backpage.com/BodyRubs/sexy-latina-bombshell-early-bird-special-60-40/7488533\n",
      "San Antonio\n",
      "http://miami.backpage.com/FemaleEscorts/latinas-ricas-y-calenticahot-and-spicy-andrea-and-yvonne-newavailable-now305-508-8037-21/28005306\n",
      "Miami\n",
      "http://austin.backpage.com/FemaleEscorts/real-pics-great-skills-good-attitude-worth-every-dime-22/10336341\n",
      "North AUSTIN\n",
      "http://austin.backpage.com/FemaleEscorts/real-pics-great-skills-good-attitude-worth-every-dime-22/10336341\n",
      "Austin\n",
      "http://washingtondc.backpage.com/FemaleEscorts/curvaceous-bombshell-22/15757830\n",
      "Gaithersburg\n",
      "http://washingtondc.backpage.com/FemaleEscorts/curvaceous-bombshell-22/15757830\n",
      "DC\n",
      "http://washingtondc.backpage.com/FemaleEscorts/curvaceous-bombshell-22/15757830\n",
      "Rockville\n",
      "http://washingtondc.backpage.com/FemaleEscorts/curvaceous-bombshell-22/15757830\n",
      "DC\n",
      "http://washingtondc.backpage.com/FemaleEscorts/curvaceous-bombshell-22/15757830\n",
      "Southern Maryland\n",
      "http://washingtondc.backpage.com/FemaleEscorts/curvaceous-bombshell-22/15757830\n",
      "DC\n",
      "http://newyork.backpage.com/FemaleEscorts/new-york-asian-escorts-available-now-18/52777675\n",
      "New York\n",
      "http://newyork.backpage.com/FemaleEscorts/new-york-asian-escorts-available-now-18/52777675\n",
      "New York\n",
      "http://washington.backpage.com/BodyRubs/gorgeous-ebony-playmate-100-in-special-sexi-and-petite-36/15371637\n",
      "Earth\n",
      "http://washington.backpage.com/BodyRubs/gorgeous-ebony-playmate-100-in-special-sexi-and-petite-36/15371637\n",
      "Seattle\n",
      "http://washington.backpage.com/BodyRubs/gorgeous-ebony-playmate-100-in-special-sexi-and-petite-36/15371637\n",
      "Tacoma\n",
      "http://britishcolumbia.backpage.com/FemaleEscorts/coming-to-grand-prairie-classy-curvy-cougar-come-play-47/8161904\n",
      "WEST END\n",
      "http://newjersey.backpage.com/FemaleEscorts/incalls-arabicspanish-feminine-soft-skin-clean-smell-good-independent-201-972-0110-25/20022559\n",
      "North Jersey\n",
      "http://newjersey.backpage.com/FemaleEscorts/incalls-arabicspanish-feminine-soft-skin-clean-smell-good-independent-201-972-0110-25/20022559\n",
      "North Bergen\n",
      "http://westpalmbeach.backpage.com/FemaleEscorts/businessmen-you-will-love-your-vip-time-with-julia-blue-football-bachelor-poker-parties-too-42/27975372\n",
      "PALM CITY\n",
      "http://westpalmbeach.backpage.com/FemaleEscorts/businessmen-you-will-love-your-vip-time-with-julia-blue-football-bachelor-poker-parties-too-42/27975372\n",
      "PALM BEACH\n",
      "http://westpalmbeach.backpage.com/FemaleEscorts/businessmen-you-will-love-your-vip-time-with-julia-blue-football-bachelor-poker-parties-too-42/27975372\n",
      "Stuart\n",
      "http://westpalmbeach.backpage.com/FemaleEscorts/businessmen-you-will-love-your-vip-time-with-julia-blue-football-bachelor-poker-parties-too-42/27975372\n",
      "Jupiter\n",
      "http://westpalmbeach.backpage.com/FemaleEscorts/businessmen-you-will-love-your-vip-time-with-julia-blue-football-bachelor-poker-parties-too-42/27975372\n",
      "West Palm Beach\n",
      "http://westpalmbeach.backpage.com/FemaleEscorts/businessmen-you-will-love-your-vip-time-with-julia-blue-football-bachelor-poker-parties-too-42/27975372\n",
      "PBG\n",
      "http://montreal.backpage.com/FemaleEscorts/the-real-naughty-alysha-available-now-incall-and-outcall-438-827-5148-19/4198550\n",
      "Alysha\n",
      "http://montreal.backpage.com/FemaleEscorts/the-real-naughty-alysha-available-now-incall-and-outcall-438-827-5148-19/4198550\n",
      "Alysha\n",
      "http://montreal.backpage.com/FemaleEscorts/the-real-naughty-alysha-available-now-incall-and-outcall-438-827-5148-19/4198550\n",
      "Montreal\n",
      "http://alberta.backpage.com/FemaleEscorts/back-for-few-dayssexy-french-katrinabig-bootypretty-facetan-and-tightall-that-you-needreal-19/8913767\n",
      "Calgary\n",
      "http://alberta.backpage.com/FemaleEscorts/back-for-few-dayssexy-french-katrinabig-bootypretty-facetan-and-tightall-that-you-needreal-19/8913767\n",
      "NEED?*REAL?\n",
      "http://alberta.backpage.com/FemaleEscorts/back-for-few-dayssexy-french-katrinabig-bootypretty-facetan-and-tightall-that-you-needreal-19/8913767\n",
      "Calgary\n",
      "http://alberta.backpage.com/FemaleEscorts/back-for-few-dayssexy-french-katrinabig-bootypretty-facetan-and-tightall-that-you-needreal-19/8913767\n",
      "NEED?*REAL?\n",
      "http://ontario.backpage.com/FemaleEscorts/gorgeous-native-calista-yongecollege-you-cant-go-wrong-with-21/24314728\n",
      "Toronto\n",
      "http://calgary.backpage.com/FemaleEscorts/sweetest-girl-in-town-very-fun-and-great-attitude-27/8915873\n",
      "Calgary\n",
      "http://calgary.backpage.com/FemaleEscorts/sweetest-girl-in-town-very-fun-and-great-attitude-27/8915873\n",
      "Calgary\n",
      "http://ohio.backpage.com/FemaleEscorts/1-hottest-in-townon-duty-let-me-be-your-stress-reliever-24/11348043\n",
      "WeSt\n",
      "http://ohio.backpage.com/FemaleEscorts/1-hottest-in-townon-duty-let-me-be-your-stress-reliever-24/11348043\n",
      "Y.O.U\n",
      "http://ohio.backpage.com/FemaleEscorts/1-hottest-in-townon-duty-let-me-be-your-stress-reliever-24/11348043\n",
      "O.O\n",
      "http://ohio.backpage.com/FemaleEscorts/1-hottest-in-townon-duty-let-me-be-your-stress-reliever-24/11348043\n",
      "Cleveland\n",
      "http://ventura.backpage.com/FemaleEscorts/new-arrival-exotic-mixed-gorgeous-22/45788482\n",
      "Thousand Oaks\n",
      "http://ventura.backpage.com/FemaleEscorts/new-arrival-exotic-mixed-gorgeous-22/45788482\n",
      "Ventura\n",
      "http://manhattan.backpage.com/FemaleEscorts/outcalls-sexy-white-veronica-baby-347570-2002-22/51369692\n",
      "Manhattan\n",
      "http://manhattan.backpage.com/FemaleEscorts/outcalls-sexy-white-veronica-baby-347570-2002-22/51369692\n",
      "NEW YORK\n",
      "http://calgary.backpage.com/FemaleEscorts/c-u-b-a-n-and-j-a-m-a-i-c-a-n-24hrs-always-ready-to-please-great-service-20/8913778\n",
      "CALGARY\n",
      "http://calgary.backpage.com/FemaleEscorts/c-u-b-a-n-and-j-a-m-a-i-c-a-n-24hrs-always-ready-to-please-great-service-20/8913778\n",
      "NE\n",
      "http://calgary.backpage.com/FemaleEscorts/c-u-b-a-n-and-j-a-m-a-i-c-a-n-24hrs-always-ready-to-please-great-service-20/8913778\n",
      "Aliyah\n",
      "http://calgary.backpage.com/FemaleEscorts/c-u-b-a-n-and-j-a-m-a-i-c-a-n-24hrs-always-ready-to-please-great-service-20/8913778\n",
      "Calgary\n",
      "http://calgary.backpage.com/FemaleEscorts/c-u-b-a-n-and-j-a-m-a-i-c-a-n-24hrs-always-ready-to-please-great-service-20/8913778\n",
      "NE\n",
      "http://calgary.backpage.com/FemaleEscorts/c-u-b-a-n-and-j-a-m-a-i-c-a-n-24hrs-always-ready-to-please-great-service-20/8913778\n",
      "5'3\n",
      "http://calgary.backpage.com/FemaleEscorts/c-u-b-a-n-and-j-a-m-a-i-c-a-n-24hrs-always-ready-to-please-great-service-20/8913778\n",
      "CALGARY\n",
      "http://calgary.backpage.com/FemaleEscorts/c-u-b-a-n-and-j-a-m-a-i-c-a-n-24hrs-always-ready-to-please-great-service-20/8913778\n",
      "Calgary\n",
      "http://seoul.backpage.com/FemaleEscorts/focus-forgot-the-world-ready-to-serve-young-glamour-and-hight-society-available-for-you-in-seoul-24/2774898\n",
      "Seoul\n",
      "http://seoul.backpage.com/FemaleEscorts/focus-forgot-the-world-ready-to-serve-young-glamour-and-hight-society-available-for-you-in-seoul-24/2774898\n",
      "Seoul\n",
      "http://seoul.backpage.com/FemaleEscorts/focus-forgot-the-world-ready-to-serve-young-glamour-and-hight-society-available-for-you-in-seoul-24/2774898\n",
      "Seoul\n",
      "http://seoul.backpage.com/FemaleEscorts/focus-forgot-the-world-ready-to-serve-young-glamour-and-hight-society-available-for-you-in-seoul-24/2774898\n",
      "Seoul\n",
      "http://northjersey.backpage.com/FemaleEscorts/hot-colombian-doll-available-now-4-u-outcall-only-lets-have-fun-tonightask-about-specials-23/20022906\n",
      "North jersey\n",
      "http://northjersey.backpage.com/FemaleEscorts/hot-colombian-doll-available-now-4-u-outcall-only-lets-have-fun-tonightask-about-specials-23/20022906\n",
      "Lyndhurst\n",
      "http://northjersey.backpage.com/FemaleEscorts/hot-colombian-doll-available-now-4-u-outcall-only-lets-have-fun-tonightask-about-specials-23/20022906\n",
      "Lodi\n",
      "http://northjersey.backpage.com/FemaleEscorts/hot-colombian-doll-available-now-4-u-outcall-only-lets-have-fun-tonightask-about-specials-23/20022906\n",
      "Hoboken\n",
      "http://northjersey.backpage.com/FemaleEscorts/hot-colombian-doll-available-now-4-u-outcall-only-lets-have-fun-tonightask-about-specials-23/20022906\n",
      "Passaic\n",
      "http://northjersey.backpage.com/FemaleEscorts/hot-colombian-doll-available-now-4-u-outcall-only-lets-have-fun-tonightask-about-specials-23/20022906\n",
      "North Jersey\n",
      "http://halifax.backpage.com/FemaleEscorts/sexy-chick-new-to-halifax-in-and-outcalls-avail-all-night-long-25/1979455\n",
      "Halifax\n",
      "http://halifax.backpage.com/FemaleEscorts/sexy-chick-new-to-halifax-in-and-outcalls-avail-all-night-long-25/1979455\n",
      "@#@3\n",
      "http://southjersey.backpage.com/FemaleEscorts/100-real-hot-brunette-ready-to-play-23/20019984\n",
      "South Jersey\n",
      "http://southjersey.backpage.com/FemaleEscorts/100-real-hot-brunette-ready-to-play-23/20019984\n",
      "Natalie\n",
      "http://sanjose.backpage.com/FemaleEscorts/back-in-town-ready-too-play-60-special-call-me-22/19824366\n",
      "San Jose\n",
      "http://sanjose.backpage.com/FemaleEscorts/back-in-town-ready-too-play-60-special-call-me-22/19824366\n",
      "Town\n",
      "http://sanjose.backpage.com/FemaleEscorts/back-in-town-ready-too-play-60-special-call-me-22/19824366\n",
      "Town\n",
      "http://denver.classivox.com/t/female-escorts/-httpwww.highclasslist.comguideindex.php/418187/\n",
      "Anonymously</button\n",
      "http://northcarolina.backpage.com/FemaleEscorts/petetite-sexybombshell-meeya-20/18370732\n",
      "Greensboro\n",
      "http://southcoast.backpage.com/FemaleEscorts/lt-m-mk-ll-ur-fantaz-0m-2-f-x0x-7742971401-22/27619468\n",
      "Black\n",
      "http://southcoast.backpage.com/FemaleEscorts/lt-m-mk-ll-ur-fantaz-0m-2-f-x0x-7742971401-22/27619468\n",
      "South Coast\n",
      "http://denver.backpage.com/FemaleEscorts/hot-hot-azn-sensation-right-here-guys-playful-and-ready-for-you-20/16538395\n",
      "Denver\n",
      "http://denver.backpage.com/FemaleEscorts/hot-hot-azn-sensation-right-here-guys-playful-and-ready-for-you-20/16538395\n",
      "Denver\n",
      "http://denver.backpage.com/FemaleEscorts/hot-hot-azn-sensation-right-here-guys-playful-and-ready-for-you-20/16538395\n",
      "Colorado\n",
      "http://sandiego.backpage.com/FemaleEscorts/s-ey-naughtyupscale-hottie-100-real-exotiic-priincess-22/17309326\n",
      "San Diego\n",
      "http://philadelphia.backpage.com/FemaleEscorts/spanish-barbie-waiting-on-you-23/21060917\n",
      "Philadelphia\n",
      "http://atlanta.backpage.com/BodyRubs/forget-therestcall-thebest-sexy-23/25261070\n",
      "Atlanta\n",
      "http://losangeles.backpage.com/FemaleEscorts/fun-with-freaky-kinky-kali-k-60sp-100hhr-140hr-21/42620021\n",
      "Costa Mesa\n",
      "http://losangeles.backpage.com/FemaleEscorts/fun-with-freaky-kinky-kali-k-60sp-100hhr-140hr-21/42620021\n",
      "Town\n",
      "http://dc.backpage.com/BodyRubs/sensual-nude-body-on-body-by-erotic-blonde-bombshell-outcall-only-26/15202170\n",
      "District Of Columbia\n",
      "http://albuquerque.backpage.com/BodyRubs/100hr-100hr-incalls-only-therapeutic-special-9pm-12am-specials-100hr-100hr-100hr-25/5948529\n",
      "Christina\n",
      "http://albuquerque.backpage.com/BodyRubs/100hr-100hr-incalls-only-therapeutic-special-9pm-12am-specials-100hr-100hr-100hr-25/5948529\n",
      "Albuquerque\n",
      "http://dallas.backpage.com/FemaleEscorts/sexii-spinner-angel-with-cute-petite-body-n-amazing-green-eyes-26/29527718\n",
      "Dallas\n",
      "http://dallas.backpage.com/FemaleEscorts/sexii-spinner-angel-with-cute-petite-body-n-amazing-green-eyes-26/29527718\n",
      "SERIOUS\n",
      "http://dallas.backpage.com/FemaleEscorts/sexii-spinner-angel-with-cute-petite-body-n-amazing-green-eyes-26/29527718\n",
      "Dallas\n",
      "http://dallas.backpage.com/FemaleEscorts/sexii-spinner-angel-with-cute-petite-body-n-amazing-green-eyes-26/29527718\n",
      "Dallas\n",
      "http://tennessee.backpage.com/BodyRubs/hot-sexy-ebony-topless-or-nude-bodyrubs-mutual-touching-prostate-pleasures-27/6325058\n",
      "West Memphis\n",
      "http://tennessee.backpage.com/BodyRubs/hot-sexy-ebony-topless-or-nude-bodyrubs-mutual-touching-prostate-pleasures-27/6325058\n",
      "Memphis\n",
      "http://tennessee.backpage.com/BodyRubs/hot-sexy-ebony-topless-or-nude-bodyrubs-mutual-touching-prostate-pleasures-27/6325058\n",
      "Covington\n",
      "http://tennessee.backpage.com/BodyRubs/hot-sexy-ebony-topless-or-nude-bodyrubs-mutual-touching-prostate-pleasures-27/6325058\n",
      "Arlington\n",
      "http://tennessee.backpage.com/BodyRubs/hot-sexy-ebony-topless-or-nude-bodyrubs-mutual-touching-prostate-pleasures-27/6325058\n",
      "Collierville\n",
      "http://georgia.backpage.com/FemaleEscorts/50-juicy-booty-playmate-it-100better-n-person-22/24894188\n",
      "Peachtree City\n",
      "http://tampa.backpage.com/FemaleEscorts/super-sexy-smack-it-grab-it-pull-my-hair-come-get-this-friday-morning-delightpinellas-park-27/13357572\n",
      "PINELLAS PARK AREA\n",
      "http://connecticut.backpage.com/FemaleEscorts/trulygorgeous-extremelybusty-just-what-youve-been-looking-for-21/8476707\n",
      "TIME&COMPANIONSHIP\n",
      "http://connecticut.backpage.com/FemaleEscorts/trulygorgeous-extremelybusty-just-what-youve-been-looking-for-21/8476707\n",
      "New Haven\n",
      "http://atlanta.backpage.com/FemaleEscorts/247-50-well-reviewed-upscale-doll-face-spinner-w-perfect-rack-full-hr-special-21/23697163\n",
      "Atlanta\n",
      "http://atlanta.backpage.com/FemaleEscorts/247-50-well-reviewed-upscale-doll-face-spinner-w-perfect-rack-full-hr-special-21/23697163\n",
      "5'3\n",
      "http://ontario.backpage.com/FemaleEscorts/outcalls-cici-monroe-ddd-busty-wt-and-full-kissable-lips-21/20399339\n",
      "Ottawa\n",
      "http://ontario.backpage.com/FemaleEscorts/outcalls-cici-monroe-ddd-busty-wt-and-full-kissable-lips-21/20399339\n",
      "5'4\n",
      "http://minneapolis.backpage.com/FemaleEscorts/andgtandgt-visiting-andltandlt-hot-new-girls-back-in-town-omg-call-us-now-21/12995155\n",
      "Ivory\n",
      "http://everett.backpage.com/FemaleEscorts/edmonds-leaving-fri-noonapple-bottom-sexy-voluptuous-sensual-blueeyes-38ds-open-minded-35/18637433\n",
      "Web\n",
      "http://southjersey.backpage.com/FemaleEscorts/exotic-italian-sexy-but-sweet-pre-game-for-your-weekend-the-rite-way-24/20023059\n",
      "Sophie\n",
      "http://southjersey.backpage.com/FemaleEscorts/exotic-italian-sexy-but-sweet-pre-game-for-your-weekend-the-rite-way-24/20023059\n",
      "Sophie\n",
      "http://southjersey.backpage.com/FemaleEscorts/exotic-italian-sexy-but-sweet-pre-game-for-your-weekend-the-rite-way-24/20023059\n",
      "Atlantic City\n",
      "http://southjersey.backpage.com/FemaleEscorts/exotic-italian-sexy-but-sweet-pre-game-for-your-weekend-the-rite-way-24/20023059\n",
      "South Jersey\n",
      "http://southjersey.backpage.com/FemaleEscorts/exotic-italian-sexy-but-sweet-pre-game-for-your-weekend-the-rite-way-24/20023059\n",
      "South Jersey\n",
      "http://southjersey.backpage.com/FemaleEscorts/exotic-italian-sexy-but-sweet-pre-game-for-your-weekend-the-rite-way-24/20023059\n",
      "South Jersey\n",
      "http://ontario.backpage.com/FemaleEscorts/new-to-ottawa-statuesque-swedish-bombshell-visiting-until-sat-book-now-15304560-mins-available-34/24249886\n",
      "Ottawa\n",
      "http://ontario.backpage.com/FemaleEscorts/new-to-ottawa-statuesque-swedish-bombshell-visiting-until-sat-book-now-15304560-mins-available-34/24249886\n",
      "Ottawa\n",
      "http://ontario.backpage.com/FemaleEscorts/new-to-ottawa-statuesque-swedish-bombshell-visiting-until-sat-book-now-15304560-mins-available-34/24249886\n",
      "Ottawa\n",
      "http://ontario.backpage.com/FemaleEscorts/new-to-ottawa-statuesque-swedish-bombshell-visiting-until-sat-book-now-15304560-mins-available-34/24249886\n",
      "Statuesque\n",
      "http://ontario.backpage.com/FemaleEscorts/new-to-ottawa-statuesque-swedish-bombshell-visiting-until-sat-book-now-15304560-mins-available-34/24249886\n",
      "Statuesque\n",
      "http://alberta.backpage.com/FemaleEscorts/ultimate-pleasure-im-here-to-please-you-come-play-indulge-20/8916325\n",
      "FETISHES\n",
      "http://alberta.backpage.com/FemaleEscorts/ultimate-pleasure-im-here-to-please-you-come-play-indulge-20/8916325\n",
      "Serious\n",
      "http://alberta.backpage.com/FemaleEscorts/ultimate-pleasure-im-here-to-please-you-come-play-indulge-20/8916325\n",
      "Calgary\n",
      "http://southjersey.backpage.com/FemaleEscorts/andgt-andgt-andgt-andgt-andgt-andgt-andgt-omg-wow-hot-no-rush-petite-24-hours-24-hours-20/20023362\n",
      "Atlantic City\n",
      "http://southjersey.backpage.com/FemaleEscorts/andgt-andgt-andgt-andgt-andgt-andgt-andgt-omg-wow-hot-no-rush-petite-24-hours-24-hours-20/20023362\n",
      "South Jersey\n",
      "http://southjersey.backpage.com/FemaleEscorts/andgt-andgt-andgt-andgt-andgt-andgt-andgt-omg-wow-hot-no-rush-petite-24-hours-24-hours-20/20023362\n",
      "Nice\n",
      "http://southjersey.backpage.com/FemaleEscorts/andgt-andgt-andgt-andgt-andgt-andgt-andgt-omg-wow-hot-no-rush-petite-24-hours-24-hours-20/20023362\n",
      "Sweet\n",
      "http://kansas.backpage.com/FemaleEscorts/petiete-blonde-fun-loving-open-mindedblondes-do-have-more-fun-44/7251779\n",
      "Wichita\n",
      "http://pittsburgh.backpage.com/FemaleEscorts/young-petite-eboney-spinner-24/21063123\n",
      "GG\n",
      "http://pittsburgh.backpage.com/FemaleEscorts/young-petite-eboney-spinner-24/21063123\n",
      "Pittsburgh\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Los Angeles\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "San Mateo\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Santa Rosa\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Palmdale\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "San Francisco\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Fresno\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "California\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Napa\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "california\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Los Gatos\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Palo Alto\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Santa Barbara\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "North Bay\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Humboldt County\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Saratoga, South Lake Tahoe\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "San Gabriel Valley\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Sacramento\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Videos\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "California\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Mendocino County\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Oakland\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "california\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Santa Clara\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Ukrainian\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Bakersfield\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Cupertino\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "San Diego\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Imperial County\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "San Jose\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "california\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Orange County\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Welsh\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Modesto\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Santa Cruz\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Sunnyvale\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "San Fernando Valley\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "San Luis Obispo\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Long Beach\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Ventura\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Santa Maria\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Palm Springs\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Chico\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "address.\",\"systemErrorMessage\":\"\",\"hasValidationError\"\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "APPOINTMENT\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Visalia\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Romanian\n",
      "http://www.cityvibe.com/inlandempire/BodyRubs/carla-sensual-massage-outcall-to-your-home-hotel-i-am-independent/1658716\n",
      "Mountain View\n",
      "http://vancouver.backpage.com/FemaleEscorts/specials-langley-ru-rutt-22/8146609\n",
      "Vancouver\n",
      "http://edmonton.backpage.com/FemaleEscorts/double-the-trouble-double-the-fun-let-us-make-u-1-boo-24/8915710\n",
      "Vicki\n",
      "http://michigan.backpage.com/FemaleEscorts/sweet-n-sexy-available-now-incalloutcall-25/13888687\n",
      "Grand Rapids\n",
      "http://providence.backpage.com/FemaleEscorts/im-the-one-you-missing-and-im-here-to-play-22/4429157\n",
      "PROVIDENCE\n",
      "http://raleigh.backpage.com/FemaleEscorts/36d-sexy-sweetheart-30/3617016\n",
      "happy!I\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, c in enumerate(train_cands):\n",
    "        #pb.bar(i)\n",
    "        doc = c[0].sentence.document.name\n",
    "        extracted_cand = c[0].get_span()\n",
    "        print(doc)\n",
    "        print(extracted_cand)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.87 µs\n",
      "0\n",
      "[INFO] fonduer.udf - Clearing existing...\n",
      "[INFO] fonduer.udf - Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 14\n",
      "1\n",
      "[INFO] fonduer.udf - Clearing existing...\n",
      "[INFO] fonduer.udf - Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 12\n"
     ]
    }
   ],
   "source": [
    "# print(\"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == 0).count())\n",
    "\n",
    "\n",
    "%time\n",
    "for i, docs in enumerate([dev_docs, test_docs]):\n",
    "    print(i)\n",
    "    candidate_extractor.apply(docs, split=i+1)#, parallelism=4)\n",
    "    print(\"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == i+1).count())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### test location matcher:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "cands_test = session.query(Location_Extraction).filter(Location_Extraction.split == 1).all()\n",
    "\n",
    "test_links = ( cc[0].sentence.document.name for cc in cands_test )\n",
    "    #print(cc[0].sentence.document.name)\n",
    "print (len(set(test_links )))\n",
    "print (len(set(cands_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_train = session.query(Location_Extraction).filter(Location_Extraction.split == 0).all()\n",
    "train_links = (cc for cc in cand_train )\n",
    "    #print(cc[0].sentence.document.name)\n",
    "print (len(set(train_links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal Featurization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 7.87 µs\n",
      "[INFO] fonduer.udf - Clearing existing...\n",
      "[INFO] fonduer.udf - Running UDF...\n",
      "[INFO] fonduer.async_annotations - Copying location_extraction_feature_updates to postgres\n",
      "[INFO] fonduer.async_annotations - b'COPY 190\\n'\n",
      "(190, 3572)\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 8.82 µs\n",
      "[INFO] fonduer.udf - Clearing existing...\n",
      "[INFO] fonduer.udf - Running UDF...\n",
      "[INFO] fonduer.async_annotations - Copying location_extraction_feature_updates to postgres\n",
      "[INFO] fonduer.async_annotations - b'COPY 14\\n'\n",
      "(14, 3572)\n",
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 11 µs\n",
      "[INFO] fonduer.udf - Clearing existing...\n",
      "[INFO] fonduer.udf - Running UDF...\n",
      "[INFO] fonduer.async_annotations - Copying location_extraction_feature_updates to postgres\n",
      "[INFO] fonduer.async_annotations - b'COPY 12\\n'\n",
      "(12, 3572)\n"
     ]
    }
   ],
   "source": [
    "# Applying the featurizer (to get feature vector describing the input)\n",
    "from fonduer import BatchFeatureAnnotator\n",
    "session.rollback()\n",
    "featurizer = BatchFeatureAnnotator(Location_Extraction)\n",
    "# Running for train set -- replace_key_set = True!\n",
    "%time\n",
    "F_train = featurizer.apply(split=0, replace_key_set=True, parallelism=cfg['parallel'])\n",
    "session.rollback()\n",
    "print(F_train.shape)\n",
    "# Running for dev set -- replace_key_set = False! Uses same featuers as dev set\n",
    "%time \n",
    "F_dev = featurizer.apply(split=1, clear=True, replace_key_set=False, parallelism=cfg['parallel'])\n",
    "session.rollback()\n",
    "print(F_dev.shape)\n",
    "%time \n",
    "F_test = featurizer.apply(split=2, clear=True, replace_key_set=False, parallelism=cfg['parallel'])\n",
    "session.rollback()\n",
    "print(F_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Adding Gold Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_chtap_labels(session,candidate_class,gold_dict,annotator_name='gold'):\n",
    "\n",
    "    ak = session.query(GoldLabelKey).filter(GoldLabelKey.name == annotator_name).first()\n",
    "    if ak is None:\n",
    "        ak = GoldLabelKey(name=annotator_name)\n",
    "        session.add(ak)\n",
    "        print (\"line_1\")\n",
    "        session.commit()\n",
    "\n",
    "    candidates = session.query(candidate_class).all()\n",
    "    gold_dict = gold_dict \n",
    "    cand_total = len(candidates)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(\"Loading {} candidate labels\".format(cand_total))\n",
    "    pb = ProgressBar(cand_total)\n",
    "    labels = []\n",
    "    for i, c in enumerate(candidates):\n",
    "        pb.bar(i)\n",
    "        doc = c[0].sentence.document.name\n",
    "        extracted_cand = c[0].get_span()\n",
    "        ## check whethe doc_name is in gold_dict keys\n",
    "        if doc in gold_dict.keys():\n",
    "            v = gold_dict[doc]\n",
    "\n",
    "            if extracted_cand in v:\n",
    "                label = GoldLabel(candidate=c, key=ak, value=1)\n",
    "                print (label)\n",
    "            else:\n",
    "                label = GoldLabel(candidate=c, key=ak, value=-1)\n",
    "                print(label)\n",
    "            session.add(label)\n",
    "            labels.append(label)\n",
    "        ## if doc_name is not in gold_dict pass it\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    session.commit()\n",
    "    pb.close()\n",
    "    session.commit()\n",
    "    print(\"AnnotatorLabels created: %s\" % (len(labels), ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoldLabel (gold = -1)\n",
      "GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "[=                                       ] 1%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=                                       ] 2%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[==                                      ] 3%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[==                                      ] 4%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[===                                     ] 5%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[===                                     ] 6%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "[===                                     ] 7%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = 1)\n",
      "[====                                    ] 8%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = 1)\n",
      "[====                                    ] 9%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=====                                   ] 10%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=====                                   ] 11%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=====                                   ] 12%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "[======                                  ] 13%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[======                                  ] 14%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=======                                 ] 15%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=======                                 ] 16%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=======                                 ] 17%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = 1)\n",
      "[========                                ] 18%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[========                                ] 19%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "[=========                               ] 20%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=========                               ] 21%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=========                               ] 22%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = 1)\n",
      "[==========                              ] 23%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[==========                              ] 24%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[==========                              ] 25%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = 1)\n",
      "GoldLabel (gold = 1)\n",
      "[===========                             ] 26%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[===========                             ] 27%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[============                            ] 28%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "[============                            ] 29%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=============                           ] 30%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = 1)\n",
      "[=============                           ] 31%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = 1)\n",
      "GoldLabel (gold = 1)\n",
      "[=============                           ] 32%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[==============                          ] 33%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[==============                          ] 34%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = 1)\n",
      "[===============                         ] 35%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[===============                         ] 36%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "[===============                         ] 37%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[================                        ] 38%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[================                        ] 39%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = 1)\n",
      "[=================                       ] 40%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = 1)\n",
      "[=================                       ] 41%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = 1)\n",
      "[=================                       ] 42%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = 1)\n",
      "[==================                      ] 43%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[==================                      ] 44%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = 1)\n",
      "[===================                     ] 45%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "[===================                     ] 46%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = 1)\n",
      "[===================                     ] 47%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[====================                    ] 48%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[====================                    ] 49%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[====================                    ] 50%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=====================                   ] 51%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "[=====================                   ] 52%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[======================                  ] 53%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[======================                  ] 54%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=======================                 ] 55%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = 1)\n",
      "[=======================                 ] 56%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "[=======================                 ] 57%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[========================                ] 58%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[========================                ] 59%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=========================               ] 60%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=========================               ] 61%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "[=========================               ] 62%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[==========================              ] 63%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "[==========================              ] 64%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[===========================             ] 65%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[===========================             ] 66%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = 1)\n",
      "[===========================             ] 67%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[============================            ] 68%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[============================            ] 69%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=============================           ] 70%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=============================           ] 71%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=============================           ] 72%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "[==============================          ] 73%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[==============================          ] 74%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[==============================          ] 75%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[===============================         ] 76%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[===============================         ] 77%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[================================        ] 78%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[================================        ] 79%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=================================       ] 80%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=================================       ] 81%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=================================       ] 82%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[==================================      ] 83%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[==================================      ] 84%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[===================================     ] 85%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = 1)\n",
      "[===================================     ] 86%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[===================================     ] 87%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================================    ] 88%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[====================================    ] 89%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = 1)\n",
      "[=====================================   ] 90%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=====================================   ] 91%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[=====================================   ] 92%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "[======================================  ] 93%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[======================================  ] 94%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[======================================= ] 95%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[======================================= ] 96%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = 1)\n",
      "[======================================= ] 97%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "[========================================] 98%GoldLabel (gold = 1)\n",
      "GoldLabel (gold = -1)\n",
      "[========================================] 99%GoldLabel (gold = -1)\n",
      "GoldLabel (gold = -1)\n",
      "[========================================] 100%\n",
      "AnnotatorLabels created: 216\n"
     ]
    }
   ],
   "source": [
    "session.rollback()\n",
    "load_chtap_labels(session,Location_Extraction,gold_dict_ , annotator_name='gold')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t-1\n",
      "  (1, 0)\t-1\n",
      "  (2, 0)\t-1\n",
      "  (3, 0)\t-1\n",
      "  (4, 0)\t1\n",
      "  (5, 0)\t-1\n",
      "  (6, 0)\t-1\n",
      "  (7, 0)\t-1\n",
      "  (8, 0)\t-1\n",
      "  (9, 0)\t1\n",
      "  (10, 0)\t-1\n",
      "  (11, 0)\t1\n",
      "  (12, 0)\t-1\n",
      "  (13, 0)\t-1\n",
      "Dev Set Balance: 21.43 Percent Positive\n",
      "Test Set Balance: 25.00 Percent Positive\n",
      "  (0, 0)\t-1\n",
      "  (1, 0)\t-1\n",
      "  (2, 0)\t-1\n",
      "  (3, 0)\t-1\n",
      "  (4, 0)\t-1\n",
      "  (5, 0)\t1\n",
      "  (6, 0)\t1\n",
      "  (7, 0)\t-1\n",
      "  (8, 0)\t1\n",
      "  (9, 0)\t-1\n",
      "  (10, 0)\t-1\n",
      "  (11, 0)\t-1\n"
     ]
    }
   ],
   "source": [
    "from fonduer.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "print (L_gold_dev)\n",
    "print('Dev Set Balance: %0.2f Percent Positive' % (100*np.sum(L_gold_dev == 1)/L_gold_dev.shape[0]))\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "print('Test Set Balance: %0.2f Percent Positive' % (100*np.sum(L_gold_test == 1)/L_gold_test.shape[0]))\n",
    "print (L_gold_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Creating LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " cand_dev = session.query(Location_Extraction).filter(Location_Extraction.split == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for true/false/abstain\n",
    "TRUE,FALSE,ABSTAIN = 1,-1,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.lf_helpers import *\n",
    "import re\n",
    "# Defining LFs\n",
    "# LF API is here: http://web.stanford.edu/~lwhsiao/api/\n",
    "\n",
    "def LF_in_breadcrumbs_1(c):\n",
    "    parent_text = c.get_parent().text\n",
    "    return FALSE if '>' in parent_text else ABSTAIN\n",
    "\n",
    "def LF_long_candidate(c):\n",
    "    parent_text = c.get_parent().text\n",
    "    return FALSE if len(parent_text) > 1000 else ABSTAIN\n",
    "\n",
    "def LF_common_real_words(c):\n",
    "    reg_pos = re.compile('other cities|since|day|escorts',re.IGNORECASE)\n",
    "    if reg_pos.search(c.get_parent().text):\n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "#def LF_in_breadcrumbs_2(c):\n",
    "#    attributes = list(get_attributes(c))\n",
    "#    return TRUE if ('class=breadcrumbs'in attributes) or ('class=inside_scroll' in attributes) else ABSTAIN\n",
    "\n",
    "def LF_head_in_tag(c):\n",
    "    tags = list(get_ancestor_tag_names(c))\n",
    "    return FALSE if 'head' in tags else TRUE\n",
    "\n",
    "def LF_body_in_tag(c):\n",
    "    tags = list(get_ancestor_tag_names(c))\n",
    "    return TRUE if 'body' in tags else FALSE\n",
    "\n",
    "def LF_table_in_tag(c):\n",
    "    tags = list(get_ancestor_tag_names(c))\n",
    "    return TRUE if 'table' in tags else ABSTAIN\n",
    "\n",
    "def LF_to_left(c):\n",
    "    return TRUE if overlap(\n",
    "      ['location','locall','outcall','stay','live','available','female escort'], \n",
    "        get_left_ngrams(c, window=3)) else FALSE\n",
    "\n",
    "def LF_to_right(c):\n",
    "    return TRUE if overlap(\n",
    "      ['escorts','incall','outcall','stay','live','available','female escort'], \n",
    "        list(get_right_ngrams(c, window=5))) else ABSTAIN\n",
    "# Need more of these...can check tutorials for inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect LFs in list\n",
    "lfs_location = [LF_in_breadcrumbs_1,\n",
    "                #LF_in_breadcrumbs_2,\n",
    "                LF_head_in_tag,\n",
    "                #LF_body_in_tag,\n",
    "                LF_to_right,\n",
    "                #LF_to_left,\n",
    "                LF_table_in_tag,\n",
    "                LF_long_candidate,\n",
    "                LF_common_real_words]\n",
    "print (lfs_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Running Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer import BatchLabelAnnotator\n",
    "\n",
    "# Annotating candidats using LFs (clear=True replaced existing)\n",
    "labeler = BatchLabelAnnotator(Location_Extraction, lfs=lfs_location)\n",
    "%time L_train = labeler.apply(split=0, clear=True, parallelism=cfg['parallel'],update_keys =True)\n",
    "%time L_dev = labeler.apply(split=1, clear=True, parallelism=cfg['parallel'],update_keys =True)\n",
    "%time L_test = labeler.apply(split=2, clear=True, parallelism=cfg['parallel'],update_keys =True)\n",
    "print(L_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a)Computing Individual LF Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.snorkel.lf_helpers import test_LF\n",
    "for lf in lfs_location:\n",
    "    print(lf.__name__)\n",
    "    tp, fp, tn, fn = test_LF(session, lf, split=1, annotator_name='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing a candidate from dev set\n",
    "ind = 0\n",
    "print(L_dev.get_candidate(session, ind))\n",
    "print(L_gold_dev[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading assessing LF performance vs. gold labels\n",
    "from fonduer.snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "%time L_dev.lf_stats(L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "from snorkel.learning import RandomSearch\n",
    "\n",
    "param_ranges = {\n",
    "    'step_size' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "    'decay' : [1.0, 0.95, 0.9],\n",
    "    'epochs' : [20, 50, 100]\n",
    "}\n",
    "\n",
    "searcher = RandomSearch(GenerativeModel, param_ranges, L_train, n=15)\n",
    "\n",
    "%time\n",
    "gen_model, run_stats = searcher.fit(L_dev, L_gold_dev)\n",
    "run_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and plotting training marginals\n",
    "train_marginals = gen_model.marginals(L_train)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing LF accuracies\n",
    "gen_model.weights.lf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pringint LF stats post-learning\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "prec, rec, f1 = gen_model.score(L_dev, L_gold_dev)\n",
    "L_dev.lf_stats(L_gold_dev, gen_model.weights.lf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "\n",
    "disc_model = SparseLogisticRegression\n",
    "param_ranges = {\n",
    "    'lr' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "    'dropout' : [0.0, 0.5]\n",
    "}\n",
    "\n",
    "model_hyperparams = {\n",
    "    'n_epochs' : 200,\n",
    "    'rebalance' : 0.5,\n",
    "    'print_freq' : 25\n",
    "}\n",
    "\n",
    "# We now add a session and probabilistic labels, as well as pass in the candidates\n",
    "# instead of the label matrix\n",
    "searcher = RandomSearch(disc_model, param_ranges, F_train, Y_train=train_marginals, n=15,\n",
    "    model_hyperparams=model_hyperparams)\n",
    "\n",
    "# We now pass in the development candidates and the gold development labels\n",
    "trained_model, run_stats = searcher.fit(F_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate discriminative on test set \n",
    "L_gold_test = np.array(load_gold_labels(session, annotator_name='gold', split=2).todense()).squeeze()\n",
    "# Get candidates, discriminative model outputs, and discriminative model predicts\n",
    "test_candidates = [F_test.get_candidate(session, i) for i in range(F_test.shape[0])]\n",
    "test_score = np.array(trained_model.predictions(F_test))\n",
    "true_pred = [test_candidates[_] for _ in np.nditer(np.where(test_score > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_gold_test\n",
    "corr = [ test_score[i] == L_gold_test[i] for i in range(len(test_score))]\n",
    "acc = np.sum(corr)/len(corr)\n",
    "print (acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing values \n",
    "gen_model_preds = (gen_model.marginals(L_test)>0.5)*2-1\n",
    "gen_corr = [ gen_model_preds[i] == L_gold_test[i] for i in range(len(gen_model_preds))]\n",
    "gen_acc = np.sum(gen_corr)/len(gen_corr)\n",
    "print(gen_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing values\n",
    "L_gold_dev = np.array(load_gold_labels(session, annotator_name='gold', split=1).todense()).squeeze()\n",
    "gen_model_preds = (gen_model.marginals(L_dev)>0.5)*2-1\n",
    "gen_corr = [ gen_model_preds[i] == L_gold_dev[i] for i in range(len(gen_model_preds))]\n",
    "gen_acc = np.sum(gen_corr)/len(gen_corr)\n",
    "print(gen_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Creating and Saving Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#getting google place and geocoding APIs\n",
    "import googlemaps as gm\n",
    "import gmaps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import MultiPoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "maps_api_key = 'AIzaSyA0Veo5Lc6JOwDjNgQvPEhQB4AiZcrYQGI'\n",
    "gmaps.configure(api_key=maps_api_key)\n",
    "\n",
    "def get_possible_locations(plc):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    plc: string describing place to match\n",
    "\n",
    "    OUTPUTS\n",
    "    qo: full json structure returned from API call\n",
    "    cl: list of candidate location strings\n",
    "    \"\"\" \n",
    "    api_key = 'AIzaSyDbk3lLZHuQVKDRBN99_oz-p4AJjIzhA0w'\n",
    "    gms = gm.Client(key=api_key)\n",
    "    qo = gm.places.places_autocomplete(gms,plc)\n",
    "    cl = [a['description'] for a in qo]\n",
    "    return qo,cl\n",
    "\n",
    "def get_geocode(plc):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    plc: string describing place to match\n",
    "\n",
    "    OUTPUTS\n",
    "    qo full json structure returned from API call\n",
    "    (lat,lon): lat-lon tuple\n",
    "    \"\"\"\n",
    "    api_key = 'AIzaSyBlLyOaasYMgMxFGUh2jJyxIG0_pZFF_jM'\n",
    "    gms = gm.Client(key=api_key)\n",
    "    qo = gm.geocoding.geocode(gms,plc)\n",
    "    lat = qo[0]['geometry']['location']['lat']\n",
    "    lng = qo[0]['geometry']['location']['lng']\n",
    "    return qo,(lat,lng)\n",
    "\n",
    "def slice_pd_by_cont(dfm,col,val,pres=True,lower=False,union=False):\n",
    "    \"\"\"\n",
    "    Returns dataframe where column values include/exclude values in provided list\n",
    "    \n",
    "    INPUTS:\n",
    "    dfm: dataframe\n",
    "    col: column header\n",
    "    val: list of strings to include/ignore\n",
    "    pres: true to include, false to exclude\n",
    "    union: include union of these values\n",
    "    \"\"\"\n",
    "    if union:\n",
    "        val = ['|'.join(val)]\n",
    "    for vl in val:\n",
    "        if ~lower:\n",
    "            if pres:\n",
    "                dfm = dfm.loc[dfm[col].str.contains(vl,na=False)]\n",
    "            else:\n",
    "                dfm = dfm.loc[~dfm[col].str.contains(vl,na=False)]\n",
    "        else:\n",
    "            if pres:\n",
    "                dfm = dfm.loc[dfm[col].str.lower().str.contains(vl,na=False)]\n",
    "            else:\n",
    "                dfm = dfm.loc[~dfm[col].str.lower().str.contains(vl,na=False)]\n",
    "    return dfm\n",
    "\n",
    "def map_candidates_and_centroid(dfm):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    dfm: dataframe containing at least latitude, longitude\n",
    "    \n",
    "    OUTPUT\n",
    "    centroid: np array of lat/lon of location centroid\n",
    "    \"\"\"\n",
    "    df_cans = dfm\n",
    "    df_cans_map = dfm[['latitude','longitude']]\n",
    "    df_cans['lat_long'] = df_cans[['latitude', 'longitude']].apply(tuple, axis=1)\n",
    "    point_tup_lst = df_cans['lat_long'].tolist()\n",
    "    points = MultiPoint(point_tup_lst)\n",
    "    cent = np.array(points.centroid)\n",
    "    cent_df = pd.DataFrame([cent]) #this is a rough centroid estimate\n",
    "    fig = gmaps.Map()\n",
    "    can_layer = gmaps.symbol_layer(\n",
    "    df_cans_map, fill_color=\"green\", stroke_color=\"green\", scale=2)\n",
    "    cent_layer = gmaps.symbol_layer(\n",
    "    cent_df, fill_color=\"red\", stroke_color=\"red\", scale=2)\n",
    "    fig.add_layer(can_layer)\n",
    "    fig.add_layer(cent_layer)\n",
    "    fig\n",
    "    return cent,fig\n",
    "\n",
    "def get_attr(obj):\n",
    "    out = [a for a in dir(obj) if not a.startswith('__') and not callable(getattr(obj,a))]\n",
    "    return out\n",
    "\n",
    "def most_common(lt):\n",
    "    data = Counter(lt)\n",
    "    return data.most_common(1)[0][0]\n",
    "\n",
    "def get_common_country(lt):\n",
    "    country_lst = []\n",
    "    country_els = []\n",
    "    for it in lt:\n",
    "        try:\n",
    "            country = pycountry.countries.lookup(it.lower())\n",
    "            country_lst.append(country.alpha_3)\n",
    "            country_els.append(it)\n",
    "        except:\n",
    "            country = None \n",
    "    if country_lst == []:\n",
    "        return 'none',[],[]\n",
    "    return most_common(country_lst),country_lst, country_els\n",
    "\n",
    "def get_common_state(lt):\n",
    "    state_lst = []\n",
    "    state_els = []\n",
    "    for it in lt:\n",
    "        sts = [a.lower() for a in state_add_dict.keys()]\n",
    "        abbs = [a.lower() for a in state_add_dict.values()]\n",
    "        if it in sts:\n",
    "            state_lst.append(it)\n",
    "            state_els.append(it)\n",
    "        elif it in abbs:\n",
    "            state_lst.append(state_dict[it])\n",
    "            state_els.append(it)\n",
    "    if state_lst == []:\n",
    "        return 'none',[],[]\n",
    "    else:\n",
    "        return most_common(state_lst), state_lst, state_els\n",
    "\n",
    "def get_possible_locale(lt,cn,st,cn_lst,st_lst):\n",
    "    locale_list = []\n",
    "    a = [b for b in lt if b not in cn_lst and b not in st_lst]\n",
    "    for b in a:\n",
    "        locales = get_possible_locations(b)\n",
    "        locales = [c for c in locales if cn in b and st in b]\n",
    "        locale_list.append(locales)\n",
    "    return locale_list\n",
    "\n",
    "# Need to unify this!\n",
    "def lookup_state_abbrev(cn):\n",
    "    try:\n",
    "        out = state_add_dict[cn]\n",
    "    except:\n",
    "        out = 'no state'\n",
    "    return out\n",
    "\n",
    "state_dict = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "}\n",
    "state_add_dict = {v: k for k, v in state_dict.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_extractions = defaultdict(list)\n",
    "num_test_cands = F_test.shape[0]\n",
    "test_cand_preds = (gen_model.marginals(L_test)>0.5)*2-1\n",
    "for ind in range(num_test_cands):\n",
    "    cand = F_test.get_candidate(session,ind)\n",
    "    parent = cand.get_parent()\n",
    "    doc_name = parent.document.name\n",
    "    # Initializing key if it doesn't exist\n",
    "    doc_extractions[doc_name]\n",
    "    loc = cand.location.get_span().lower()\n",
    "    if test_cand_preds[ind] == 1:\n",
    "        doc_extractions[doc_name].append(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_extractions = defaultdict(set)\n",
    "for doc_name, extract_list in doc_extractions.items():\n",
    "    out_extractions[doc_name] = list(set(extract_list))\n",
    "df_out = pd.DataFrame()\n",
    "df_out = df_labeled[df_labeled['file name'].isin(list(out_extractions.keys()))]\n",
    "df_out['extracted_location'] = df_out.apply(lambda row: out_extractions[row['file name']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv('../output/location_extractions.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_locales = defaultdict(list)\n",
    "for doc_name, extract_list in doc_extractions.items():\n",
    "    \n",
    "    # Getting country names\n",
    "    probable_country,country_list, country_els = get_common_country(extract_list)\n",
    "    if lookup_country_alpha3(probable_country) == 'USA' and len(extract_list) >1:\n",
    "        # Getting state names\n",
    "        probable_state,state_list,state_els = get_common_state(extract_list)\n",
    "    else:\n",
    "        probable_state,state_list,state_els = 'none',[],[]\n",
    "    \n",
    "    locale_list = []\n",
    "    a = [b for b in extract_list if b not in country_els and b not in state_els] #need lookup here\n",
    "    print(a)\n",
    "    if a == []:\n",
    "        if probable_state != 'none' and probable_country != 'none' and a == []:\n",
    "            locale_list = ['none,none,'+state_add_dict[probable_state]+','+probable_country]\n",
    "    else:\n",
    "        most_common_locale = most_common(a)\n",
    "        aset = list(set(a))\n",
    "        for b in aset:\n",
    "                locale_tmp = []\n",
    "                qo,locales = get_possible_locations(b)\n",
    "                not_exact = 1\n",
    "                count = 0\n",
    "                while not_exact and count<len(locales):\n",
    "                    print('Checking Locale %d of %d' %(count,len(locales)))\n",
    "                    c = locales[count]\n",
    "                    spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
    "                    import pdb; pdb.set_trace()\n",
    "                    if lookup_country_name(probable_country).lower() in spl:\n",
    "                        if lookup_state_abbrev(probable_state).lower() in spl: \n",
    "                            if spl[0].lower() == most_common_locale.lower() and len(spl) == 3:\n",
    "                                locale_list = ['none']+spl\n",
    "                                locale_list = [','.join(locale_list)]\n",
    "                                not_exact = 0\n",
    "                                print('Exact City Found')\n",
    "                            elif spl[0].lower() == most_common_locale.lower() and len(spl) == 4:\n",
    "                                locale_list = [','.join(spl)]\n",
    "                                not_exact = 0\n",
    "                                print('Exact Location Found')\n",
    "                            else:             \n",
    "                                locale_list.append(','.join(spl))  \n",
    "                                count = count+1\n",
    "                        else:\n",
    "                            count = count+1         \n",
    "                    else:\n",
    "                        count = count+1\n",
    "        \n",
    "    import pdb; pdb.set_trace()\n",
    "    #reformatting for labeling comparison\n",
    "    locale_list_out = []\n",
    "    for c in locale_list:\n",
    "        b = c.split(',')\n",
    "        print(b)\n",
    "        b[-1] = str(lookup_country_alpha3(b[-1]).lower())\n",
    "        b[-2] = state_dict[b[-2].upper()].lower()\n",
    "        locale_list_out.append(','.join(b)) \n",
    "    out_locales[doc_name] = locale_list_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in out_locales.keys():\n",
    "    if out_locales[ii] == []:\n",
    "        out_locales[ii] = ['none','none','none','none']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
