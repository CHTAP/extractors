{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Parsing Files, Adding Candidates and Labels to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Loading config\n",
    "with open(\"run_config_memex_gold_loc_file.json\") as fl:\n",
    "    cfg = json.load(fl)\n",
    "cfg_params = cfg['parameters']\n",
    "\n",
    "# Setting snorkel path and output root\n",
    "import os\n",
    "from os.path import join\n",
    "output_root = join(cfg_params['output_path'],cfg_params['experiment_name'])\n",
    "\n",
    "# Old import grammar\n",
    "os.environ['FONDUERDBNAME'] = cfg['postgres_db_name']\n",
    "os.environ['SNORKELDB'] = join(cfg['postgres_location'],os.environ['FONDUERDBNAME'])\n",
    "\n",
    "# For loading input files\n",
    "import pandas as pd\n",
    "\n",
    "# For running Snorkel\n",
    "from fonduer import SnorkelSession\n",
    "from fonduer.models import candidate_subclass\n",
    "from fonduer import HTMLPreprocessor, OmniParser\n",
    "#from fonduer import Meta\n",
    "from utils import MEMEXJsonPreprocessor, HTMLListPreprocessor, MEMEXJsonLGZIPPreprocessor\n",
    "\n",
    "#old snorkel imports\n",
    "#from snorkel.contrib.fonduer import SnorkelSession\n",
    "#from snorkel.contrib.fonduer.models import candidate_subclass\n",
    "#from snorkel.contrib.fonduer import HTMLPreprocessor, OmniParser\n",
    "#from utils import HTMLListPreprocessor, MEMEXJsonPreprocessor\n",
    "\n",
    "#from sqlalchemy import create_engine\n",
    "#snorkeldb = create_engine(os.environ['SNORKELDB'], isolation_level=\"AUTOCOMMIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up jsons\n",
    "# Load up content.tsv for gold labels?\n",
    "# Run doc preprocessor on jsons to get raw content\n",
    "\n",
    "#Creating path to labeled data\n",
    "pth_labeled = cfg['labeled_data_path']\n",
    "\n",
    "# Getting labeled data file name\n",
    "fl_labeled = cfg['labeled_data_file']\n",
    "\n",
    "# Loading labeled data into dataframe\n",
    "#df_labeled = pd.read_csv(join(pth_labeled,fl_labeled),sep='\\t',names=['ind','source','type','url','content','extractions'])\n",
    "df_labeled = pd.read_csv(join(pth_labeled,fl_labeled),sep=',',names=['ind','ind_old','source','type','url','content','extractions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled['extractions'] = df_labeled['extractions'].apply(lambda x: x.replace('\"', '').strip().split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ind</th>\n",
       "      <th>ind_old</th>\n",
       "      <th>source</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>extractions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74999560</td>\n",
       "      <td>backpage</td>\n",
       "      <td>ads</td>\n",
       "      <td>http://ottawa.backpage.com/FemaleEscorts/sabri...</td>\n",
       "      <td>LOOKING FOR QUALITY? THEN I'M YOUR GIRL! I am ...</td>\n",
       "      <td>{\"post_date\": \"2016-01-08\", \"title\": \"Sabrinaüíï...</td>\n",
       "      <td>[Ottawa,  Ontario]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>74999563</td>\n",
       "      <td>backpage</td>\n",
       "      <td>ads</td>\n",
       "      <td>http://indiana.backpage.com/FemaleEscorts/inde...</td>\n",
       "      <td>?? 100 % INDEPENDENT ??\\\\n&lt;br&gt; Sweet, Always d...</td>\n",
       "      <td>{\"location\": \"Indiana\", \"phone\": \"(317) 372-48...</td>\n",
       "      <td>[Indiana]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>74999564</td>\n",
       "      <td>backpage</td>\n",
       "      <td>ads</td>\n",
       "      <td>http://melbourne.backpage.com/BodyRubs/sexual-...</td>\n",
       "      <td>HI GENTLEMEN, ARE YOU LOOKING FOR SOME PEACE O...</td>\n",
       "      <td>{\"title\": \"Sexual Healing NUDE  TANTRIC Massag...</td>\n",
       "      <td>[Melbourne,  Australia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>74999566</td>\n",
       "      <td>backpage</td>\n",
       "      <td>ads</td>\n",
       "      <td>http://dallas.backpage.com/BodyRubs/latina-the...</td>\n",
       "      <td>Hello Im Ashley Professional Therapist Incalls...</td>\n",
       "      <td>{\"post_date\": \"2015-07-03\", \"phone\": \"(469) 63...</td>\n",
       "      <td>[Dallas,  Texas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>74999568</td>\n",
       "      <td>backpage</td>\n",
       "      <td>ads</td>\n",
       "      <td>http://louisiana.backpage.com/FemaleEscorts/yo...</td>\n",
       "      <td>&lt;iframe src=\\\\http://www.adultsearch.com/class...</td>\n",
       "      <td>{\"age\": \"19\", \"post_date\": \"2014-05-29\", \"loca...</td>\n",
       "      <td>[Louisiana]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>74999569</td>\n",
       "      <td>backpage</td>\n",
       "      <td>ads</td>\n",
       "      <td>http://mumbai.backpage.com/FemaleEscorts/sange...</td>\n",
       "      <td>Hello Friends !!\\\\n&lt;br&gt; I am Sangeeta Thomas, ...</td>\n",
       "      <td>{\"location\": \"Mumbai, India\", \"ethnicity\": \"fr...</td>\n",
       "      <td>[Mumbai,  India]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>74999570</td>\n",
       "      <td>backpage</td>\n",
       "      <td>ads</td>\n",
       "      <td>http://hyderabad.backpage.com/FemaleEscorts/th...</td>\n",
       "      <td>&lt;b&gt;&lt;/b&gt;THE BEST TOLLY WOOD ESCORT STUDIO ESCOR...</td>\n",
       "      <td>{\"age\": \"22\", \"post_date\": \"2014-02-01\", \"loca...</td>\n",
       "      <td>[Hyderabad,  India]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>74999574</td>\n",
       "      <td>backpage</td>\n",
       "      <td>ads</td>\n",
       "      <td>http://charlestonwv.backpage.com/FemaleEscorts...</td>\n",
       "      <td>Looking for Mature clean businessmen only! Im ...</td>\n",
       "      <td>{\"age\": \"26\", \"phone\": \"(304) 627-4355\", \"loca...</td>\n",
       "      <td>[Charleston,  West Virginia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>74999578</td>\n",
       "      <td>backpage</td>\n",
       "      <td>ads</td>\n",
       "      <td>http://washington.backpage.com/FemaleEscorts/p...</td>\n",
       "      <td>Hey I'm \\\\n&lt;b&gt;Annabelle&lt;/b&gt;. I'm down to earth...</td>\n",
       "      <td>{\"post_date\": \"2016-01-26\", \"title\": \"‚¨õÔ∏è‚óºÔ∏è‚óºÔ∏è üçí...</td>\n",
       "      <td>[Washington]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>74999580</td>\n",
       "      <td>backpage</td>\n",
       "      <td>ads</td>\n",
       "      <td>http://kc.backpage.com/BodyRubs/jingle_bells/1...</td>\n",
       "      <td>JOIN ME FOR WARMTH !\\\\n&lt;br&gt; \\\\n&lt;br&gt; I ENJOY TH...</td>\n",
       "      <td>{\"title\": \"jingle_bells (  *  )(  *  ) - Kansa...</td>\n",
       "      <td>[Kansas City,  Missouri]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ind   ind_old source  \\\n",
       "2   74999560  backpage    ads   \n",
       "5   74999563  backpage    ads   \n",
       "6   74999564  backpage    ads   \n",
       "8   74999566  backpage    ads   \n",
       "10  74999568  backpage    ads   \n",
       "11  74999569  backpage    ads   \n",
       "12  74999570  backpage    ads   \n",
       "16  74999574  backpage    ads   \n",
       "20  74999578  backpage    ads   \n",
       "22  74999580  backpage    ads   \n",
       "\n",
       "                                                 type  \\\n",
       "2   http://ottawa.backpage.com/FemaleEscorts/sabri...   \n",
       "5   http://indiana.backpage.com/FemaleEscorts/inde...   \n",
       "6   http://melbourne.backpage.com/BodyRubs/sexual-...   \n",
       "8   http://dallas.backpage.com/BodyRubs/latina-the...   \n",
       "10  http://louisiana.backpage.com/FemaleEscorts/yo...   \n",
       "11  http://mumbai.backpage.com/FemaleEscorts/sange...   \n",
       "12  http://hyderabad.backpage.com/FemaleEscorts/th...   \n",
       "16  http://charlestonwv.backpage.com/FemaleEscorts...   \n",
       "20  http://washington.backpage.com/FemaleEscorts/p...   \n",
       "22  http://kc.backpage.com/BodyRubs/jingle_bells/1...   \n",
       "\n",
       "                                                  url  \\\n",
       "2   LOOKING FOR QUALITY? THEN I'M YOUR GIRL! I am ...   \n",
       "5   ?? 100 % INDEPENDENT ??\\\\n<br> Sweet, Always d...   \n",
       "6   HI GENTLEMEN, ARE YOU LOOKING FOR SOME PEACE O...   \n",
       "8   Hello Im Ashley Professional Therapist Incalls...   \n",
       "10  <iframe src=\\\\http://www.adultsearch.com/class...   \n",
       "11  Hello Friends !!\\\\n<br> I am Sangeeta Thomas, ...   \n",
       "12  <b></b>THE BEST TOLLY WOOD ESCORT STUDIO ESCOR...   \n",
       "16  Looking for Mature clean businessmen only! Im ...   \n",
       "20  Hey I'm \\\\n<b>Annabelle</b>. I'm down to earth...   \n",
       "22  JOIN ME FOR WARMTH !\\\\n<br> \\\\n<br> I ENJOY TH...   \n",
       "\n",
       "                                              content  \\\n",
       "2   {\"post_date\": \"2016-01-08\", \"title\": \"Sabrinaüíï...   \n",
       "5   {\"location\": \"Indiana\", \"phone\": \"(317) 372-48...   \n",
       "6   {\"title\": \"Sexual Healing NUDE  TANTRIC Massag...   \n",
       "8   {\"post_date\": \"2015-07-03\", \"phone\": \"(469) 63...   \n",
       "10  {\"age\": \"19\", \"post_date\": \"2014-05-29\", \"loca...   \n",
       "11  {\"location\": \"Mumbai, India\", \"ethnicity\": \"fr...   \n",
       "12  {\"age\": \"22\", \"post_date\": \"2014-02-01\", \"loca...   \n",
       "16  {\"age\": \"26\", \"phone\": \"(304) 627-4355\", \"loca...   \n",
       "20  {\"post_date\": \"2016-01-26\", \"title\": \"‚¨õÔ∏è‚óºÔ∏è‚óºÔ∏è üçí...   \n",
       "22  {\"title\": \"jingle_bells (  *  )(  *  ) - Kansa...   \n",
       "\n",
       "                     extractions  \n",
       "2             [Ottawa,  Ontario]  \n",
       "5                      [Indiana]  \n",
       "6        [Melbourne,  Australia]  \n",
       "8               [Dallas,  Texas]  \n",
       "10                   [Louisiana]  \n",
       "11              [Mumbai,  India]  \n",
       "12           [Hyderabad,  India]  \n",
       "16  [Charleston,  West Virginia]  \n",
       "20                  [Washington]  \n",
       "22      [Kansas City,  Missouri]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3017 JSON files indicated\n"
     ]
    }
   ],
   "source": [
    "#Load html data from json files\n",
    "path_unlabeled = cfg['unlabeled_data_path']\n",
    "fl_unlabeled = cfg['unlabeled_data_file']\n",
    "\n",
    "def retrieve_all_files(dr):\n",
    "    lst = []\n",
    "    for root, directories, filenames in os.walk(dr):\n",
    "         for filename in filenames: \n",
    "            lst.append(os.path.join(root,filename))\n",
    "    return lst\n",
    "    \n",
    "#getting all files recursively\n",
    "data_loc = os.path.join(path_unlabeled,fl_unlabeled) \n",
    "path_list_total = retrieve_all_files(data_loc)\n",
    "print(f'{len(path_list_total)} JSON files indicated')\n",
    "\n",
    "# Start snorkel session and creating location subclass\n",
    "session = SnorkelSession()     \n",
    "#session = Meta.init(\"postgres://localhost:5432/\" + cfg['postgres_db_name']).SnorkelSession()\n",
    "Location_Extraction = candidate_subclass('location_extraction',\\\n",
    "                          [\"location\"])\n",
    "#Phone_Extraction = candidate_subclass('location_extraction',\\\n",
    "#                          [\"location\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice, chain\n",
    "from fonduer.snorkel.utils import ProgressBar\n",
    "from fonduer.snorkel.models import GoldLabel, GoldLabelKey, Document\n",
    "\n",
    "class MEMEXJsonLGZIPPreprocessor(HTMLListPreprocessor):\n",
    "    \n",
    "    def __init__(self, path, file_list, encoding=\"utf-8\", max_docs=float('inf'), lines_per_entry=6, verbose=False):\n",
    "        self.path = path\n",
    "        self.encoding = encoding\n",
    "        self.max_docs = max_docs\n",
    "        self.file_list = file_list\n",
    "        self.lines_per_entry = lines_per_entry\n",
    "        self.verbose=verbose\n",
    "        \n",
    "    def _get_files(self,path_list):\n",
    "        fpaths = [fl for fl in path_list]\n",
    "        return fpaths\n",
    "    \n",
    "    def _can_read(self, fpath):\n",
    "        return fpath.endswith('jsonl') or fpath.endswith('gz')\n",
    "    \n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Parses a file or directory of files into a set of Document objects.\n",
    "        \"\"\"\n",
    "        doc_count = 0\n",
    "        for file_name in self._get_files(self.file_list):\n",
    "            if self._can_read(file_name):\n",
    "                for doc, text in self.parse_file(file_name):\n",
    "                    yield doc, text\n",
    "                    doc_count += 1\n",
    "                    if self.verbose:\n",
    "                        print(f'Parsed {doc_count} docs...')\n",
    "                    if doc_count >= self.max_docs:\n",
    "                        return\n",
    "                    \n",
    "    def _lines_per_n(self, f, n):\n",
    "        for line in f:\n",
    "            yield ''.join(chain([line], islice(f, n - 1)))\n",
    "        \n",
    "    def _read_content_file(self, fl):\n",
    "        json_lst = []\n",
    "        if fl.endswith('gz'):\n",
    "            with gzip.GzipFile(fl, 'r') as fin: \n",
    "                f = fin.read()\n",
    "            for chunk in f.splitlines():\n",
    "                jfile = json.loads(chunk)\n",
    "                json_lst.append(jfile)\n",
    "\n",
    "        elif fl.endswith('jsonl'):\n",
    "            with open(fl) as f:\n",
    "                for chunk in self._lines_per_n(f, self.lines_per_entry):\n",
    "                    jfile = json.loads(chunk)\n",
    "                    json_lst.append(jfile)\n",
    "        else:\n",
    "            print('Unrecognized file type!')\n",
    "                    \n",
    "        json_pd = pd.DataFrame(json_lst)\n",
    "        #json_pd = pd.DataFrame(json_lst).dropna()\n",
    "        return json_pd\n",
    "    \n",
    "    def parse_file(self, file_name):\n",
    "        df = self._read_content_file(file_name)\n",
    "        if 'raw_content' in df.keys():\n",
    "            for index, row in df.iterrows():\n",
    "                name = row.url\n",
    "                stable_id = self.get_stable_id(name)\n",
    "                text = row.raw_content[1:-1].encode(self.encoding)\n",
    "                yield Document(name=name, stable_id=stable_id, text=str(text),\n",
    "                                   meta={'file_name' : file_name}), str(text)\n",
    "        else:\n",
    "            print('File with no raw content!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3017\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "print(len(path_list_total))\n",
    "with gzip.GzipFile(path_list_total[doc_ind], 'rb') as fin: \n",
    "                f = fin.read().decode('utf-8')\n",
    "\n",
    "def _lines_per_n(self, f, n):\n",
    "        for line in f:\n",
    "            yield ''.join(chain([line], islice(f, n - 1)))\n",
    "#for chunk in doc_preprocessor._lines_per_n(f, 1):\n",
    "#                jfile = json.loads(chunk)\n",
    "#                json_lst.append(jfile)\n",
    "\n",
    "fspl = f.splitlines()\n",
    "\n",
    "json_lst = []\n",
    "for jobj in fspl:\n",
    "    jfile = json.loads(jobj)\n",
    "    json_lst.append(jfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_pd = pd.DataFrame(json_lst)\n",
    "'raw_content' in json_pd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inds for image and doc for checking\n",
    "doc_ind = 300\n",
    "img_ind = 10\n",
    "\n",
    "# Getting parameter for max number of docs to load from labeled/unlabeled\n",
    "#max_docs = cfg['max_docs']\n",
    "max_docs = 100\n",
    "\n",
    "# Setting jsons to load\n",
    "path_list = [path_list_total[img_ind], path_list_total[doc_ind]]\n",
    "\n",
    "# Preprocessing documents from path_list\n",
    "doc_preprocessor = MEMEXJsonLGZIPPreprocessor(data_loc,\\\n",
    "                                file_list=path_list,max_docs=max_docs,encoding='utf-8',lines_per_entry=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File with no raw content!\n",
      "Parsed 1 docs...\n",
      "Parsed 2 docs...\n",
      "Parsed 3 docs...\n",
      "Parsed 4 docs...\n",
      "Parsed 5 docs...\n",
      "Parsed 6 docs...\n",
      "Parsed 7 docs...\n",
      "Parsed 8 docs...\n",
      "Parsed 9 docs...\n",
      "Parsed 10 docs...\n",
      "Parsed 11 docs...\n",
      "Parsed 12 docs...\n",
      "Parsed 13 docs...\n",
      "Parsed 14 docs...\n",
      "Parsed 15 docs...\n",
      "Parsed 16 docs...\n",
      "Parsed 17 docs...\n",
      "Parsed 18 docs...\n",
      "Parsed 19 docs...\n",
      "Parsed 20 docs...\n",
      "Parsed 21 docs...\n",
      "Parsed 22 docs...\n",
      "Parsed 23 docs...\n",
      "Parsed 24 docs...\n",
      "Parsed 25 docs...\n",
      "Parsed 26 docs...\n",
      "Parsed 27 docs...\n",
      "Parsed 28 docs...\n",
      "Parsed 29 docs...\n",
      "Parsed 30 docs...\n",
      "Parsed 31 docs...\n",
      "Parsed 32 docs...\n",
      "Parsed 33 docs...\n",
      "Parsed 34 docs...\n",
      "Parsed 35 docs...\n",
      "Parsed 36 docs...\n",
      "Parsed 37 docs...\n",
      "Parsed 38 docs...\n",
      "Parsed 39 docs...\n",
      "Parsed 40 docs...\n",
      "Parsed 41 docs...\n",
      "Parsed 42 docs...\n",
      "Parsed 43 docs...\n",
      "Parsed 44 docs...\n",
      "Parsed 45 docs...\n",
      "Parsed 46 docs...\n",
      "Parsed 47 docs...\n",
      "Parsed 48 docs...\n",
      "Parsed 49 docs...\n",
      "Parsed 50 docs...\n",
      "Parsed 51 docs...\n",
      "Parsed 52 docs...\n",
      "Parsed 53 docs...\n",
      "Parsed 54 docs...\n",
      "Parsed 55 docs...\n",
      "Parsed 56 docs...\n",
      "Parsed 57 docs...\n",
      "Parsed 58 docs...\n",
      "Parsed 59 docs...\n",
      "Parsed 60 docs...\n",
      "Parsed 61 docs...\n",
      "Parsed 62 docs...\n",
      "Parsed 63 docs...\n",
      "Parsed 64 docs...\n",
      "Parsed 65 docs...\n",
      "Parsed 66 docs...\n",
      "Parsed 67 docs...\n",
      "Parsed 68 docs...\n",
      "Parsed 69 docs...\n",
      "Parsed 70 docs...\n",
      "Parsed 71 docs...\n",
      "Parsed 72 docs...\n",
      "Parsed 73 docs...\n",
      "Parsed 74 docs...\n",
      "Parsed 75 docs...\n",
      "Parsed 76 docs...\n",
      "Parsed 77 docs...\n",
      "Parsed 78 docs...\n",
      "Parsed 79 docs...\n",
      "Parsed 80 docs...\n",
      "Parsed 81 docs...\n",
      "Parsed 82 docs...\n",
      "Parsed 83 docs...\n",
      "Parsed 84 docs...\n",
      "Parsed 85 docs...\n",
      "Parsed 86 docs...\n",
      "Parsed 87 docs...\n",
      "Parsed 88 docs...\n",
      "Parsed 89 docs...\n",
      "Parsed 90 docs...\n",
      "Parsed 91 docs...\n",
      "Parsed 92 docs...\n",
      "Parsed 93 docs...\n",
      "Parsed 94 docs...\n",
      "Parsed 95 docs...\n",
      "Parsed 96 docs...\n",
      "Parsed 97 docs...\n",
      "Parsed 98 docs...\n",
      "Parsed 99 docs...\n",
      "Parsed 100 docs...\n",
      "CPU times: user 7.56 s, sys: 40.5 s, total: 48 s\n",
      "Wall time: 7min 1s\n"
     ]
    }
   ],
   "source": [
    "# Ingest data into Fonduer via parser\n",
    "corpus_parser = OmniParser(structural=True, lingual=True, visual=False)\n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=cfg['parallel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 100\n",
      "Phrases: 33375\n"
     ]
    }
   ],
   "source": [
    "from fonduer.models import Document, Phrase\n",
    "\n",
    "# Checking database contents\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Phrases:\", session.query(Phrase).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Dividing into Test/Train, Extracting Features, Throttling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 80\n",
      "dev: 10\n",
      "test: 10\n",
      "['http://eroticmugshots.com/newyork-escorts/848-482-3651/?pid=27872053',\n",
      " 'http://liveescortreviews.com/ad/winnipeg/204-583-5263/1/30068',\n",
      " 'http://liveescortreviews.com/ad/houston/719-233-0873/1/278842',\n",
      " 'http://liveescortreviews.com/ad/philadelphia/0/1/158892',\n",
      " 'http://escortsinthe.us/347-593-6634/?pid=20679642',\n",
      " 'http://escortads.xxx/217-560-2578/?pid=11456451',\n",
      " 'http://liveescortreviews.com/ad/denver/0/1/136339',\n",
      " 'http://213-401-5625.escortphonelist.com/1oo-ra-mixed-uty-1-pmt-specials-2403787.html',\n",
      " 'http://liveescortreviews.com/ad/eastbay/702-666-5392/1/82449',\n",
      " 'http://liveescortreviews.com/ad/vancouver/533-324-3510/1/50226',\n",
      " 'http://901-900-4675.escortphonelist.com/houstons-finest-brunette-bombshells-incall-outcall-day-or-night-2401169.html',\n",
      " 'http://liveescortreviews.com/ad/brooklyn/202-441-3524/1/185190',\n",
      " 'http://escortsinthe.us/414-364-9644/?pid=11268865',\n",
      " 'http://757-581-9682.escortsincollege.com/come-on-over-ill-send-u-home-happy-2402098.html',\n",
      " 'http://www.usasexguide.info/forum/showthread.php?8710-Escort-Reviews/page14',\n",
      " 'http://510-717-0408.escortsincollege.com/exotic-sexy-new-honey-2401489.html',\n",
      " 'http://escortsinthe.us/623-428-9716/?pid=30599478',\n",
      " 'http://liveescortreviews.com/ad/northjersey/469-263-3756/1/199272',\n",
      " 'http://eroticmugshots.com/virginiabeach-escorts/206-578-1555/?pid=19074845',\n",
      " 'http://liveescortreviews.com/ad/inlandempire/909-999-4802/1/119601',\n",
      " 'http://liveescortreviews.com/ad/raleigh/919-798-7987/1/87386',\n",
      " 'http://liveescortreviews.com/ad/sanfernandovalley/310-351-6432/1/108267',\n",
      " 'http://eroticmugshots.com/fresno-escorts/415-689-6542/?pid=11618325',\n",
      " 'http://www.usasexguide.info/forum/showthread.php?3988-Streetwalker-Reports/page47',\n",
      " 'http://liveescortreviews.com/ad/atlanta/0/1/310763',\n",
      " 'http://massagetroll.com/sanfrancisco-massages/415-419-9587/?pid=20987837',\n",
      " 'http://eroticmugshots.com/houston-escorts/615-710-9936/?pid=24333647',\n",
      " 'http://eroticmugshots.com/moseslake-escorts/509-631-8656/?pid=18669842',\n",
      " 'http://liveescortreviews.com/ad/norfolk/757-943-4117/1/21822',\n",
      " 'http://liveescortreviews.com/ad/pittsburgh/925-453-1346/1/26592',\n",
      " 'http://liveescortreviews.com/ad/detroit/734-680-2964/1/147728',\n",
      " 'http://myproviderguide.com/kitchener/escorts/6252788',\n",
      " 'http://escortads.xxx/678-779-4989/?pid=6145271',\n",
      " 'http://escortads.xxx/604-715-2083/?pid=8377577',\n",
      " 'http://eroticmugshots.com/sanbernardino-escorts/562-277-4876/?pid=8002929',\n",
      " 'http://eroticmugshots.com/kansascity-escorts/530-405-7155/?pid=11901745',\n",
      " 'http://escortsinthe.us/908-899-3245/?pid=20693003',\n",
      " 'http://eroticmugshots.com/miami-escorts/646-657-7590/?pid=52371356',\n",
      " 'http://www.usasexguide.info/forum/showthread.php?3988-Streetwalker-Reports/page418',\n",
      " 'http://escortsinthe.us/901-406-8234/?pid=24287689',\n",
      " 'http://liveescortreviews.com/ad/washingtondc/702-490-1269/1/335098',\n",
      " 'http://escortsinthe.us/907-223-2548/?pid=3786180',\n",
      " 'http://escortads.xxx/647-868-7277/?pid=24995784',\n",
      " 'http://eroticmugshots.com/sanbernardino-escorts/562-277-4876/?pid=7500579',\n",
      " 'http://liveescortreviews.com/ad/eastbay/209-564-8137/1/82249',\n",
      " 'http://eroticmugshots.com/windsor-escorts/647-829-7963/?pid=23274646',\n",
      " 'http://liveescortreviews.com/ad/nova/301-500-9462/1/150042',\n",
      " 'http://liveescortreviews.com/ad/chicago/646-342-0240/1/343095',\n",
      " 'http://eroticmugshots.com/lakecharles-escorts/646-844-4642/?pid=11696336',\n",
      " 'http://eroticmugshots.com/memphis-escorts/573-881-7150/?pid=12987208',\n",
      " 'http://liveescortreviews.com/ad/atlanta/404-838-1507/1/311080',\n",
      " 'http://escortsinthe.us/907-306-1561/?pid=3900799',\n",
      " 'http://eroticmugshots.com/virginiabeach-escorts/561-401-6053/?pid=13530568',\n",
      " 'http://escortsinthe.us/414-208-5378/?pid=11285523',\n",
      " 'http://escortads.xxx/480-725-2034/?pid=30659818',\n",
      " 'http://eroticmugshots.com/sacramento-escorts/210-645-5483/?pid=7992175',\n",
      " 'http://liveescortreviews.com/ad/houston/832-988-4495/1/279008',\n",
      " 'http://liveescortreviews.com/ad/washingtondc/571-354-9446/1/335524',\n",
      " 'http://liveescortreviews.com/ad/lasvegas/702-685-1525/1/310563',\n",
      " 'http://eroticmugshots.com/nova-escorts/325-074-8202/?pid=20945155',\n",
      " 'http://liveescortreviews.com/ad/ottawa/613-266-2565/1/54633',\n",
      " 'http://escortads.xxx/347-720-7028/?pid=54161782',\n",
      " 'http://443-286-2124.escortphonelist.com/22-snowbunny-incalls-only-2402955.html',\n",
      " 'http://liveescortreviews.com/ad/nashville/615-420-8669/1/27886',\n",
      " 'http://480-878-7739.escortsincollege.com/arizona-top-escort-agency-2401356.html',\n",
      " 'http://613-663-9398.escortphonelist.com/new-petite-barbiejapantoby-2401460.html',\n",
      " 'http://liveescortreviews.com/ad/queens/917-362-2112/1/201535',\n",
      " 'http://619-382-0730.escortsincollege.com/new-gentlemens-1-choice-1000-real-and-accurate-petiete-goddess-2401984.html',\n",
      " 'http://escortads.xxx/647-821-5057/?pid=25243745',\n",
      " 'http://liveescortreviews.com/ad/minneapolis/612-806-6989/1/97609',\n",
      " 'http://eroticmugshots.com/myrtlebeach-escorts/305-903-0642/?pid=18187023',\n",
      " 'http://liveescortreviews.com/ad/hamilton/416-476-1191/1/41923',\n",
      " 'http://liveescortreviews.com/ad/washingtondc/419-975-3120/1/335358',\n",
      " 'http://www.cityvibe.com/santabarbara/Escorts/is-it-me-or-is-it-getting-cold-outside/1678345',\n",
      " 'http://escortsinthe.us/206-304-8745/?pid=17061352',\n",
      " 'http://504-320-9929.escortphonelist.com/alexis-the-specialist-2401846.html',\n",
      " 'http://myproviderguide.com/san-fernando-valley/escorts/6252399',\n",
      " 'http://escortsinthe.us/580-510-9025/?pid=8390957',\n",
      " 'http://717-219-7564.escortsincollege.com/outcall-til-9-pm-then-in-cal-good-morning-2403279.html',\n",
      " 'http://liveescortreviews.com/ad/hartford/504-906-5469/1/58058']\n",
      "['http://liveescortreviews.com/ad/sanjose/424-240-6600/1/120145',\n",
      " 'http://liveescortreviews.com/ad/ftlauderdale/786-657-8763/1/138782',\n",
      " 'http://779-200-1743.escortphonelist.com/hot-like-fire-sweet-as-pie-2402378.html',\n",
      " 'http://eroticmugshots.com/eugene-escorts/760-520-2319/?pid=30381058',\n",
      " 'http://liveescortreviews.com/ad/brooklyn/718-688-4495/1/185322',\n",
      " 'http://323-484-3014.escortsincollege.com/new-skilled-vixen-upscale-freak-bombshell-2402931.html',\n",
      " 'http://386-295-7931.escortphonelist.com/sexy-an-waiting-2403292.html',\n",
      " 'http://myproviderguide.com/saint-louis/escorts/6253330',\n",
      " 'http://eroticmugshots.com/losangeles-escorts/424-299-9790/?pid=16868105',\n",
      " 'http://liveescortreviews.com/ad/detroit/616-606-8987/1/147589']\n",
      "['http://216-609-4464.escortsincollege.com/everyone-loves-chocolate-2403036.html',\n",
      " 'http://liveescortreviews.com/ad/orangecounty/661-912-9127/1/226058',\n",
      " 'http://liveescortreviews.com/ad/sanfernandovalley/916-258-2229/1/108198',\n",
      " 'http://250-448-8854.escortphonelist.com/b-e-a-c-h-b-u-n-n-i-e-s-new-new-new-2403915.html',\n",
      " 'http://massagetroll.com/jacksonville-massages/904-729-1228/?pid=6738474',\n",
      " 'http://eroticmugshots.com/neworleans-escorts/504-541-7895/?pid=12085285',\n",
      " 'http://916-504-7095.escortphonelist.com/nanas-be-too-gorgeous-2401857.html',\n",
      " 'http://liveescortreviews.com/ad/denver/720-459-1186/1/136641',\n",
      " 'http://eroticmugshots.com/atlanta-escorts/424-260-6978/?pid=15224943',\n",
      " 'http://escortads.xxx/475-201-6673/?pid=8746542']\n"
     ]
    }
   ],
   "source": [
    "# Getting all documents parsed by Fonduer\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "dev_set_sz = np.round(ld*0.1)\n",
    "test_set_sz = np.round(ld*0.1)\n",
    "train_set_sz = ld - dev_set_sz - test_set_sz\n",
    "\n",
    "# Setting up train, dev, and test sets\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "\n",
    "# Creating list of (document name, Fonduer document object) tuples\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "shuffle(data)\n",
    "\n",
    "# Adding unlabeled data to train set, \n",
    "# labaled data to dev/test sets in alternating fashion\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if i<train_set_sz:\n",
    "        train_docs.add(doc)\n",
    "    else:\n",
    "        if len(dev_docs)<=len(test_docs):\n",
    "            dev_docs.add(doc)\n",
    "        else:\n",
    "            test_docs.add(doc)\n",
    "\n",
    "#Printing length of train/test/dev sets\n",
    "print(\"train:\",len(train_docs))\n",
    "print(\"dev:\" ,len(dev_docs))\n",
    "print(\"test:\",len(test_docs))\n",
    "\n",
    "#Printing some filenames \n",
    "from pprint import pprint\n",
    "pprint([x.name for x in train_docs])\n",
    "pprint([x.name for x in dev_docs])\n",
    "pprint([x.name for x in test_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing matchers module and defining LocationMatcher\n",
    "from fonduer.snorkel.matchers import *\n",
    "location_matcher = LocationMatcher(longest_match_only=True) \n",
    "\n",
    "#importing NGrams and defining location_ngrams \n",
    "from fonduer.candidates import OmniNgrams\n",
    "location_ngrams = OmniNgrams(n_max=30, split_tokens=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.lf_helpers import *\n",
    "import re\n",
    "\n",
    "# Creating filter to eliminate mentions of currency  \n",
    "def location_currencies_filter(location):\n",
    "    list_currencies = [ \"dollar\", \"dollars\", \"lira\",\"kwacha\",\"rials\",\"rial\",\"dong\",\"dongs\",\"fuerte\",\"euro\",\n",
    "                       \"euros\",\"vatu\",\"som\",\"peso\",\"sterling\",\"sterlings\",\"soms\",\"pestos\",\n",
    "                       \"pounds\", \n",
    "                  \"pound\",\"dirham\",\"dirhams\",\"hryvnia\",\"manat\",\"manats\",\"liras\",\"lira\",\n",
    "                       \"dinar\",\"dinars\",\"pa'anga\",\"franc\",\"baht\",\"schilling\",\n",
    "                  \"somoni\",\"krona\",\"lilangeni\",\"rupee\",\"rand\",\"shilling\",\"leone\",\"riyal\",\"dobra\",\n",
    "                  \"tala\",\"ruble\",\"zloty\",\"peso\",\"sol\",\"quarani\",\"kina\",\"guinean\",\"balboa\",\"krone\",\"naira\",\n",
    "                  \"cordoba\",\"kyat\",\"metical\",\"togrog\",\"leu\",\"ouguiya\",\"rufiyaa\",\"ringgit\",\"kwacha\",\n",
    "                  \"ariary\",\"denar\",\"litas\",\"loti\",\"lats\",\"kip\",\"som\",\"won\",\"tenge\",\"yen\",\"shekel\",\"rupiah\",\n",
    "                  \"forint\",\"lempira\",\"gourde\",\"quetzal\",\"cedi\",\"lari\",\"dalasi\",\"cfp\",\"birr\",\"kroon\",\"nakfa\",\n",
    "                  \"cfa\",\"Peso\",\"koruna\",\"croatian\",\"colon\",\"yuan\",\"escudo\",\"cape\",\"riel\",\"lev\",\"real\"\n",
    "                  ,\"real\",\"mark\",\"boliviano\",\"ngultrum\",\"taka\",\"manat\",\"dram\",\"kwanza\",\"lek\",\"afghani\",\"renminbi\"]\n",
    "\n",
    "    \n",
    "    cand_right_tokens = list(get_right_ngrams(location,window=10))\n",
    "    for cand in cand_right_tokens:\n",
    "        if cand not in list_currencies:\n",
    "            return location\n",
    "\n",
    "# Setting candidate filter to location_currencies_filter\n",
    "candidate_filter = location_currencies_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 8.56 s, total: 8.56 s\n",
      "Wall time: 11.8 s\n",
      "Number of candidates: 40\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 11.7 ¬µs\n",
      "Number of candidates: 3\n",
      "Number of candidates: 5\n"
     ]
    }
   ],
   "source": [
    "from fonduer.candidates import CandidateExtractor\n",
    "\n",
    "# Defining candidate extractor\n",
    "candidate_extractor = CandidateExtractor(Location_Extraction,\n",
    "                                         [location_ngrams], [location_matcher],\n",
    "                                         candidate_filter=candidate_filter)\n",
    "\n",
    "# Extracting candidates from each split\n",
    "%time candidate_extractor.apply(train_docs, split=0, parallelism=cfg['parallel'])\n",
    "print(\"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == 0).count())\n",
    "%time\n",
    "for i, docs in enumerate([dev_docs, test_docs]):\n",
    "    candidate_extractor.apply(docs, split=i+1, parallelism=cfg['parallel'])\n",
    "    print(\"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == i+1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cands_dev = session.query(Location_Extraction).filter(Location_Extraction.split == 1).all()\n",
    "len(cands_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=83127, chars=[32,56], words=[5,5]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Phrase (Doc: b'http://eroticmugshots.com/neworleans-escorts/504-541-7895/?pid=12085285', Index: 27, Text: b\"Please contact the webmaster at bocoperations@gmail.com.\\\\');\")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 0\n",
    "print(cands_dev[ind])\n",
    "cands_dev[ind].get_parent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[location_extraction(Span(\"b'FuNN'\", sentence=152617, chars=[41,44], words=[9,9])),\n",
       " location_extraction(Span(\"b'New Christina'\", sentence=137252, chars=[57,69], words=[14,15])),\n",
       " location_extraction(Span(\"b'Christina'\", sentence=137252, chars=[61,69], words=[15,15])),\n",
       " location_extraction(Span(\"b'South'\", sentence=127026, chars=[66,70], words=[12,12])),\n",
       " location_extraction(Span(\"b'New'\", sentence=137252, chars=[57,59], words=[14,14])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=141595, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=127984, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=139071, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'Blonde'\", sentence=133400, chars=[19,24], words=[3,3])),\n",
       " location_extraction(Span(\"b'Atlantic'\", sentence=144328, chars=[0,7], words=[0,0])),\n",
       " location_extraction(Span(\"b'Blonde'\", sentence=134115, chars=[19,24], words=[3,3])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=135665, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=142443, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'East'\", sentence=141818, chars=[35,38], words=[3,3])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=118390, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'Bay'\", sentence=141688, chars=[17,19], words=[3,3])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=146246, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'East Bay'\", sentence=141818, chars=[35,42], words=[3,4])),\n",
       " location_extraction(Span(\"b'Bay'\", sentence=141818, chars=[40,42], words=[4,4])),\n",
       " location_extraction(Span(\"b' Beauty Artist and Sales Advisor'\", sentence=123819, chars=[53,84], words=[10,15])),\n",
       " location_extraction(Span(\"b'\\\\n\\\\n'\", sentence=141818, chars=[30,33], words=[2,2])),\n",
       " location_extraction(Span(\"b'Beauty Artist and Sales Advisor'\", sentence=123819, chars=[54,84], words=[11,15])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=154518, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'East'\", sentence=141688, chars=[12,15], words=[2,2])),\n",
       " location_extraction(Span(\"b'Artist and Sales'\", sentence=123819, chars=[61,76], words=[12,14])),\n",
       " location_extraction(Span(\"b'\\\\n\\\\n East'\", sentence=141818, chars=[30,38], words=[2,3])),\n",
       " location_extraction(Span(\"b'and'\", sentence=123819, chars=[68,70], words=[13,13])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=124584, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'East Bay'\", sentence=141688, chars=[12,19], words=[2,3])),\n",
       " location_extraction(Span(\"b'and Sales'\", sentence=123819, chars=[68,76], words=[13,14])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=130321, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'\\\\n\\\\n East Bay'\", sentence=141818, chars=[30,42], words=[2,4])),\n",
       " location_extraction(Span(\"b'Artist and Sales Advisor'\", sentence=123819, chars=[61,84], words=[12,15])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=118190, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'Sales'\", sentence=123819, chars=[72,76], words=[14,14])),\n",
       " location_extraction(Span(\"b'Valley'\", sentence=144844, chars=[34,39], words=[5,5])),\n",
       " location_extraction(Span(\"b'NH'\", sentence=124050, chars=[114,115], words=[18,18])),\n",
       " location_extraction(Span(\"b'West Coast'\", sentence=123750, chars=[0,9], words=[0,1])),\n",
       " location_extraction(Span(\"b'and Sales Advisor'\", sentence=123819, chars=[68,84], words=[13,15])),\n",
       " location_extraction(Span(\"b'West'\", sentence=123750, chars=[0,3], words=[0,0])),\n",
       " location_extraction(Span(\"b'NH Job'\", sentence=124050, chars=[114,119], words=[18,19])),\n",
       " location_extraction(Span(\"b'Coast'\", sentence=123750, chars=[5,9], words=[1,1])),\n",
       " location_extraction(Span(\"b'Mediterranean'\", sentence=149926, chars=[24,36], words=[4,4])),\n",
       " location_extraction(Span(\"b'Sales Advisor'\", sentence=123819, chars=[72,84], words=[14,15])),\n",
       " location_extraction(Span(\"b'Gina'\", sentence=149697, chars=[40,43], words=[7,7])),\n",
       " location_extraction(Span(\"b'Job'\", sentence=124050, chars=[117,119], words=[19,19])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=149513, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'Advisor'\", sentence=123819, chars=[78,84], words=[15,15])),\n",
       " location_extraction(Span(\"b'NH Job Summary'\", sentence=124050, chars=[114,127], words=[18,20])),\n",
       " location_extraction(Span(\"b'Job Summary'\", sentence=124050, chars=[117,127], words=[19,20])),\n",
       " location_extraction(Span(\"b'Summary'\", sentence=124050, chars=[121,127], words=[20,20])),\n",
       " location_extraction(Span(\"b' '\", sentence=123819, chars=[53,53], words=[10,10])),\n",
       " location_extraction(Span(\"b' Beauty'\", sentence=123819, chars=[53,59], words=[10,11])),\n",
       " location_extraction(Span(\"b'Beauty'\", sentence=123819, chars=[54,59], words=[11,11])),\n",
       " location_extraction(Span(\"b' Beauty Artist'\", sentence=123819, chars=[53,66], words=[10,12])),\n",
       " location_extraction(Span(\"b'Beauty Artist'\", sentence=123819, chars=[54,66], words=[11,12])),\n",
       " location_extraction(Span(\"b' Beauty Artist and'\", sentence=123819, chars=[53,70], words=[10,13])),\n",
       " location_extraction(Span(\"b'Beauty Artist and'\", sentence=123819, chars=[54,70], words=[11,13])),\n",
       " location_extraction(Span(\"b'Artist'\", sentence=123819, chars=[61,66], words=[12,12])),\n",
       " location_extraction(Span(\"b' Beauty Artist and Sales'\", sentence=123819, chars=[53,76], words=[10,14])),\n",
       " location_extraction(Span(\"b'Beauty Artist and Sales'\", sentence=123819, chars=[54,76], words=[11,14])),\n",
       " location_extraction(Span(\"b'Artist and'\", sentence=123819, chars=[61,70], words=[12,13])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=143741, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=123825, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=157216, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=157268, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=134325, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=124544, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=126234, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=121860, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'Bay'\", sentence=154769, chars=[152,154], words=[26,26])),\n",
       " location_extraction(Span(\"b'North Bay'\", sentence=154769, chars=[146,154], words=[25,26])),\n",
       " location_extraction(Span(\"b'North'\", sentence=154769, chars=[146,150], words=[25,25])),\n",
       " location_extraction(Span(\"b'EAST'\", sentence=128259, chars=[48,51], words=[9,9])),\n",
       " location_extraction(Span(\"b'EGLINTON'\", sentence=128259, chars=[39,46], words=[8,8])),\n",
       " location_extraction(Span(\"b'EGLINTON EAST'\", sentence=128259, chars=[39,51], words=[8,9])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=132783, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'Mediterranean'\", sentence=130703, chars=[24,36], words=[4,4])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=130224, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=136802, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'Call'\", sentence=130482, chars=[0,3], words=[0,0])),\n",
       " location_extraction(Span(\"b'Call 49'\", sentence=130482, chars=[0,6], words=[0,1])),\n",
       " location_extraction(Span(\"b'49'\", sentence=130482, chars=[5,6], words=[1,1])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=135650, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=128720, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=145096, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=144965, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'New Exotic Blonde'\", sentence=129330, chars=[48,64], words=[13,15])),\n",
       " location_extraction(Span(\"b'DONT'\", sentence=152226, chars=[15,18], words=[2,2])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=128595, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'-'\", sentence=152226, chars=[34,34], words=[5,5])),\n",
       " location_extraction(Span(\"b'Exotic Blonde'\", sentence=129330, chars=[52,64], words=[14,15])),\n",
       " location_extraction(Span(\"b'-a'\", sentence=152226, chars=[34,35], words=[5,6])),\n",
       " location_extraction(Span(\"b'New Exotic Blonde Bombshell'\", sentence=129330, chars=[48,74], words=[13,16])),\n",
       " location_extraction(Span(\"b'DONT MISS'\", sentence=152226, chars=[15,23], words=[2,3])),\n",
       " location_extraction(Span(\"b'Blonde'\", sentence=129330, chars=[59,64], words=[15,15])),\n",
       " location_extraction(Span(\"b'a'\", sentence=152226, chars=[35,35], words=[6,6])),\n",
       " location_extraction(Span(\"b'Exotic Blonde Bombshell'\", sentence=129330, chars=[52,74], words=[14,16])),\n",
       " location_extraction(Span(\"b'MISS'\", sentence=152226, chars=[20,23], words=[3,3])),\n",
       " location_extraction(Span(\"b'New'\", sentence=129330, chars=[48,50], words=[13,13])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=151375, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'Blonde Bombshell'\", sentence=129330, chars=[59,74], words=[15,16])),\n",
       " location_extraction(Span(\"b'Area'\", sentence=131892, chars=[155,158], words=[25,25])),\n",
       " location_extraction(Span(\"b'DONT MISS OUT-@last'\", sentence=152226, chars=[15,33], words=[2,4])),\n",
       " location_extraction(Span(\"b'the'\", sentence=131892, chars=[143,145], words=[23,23])),\n",
       " location_extraction(Span(\"b'New Exotic'\", sentence=129330, chars=[48,57], words=[13,14])),\n",
       " location_extraction(Span(\"b'DONT MISS OUT-@last-'\", sentence=152226, chars=[15,34], words=[2,5])),\n",
       " location_extraction(Span(\"b'the Holland'\", sentence=131892, chars=[143,153], words=[23,24])),\n",
       " location_extraction(Span(\"b'Bombshell'\", sentence=129330, chars=[66,74], words=[16,16])),\n",
       " location_extraction(Span(\"b'DONT MISS OUT-@last-a'\", sentence=152226, chars=[15,35], words=[2,6])),\n",
       " location_extraction(Span(\"b'Holland'\", sentence=131892, chars=[147,153], words=[24,24])),\n",
       " location_extraction(Span(\"b'Exotic'\", sentence=129330, chars=[52,57], words=[14,14])),\n",
       " location_extraction(Span(\"b'MISS OUT-@last'\", sentence=152226, chars=[20,33], words=[3,4])),\n",
       " location_extraction(Span(\"b'the Holland Area'\", sentence=131892, chars=[143,158], words=[23,25])),\n",
       " location_extraction(Span(\"b'MISS OUT-@last-'\", sentence=152226, chars=[20,34], words=[3,5])),\n",
       " location_extraction(Span(\"b'Holland Area'\", sentence=131892, chars=[147,158], words=[24,25])),\n",
       " location_extraction(Span(\"b'MISS OUT-@last-a'\", sentence=152226, chars=[20,35], words=[3,6])),\n",
       " location_extraction(Span(\"b'OUT-@last'\", sentence=152226, chars=[25,33], words=[4,4])),\n",
       " location_extraction(Span(\"b'OUT-@last-'\", sentence=152226, chars=[25,34], words=[4,5])),\n",
       " location_extraction(Span(\"b'OUT-@last-a'\", sentence=152226, chars=[25,35], words=[4,6])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=125433, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=149123, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=122958, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=139252, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'Midwest'\", sentence=122794, chars=[194,200], words=[33,33])),\n",
       " location_extraction(Span(\"b'Highland Park'\", sentence=122647, chars=[15,27], words=[3,4])),\n",
       " location_extraction(Span(\"b'Park'\", sentence=122647, chars=[24,27], words=[4,4])),\n",
       " location_extraction(Span(\"b'Highland'\", sentence=122647, chars=[15,22], words=[3,3]))]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.query(Location_Extraction).filter(Location_Extraction.split == 0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 14.3 ¬µs\n",
      "Copying location_extraction_feature_updates to postgres\n",
      "b'COPY 128\\n'\n",
      "(128, 4099)\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 14.8 ¬µs\n",
      "Copying location_extraction_feature_updates to postgres\n",
      "b'COPY 17\\n'\n",
      "(17, 4099)\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10.5 ¬µs\n",
      "Copying location_extraction_feature_updates to postgres\n",
      "b'COPY 10\\n'\n",
      "(10, 4099)\n"
     ]
    }
   ],
   "source": [
    "# Applying the featurizer (to get feature vector describing the input)\n",
    "from fonduer import BatchFeatureAnnotator\n",
    "session.rollback()\n",
    "featurizer = BatchFeatureAnnotator(Location_Extraction)\n",
    "# Running for train set -- replace_key_set = True!\n",
    "%time\n",
    "F_train = featurizer.apply(split=0, replace_key_set=True, parallelism=cfg['parallel'])\n",
    "session.rollback()\n",
    "print(F_train.shape)\n",
    "# Running for dev set -- replace_key_set = False! Uses same featuers as dev set\n",
    "%time \n",
    "F_dev = featurizer.apply(split=1, clear=True, replace_key_set=False, parallelism=cfg['parallel'])\n",
    "session.rollback()\n",
    "print(F_dev.shape)\n",
    "%time \n",
    "F_test = featurizer.apply(split=2, clear=True, replace_key_set=False, parallelism=cfg['parallel'])\n",
    "session.rollback()\n",
    "print(F_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Adding Gold Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import range\n",
    "import csv\n",
    "import codecs\n",
    "import pycountry\n",
    "import us\n",
    "import editdistance\n",
    "\n",
    "from fonduer.snorkel.utils import ProgressBar\n",
    "from fonduer.snorkel.models import GoldLabel, GoldLabelKey\n",
    "\n",
    "# Defining function for getting gold labels\n",
    "# Could go in utils file later!\n",
    "\n",
    "def lookup_country_name(cn):\n",
    "    try:\n",
    "        out = pycountry.countries.lookup(cn).name\n",
    "    except:\n",
    "        out = 'no country'\n",
    "    return out\n",
    "\n",
    "def lookup_country_alpha3(cn):\n",
    "    try:\n",
    "        out = pycountry.countries.lookup(cn).alpha_3\n",
    "    except:\n",
    "        out = 'no country'\n",
    "    return out\n",
    "\n",
    "def lookup_country_alpha2(cn):\n",
    "    try:\n",
    "        out = pycountry.countries.lookup(cn).alpha_2\n",
    "    except:\n",
    "        out = 'no country'\n",
    "    return out\n",
    "\n",
    "def lookup_state_name(cn):\n",
    "    try:\n",
    "        out = us.states.lookup(val).name\n",
    "    except:\n",
    "        out = 'no state'\n",
    "    return out\n",
    "\n",
    "def lookup_state_abbr(cn):\n",
    "    try:\n",
    "        out = us.states.lookup(val).abbr\n",
    "    except:\n",
    "        out = 'no state'\n",
    "    return out\n",
    "\n",
    "def check_editdistance(val,targets):\n",
    "    for tgt in targets:\n",
    "        if editdistance.eval(val,tgt)<=3:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def match_val_targets(val,targets):\n",
    "    if val in targets: return True\n",
    "    if lookup_country_name(val).lower() in targets: return True\n",
    "    if lookup_country_alpha2(val).lower() in targets: return True\n",
    "    if lookup_country_alpha3(val).lower() in targets: return True\n",
    "    if lookup_state_name(val).lower() in targets: return True\n",
    "    if lookup_state_abbr(val).lower() in targets: return True\n",
    "    if any([a in val for a in targets]): return True\n",
    "    if check_editdistance(val,targets): return True\n",
    "    return False\n",
    "    \n",
    "def load_chtap_labels(session, candidate_class, df, target, annotator_name='gold'):\n",
    "    \n",
    "    # Database nonsense to make sure that there is a \"gold\" annotator \n",
    "    ak = session.query(GoldLabelKey).filter(GoldLabelKey.name == annotator_name).first()\n",
    "    if ak is None:\n",
    "        ak = GoldLabelKey(name=annotator_name)\n",
    "        session.add(ak)\n",
    "        session.commit()   \n",
    "    \n",
    "    # Getting all candidates from dev/test set only (splits 1 and 2)\n",
    "    candidates = session.query(candidate_class).filter(candidate_class.split != 0).all()\n",
    "    cand_total = len(candidates)\n",
    "    print('Loading', cand_total, 'candidate labels')\n",
    "    pb = ProgressBar(cand_total)\n",
    "    labels=[]\n",
    "    \n",
    "    # For each candidate, add appropriate gold label\n",
    "    for i, c in enumerate(candidates):\n",
    "        pb.bar(i)\n",
    "        # Get document name for candidate\n",
    "        doc = c[0].sentence.document.name\n",
    "        # Get text span for candidate\n",
    "        val = c[0].get_span().lower()\n",
    "        # Get location label from labeled dataframe (input)\n",
    "        target_strings = df[df['file name']==doc][target].tolist()\n",
    "        # Handling location extraction\n",
    "        if target == 'location':\n",
    "                if target_strings == []:\n",
    "                    targets = ''\n",
    "                else:\n",
    "                    targets = target_strings[0].lower().split(',')\n",
    "                    targets = [a.strip() for a in targets]\n",
    "        # Keeping this in comments...don't know what it was for\n",
    "        #context_stable_ids = '~~'.join([i.stable_id for i in c.get_contexts()])\n",
    "        label = session.query(GoldLabel).filter(GoldLabel.key == ak).filter(GoldLabel.candidate == c).first()\n",
    "        if label is None:\n",
    "            # Matching target label string to extract span, adding TRUE label if found, FALSE if not\n",
    "            # This conditional could be improved (use regex, etc.)\n",
    "            if match_val_targets(val,targets):\n",
    "                label = GoldLabel(candidate=c, key=ak, value=1)\n",
    "            else:\n",
    "                label = GoldLabel(candidate=c, key=ak, value=-1)\n",
    "            session.add(label)\n",
    "            labels.append(label)\n",
    "    session.commit()\n",
    "    pb.close()\n",
    "    print(\"AnnotatorLabels created: %s\" % (len(labels),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "ObjectDeletedError",
     "evalue": "Instance '<location_extraction at 0x7f542e7f4c50>' has been deleted, or its row is otherwise not present.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mObjectDeletedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                                 \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_default_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_safe_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m# A user-provided repr. Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/fonduer/snorkel/models/candidate.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         return \"%s(%s)\" % (\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/fonduer/snorkel/models/candidate.py\u001b[0m in \u001b[0;36mget_contexts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;34m\"\"\"Get a tuple of the consituent contexts making up this candidate\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__argnames__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/fonduer/snorkel/models/candidate.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;34m\"\"\"Get a tuple of the consituent contexts making up this candidate\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__argnames__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/orm/attributes.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdict_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/orm/attributes.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, state, dict_, passive)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallable_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mATTR_EMPTY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/orm/strategies.py\u001b[0m in \u001b[0;36m_load_for_state\u001b[0;34m(self, state, passive)\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m                 \u001b[0mpassive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m             )\n\u001b[1;32m    607\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPASSIVE_NO_RESULT\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/orm/strategies.py\u001b[0m in \u001b[0;36m_get_ident_for_use_get\u001b[0;34m(self, session, state, passive)\u001b[0m\n\u001b[1;32m    639\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_equated_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m                 passive=passive)\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mpk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         ]\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/orm/strategies.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    639\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_equated_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m                 passive=passive)\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mpk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         ]\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/orm/mapper.py\u001b[0m in \u001b[0;36m_get_state_attr_by_column\u001b[0;34m(self, state, dict_, column, passive)\u001b[0m\n\u001b[1;32m   2617\u001b[0m             passive=attributes.PASSIVE_RETURN_NEVER_SET):\n\u001b[1;32m   2618\u001b[0m         \u001b[0mprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_columntoproperty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2619\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_committed_state_attr_by_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/orm/attributes.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, state, dict_, passive)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpired_attributes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_expired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0mcallable_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/orm/state.py\u001b[0m in \u001b[0;36m_load_expired\u001b[0;34m(self, state, passive)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmodified\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeferred_scalar_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;31m# if the loader failed, or this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/orm/loading.py\u001b[0m in \u001b[0;36mload_scalar_attributes\u001b[0;34m(mapper, state, attribute_names)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;31m# may not complete (even if PK attributes are assigned)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_key\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0morm_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectDeletedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mObjectDeletedError\u001b[0m: Instance '<location_extraction at 0x7f542e7f4c50>' has been deleted, or its row is otherwise not present."
     ]
    }
   ],
   "source": [
    "cands_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_query = cands_dev[0].get_parent().document.name \n",
    "out = df_labeled.loc[df_labeled['url'].str.contains('http://losangeles.backpage.com/FemaleEscorts/')]\n",
    "\n",
    "#TO DO:\n",
    "# LOAD UP JSON FILE\n",
    "# SEE IF THERE IS ANY OVERLAP BETWEEN JSON FILE CONTENTS AND PD DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../memex-data/escorts/2014/12/13/0000.jsonl']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a single json file\n",
    "#json_files = ['0000.flat.json','0001.flat.json','0002.flat.json','0003.flat.json','0004.flat.json']\n",
    "json_files = path_list[0:1]\n",
    "json_pd = pd.DataFrame()\n",
    "for fl in json_files:\n",
    "    pd_temp = doc_preprocessor._read_content_file(fl)\n",
    "    json_pd = json_pd.append(pd_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking number of files in csv: 77329263\n",
    "source_list = df_labeled['source'].tolist()\n",
    "source_set = set(source_list)\n",
    "url_list = df_labeled['url'].tolist()\n",
    "url_set = set(url_list)\n",
    "\n",
    "# Checking number of files in json\n",
    "json_url_list = json_pd['url'].tolist()\n",
    "json_url_set = set(json_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CSV Records: 66176484\n",
      "Number of JSON Records 10806\n",
      "Number of intersections: 10756\n"
     ]
    }
   ],
   "source": [
    "url_int = json_url_set & url_set\n",
    "print('Number of CSV Records: %d' % len(url_set))\n",
    "print('Number of JSON Records %d' % len(json_url_set))\n",
    "print('Number of intersections: %d' % len(url_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int = df_labeled[df_labeled['url'].isin(json_url_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ind', 'source', 'type', 'url', 'content', 'extractions'], dtype='object')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_int.to_csv('test_csv_escorts-2014-12-13-0000-jsonl.csv')\n",
    "ind = 10\n",
    "url = json_pd['url'][ind]\n",
    "lab_loc = df_labeled.loc[df_labeled['url'] == url]\n",
    "json_loc = json_pd.loc[json_pd['url'] == url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://eroticmugshots.com/saltlakecity-escorts/818-660-9498/?pid=28623153']\n",
      "['http://eroticmugshots.com/saltlakecity-escorts/818-660-9498/?pid=28623153']\n"
     ]
    }
   ],
   "source": [
    "print(lab_loc['url'].tolist())\n",
    "print(json_loc['url'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<br>My name is Torri Lee Ann. I am twenty three years old with aperfect body and a glowing smile. Very out-going and down to earth. Mindintellectually and physically. \\\\\\\\n<br>Visiting from California for a short while.\\\\\\\\n<br>Looking to enjoy the time and companionship of upscale gentlemen who can appreciate the class and beauty of alady.\\\\\\\\n<br> Ill be waiting for the pleasure of meeting you\\\\\\\\n<br> xoxo Torri\\\\\\\\n<br>818-66O-9498 \\\\\\\\n<br>ps. Fetish friendly :).&nbsp;Call \\\\\\\\n<u>818-660-9498</u>.']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_loc['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_type</th>\n",
       "      <th>crawl_data</th>\n",
       "      <th>crawler</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>extracted_metadata</th>\n",
       "      <th>extracted_text</th>\n",
       "      <th>extractions</th>\n",
       "      <th>obj_original_url</th>\n",
       "      <th>obj_parent</th>\n",
       "      <th>obj_stored_url</th>\n",
       "      <th>raw_content</th>\n",
       "      <th>team</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>application/xhtml+xml; charset=windows-1252</td>\n",
       "      <td>{'status': '200', 'headers': '{'X-Varnish': ['...</td>\n",
       "      <td>scrapy-cluster</td>\n",
       "      <td>0FE26A77426E0B7C2AAA671701D38035A682DEFA5EFF82...</td>\n",
       "      <td>{'dc:title': '818-660-9498', 'description': '8...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n818-660-9498\\n\\n\\n \\n\\n\\...</td>\n",
       "      <td>{'posttime': {'attribs': {'website': 'github.c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0...</td>\n",
       "      <td>IST</td>\n",
       "      <td>2014-12-13T00:42:38</td>\n",
       "      <td>escorts</td>\n",
       "      <td>http://eroticmugshots.com/saltlakecity-escorts...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   content_type  \\\n",
       "10  application/xhtml+xml; charset=windows-1252   \n",
       "\n",
       "                                           crawl_data         crawler  \\\n",
       "10  {'status': '200', 'headers': '{'X-Varnish': ['...  scrapy-cluster   \n",
       "\n",
       "                                               doc_id  \\\n",
       "10  0FE26A77426E0B7C2AAA671701D38035A682DEFA5EFF82...   \n",
       "\n",
       "                                   extracted_metadata  \\\n",
       "10  {'dc:title': '818-660-9498', 'description': '8...   \n",
       "\n",
       "                                       extracted_text  \\\n",
       "10  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n818-660-9498\\n\\n\\n \\n\\n\\...   \n",
       "\n",
       "                                          extractions obj_original_url  \\\n",
       "10  {'posttime': {'attribs': {'website': 'github.c...              NaN   \n",
       "\n",
       "   obj_parent obj_stored_url  \\\n",
       "10        NaN            NaN   \n",
       "\n",
       "                                          raw_content team  \\\n",
       "10  \\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0...  IST   \n",
       "\n",
       "              timestamp     type  \\\n",
       "10  2014-12-13T00:42:38  escorts   \n",
       "\n",
       "                                                  url version  \n",
       "10  http://eroticmugshots.com/saltlakecity-escorts...     2.0  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Index(['content_type', 'crawl_data', 'crawler', 'doc_id', 'extracted_metadata',\n",
    "#       'extracted_text', 'extractions', 'obj_original_url', 'obj_parent',\n",
    "#       'obj_stored_url', 'raw_content', 'team', 'timestamp', 'type', 'url',\n",
    "#       'version'],\n",
    "#      dtype='object')\n",
    "#json_loc['raw_content'].tolist()\n",
    "json_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"post_date\": \"2014-07-29\", \"title\": \"818-660-9498\", \"age\": \"24\", \"ethnicity\": \"white|latin|latina|ebony|asian\", \"phone\": \"(206) 922-9303|(305) 849-8140|(312) 600-8628|(347) 940-1982|(401) 324-9388|(414) 914-3777|(416) 554-3337|(442) 222-0227|(469) 510-5849|(608) 609-5899|(623) 500-7076|(732) 621-4443|(773) 412-2044|(786) 504-1860|(818) 660-9498|(832) 914-9667|(917) 676-1333\"}']\n"
     ]
    }
   ],
   "source": [
    "print(lab_loc['extractions'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'age': {'attribs': {'class': 'custom',\n",
       "    'target': 'eroticmugshots',\n",
       "    'type': 'age',\n",
       "    'version': '1.0',\n",
       "    'website': 'github.com\\\\/istresearch'},\n",
       "   'results': ['24']},\n",
       "  'city': {'attribs': {'class': 'custom',\n",
       "    'target': 'eroticmugshots',\n",
       "    'type': 'city',\n",
       "    'version': '1.0',\n",
       "    'website': 'github.com\\\\/istresearch'},\n",
       "   'results': ['Dallas']},\n",
       "  'email': {'attribs': {'type': 'Age',\n",
       "    'version': '1.0',\n",
       "    'website': 'github.com/istresearch'},\n",
       "   'results': ['mariajgomezem@yahoo.com.co.']},\n",
       "  'ethnicity': {'attribs': {'type': 'Ethnicity',\n",
       "    'version': '1.0',\n",
       "    'website': 'github.com/istresearch'},\n",
       "   'results': ['white', 'latin', 'latina', 'ebony', 'asian']},\n",
       "  'name': {'attribs': {'type': 'Name',\n",
       "    'version': '1.0',\n",
       "    'website': 'github.com/istresearch'},\n",
       "   'results': ['torri']},\n",
       "  'phone': {'attribs': {'class': 'custom',\n",
       "    'target': 'eroticmugshots',\n",
       "    'type': 'phone',\n",
       "    'version': '1.0',\n",
       "    'website': 'github.com\\\\/istresearch'},\n",
       "   'results': ['818-660-9498']},\n",
       "  'phonenumber': {'attribs': {'type': 'Phone number',\n",
       "    'version': '1.0',\n",
       "    'website': 'github.com/istresearch'},\n",
       "   'results': ['3126008628',\n",
       "    '258186609498',\n",
       "    '20140729223000',\n",
       "    '8329149667',\n",
       "    '188186609498',\n",
       "    '238186609498',\n",
       "    '4422220227',\n",
       "    '8186609498',\n",
       "    '7326214443',\n",
       "    '7734122044',\n",
       "    '198186609498',\n",
       "    '6235007076',\n",
       "    '647702682518',\n",
       "    '4162751296100',\n",
       "    '64770268251',\n",
       "    '9176761333',\n",
       "    '416520519821',\n",
       "    '4695105849',\n",
       "    '4013249388',\n",
       "    '3058498140',\n",
       "    '6086095899',\n",
       "    '4165543337',\n",
       "    '228186609498',\n",
       "    '248186609498',\n",
       "    '81866094988186',\n",
       "    '2069229303',\n",
       "    '28186609498',\n",
       "    '3479401982',\n",
       "    '3010006018',\n",
       "    '7865041860',\n",
       "    '4149143777']},\n",
       "  'posttime': {'attribs': {'class': 'custom',\n",
       "    'target': 'eroticmugshots',\n",
       "    'type': 'posttime',\n",
       "    'version': '1.0',\n",
       "    'website': 'github.com\\\\/istresearch'},\n",
       "   'results': ['2014-07-29 22:30:00']},\n",
       "  'rate': {'attribs': {'type': 'Rate',\n",
       "    'version': '1.0',\n",
       "    'website': 'github.com/istresearch'},\n",
       "   'results': ['60-0:15', '90-0:30', '150-1:00']},\n",
       "  'reviewsite1': {'attribs': {'class': 'custom',\n",
       "    'target': 'eroticmugshots',\n",
       "    'type': 'reviewsite1',\n",
       "    'version': '1.0',\n",
       "    'website': 'github.com\\\\/istresearch'},\n",
       "   'results': ['coop.theeroticreview.com/hit.php?s=1&p=2&w=101908&t=0&c=74&u=http://www.theeroticreview.com/reviews/newreviewsList.asp?phone=818-660-9498']},\n",
       "  'sid': {'attribs': {'class': 'custom',\n",
       "    'target': 'eroticmugshots',\n",
       "    'type': 'sid',\n",
       "    'version': '1.0',\n",
       "    'website': 'github.com\\\\/istresearch'},\n",
       "   'results': ['28623153']},\n",
       "  'state': {'attribs': {'class': 'custom',\n",
       "    'target': 'eroticmugshots',\n",
       "    'type': 'state',\n",
       "    'version': '1.0',\n",
       "    'website': 'github.com\\\\/istresearch'},\n",
       "   'results': ['Texas']},\n",
       "  'text': {'attribs': {'class': 'custom',\n",
       "    'target': 'eroticmugshots',\n",
       "    'type': 'text',\n",
       "    'version': '1.0',\n",
       "    'website': 'github.com\\\\/istresearch'},\n",
       "   'results': ['<br/>My name is Torri Lee Ann. I am twenty three years old with aperfect body and a glowing smile. Very out-going and down to earth. Mindintellectually and physically. <br/>Visiting from California for a short while.<br/>Looking to enjoy the time and companionship of upscale gentlemen who can appreciate the class and beauty of alady.<br/> Ill be waiting for the pleasure of meeting you<br/> xoxo Torri<br/>818-66O-9498 <br/>ps. Fetish friendly :).&nbsp;Call <u>818-660-9498</u>.']},\n",
       "  'title': {'attribs': {'class': 'custom',\n",
       "    'target': 'eroticmugshots',\n",
       "    'type': 'title',\n",
       "    'version': '1.0',\n",
       "    'website': 'github.com\\\\/istresearch'},\n",
       "   'results': ['818-660-9498']}}]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_loc['extractions'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adultsearch',\n",
       " 'anunico',\n",
       " 'asexyservice',\n",
       " 'backpage',\n",
       " 'cityvibe',\n",
       " 'cityxguide',\n",
       " 'classivox',\n",
       " 'craigslist',\n",
       " 'eroticmugshots',\n",
       " 'escortadsxxx',\n",
       " 'escortphonelist',\n",
       " 'escortsinca',\n",
       " 'escortsincollege',\n",
       " 'escortsintheus',\n",
       " 'happymassage',\n",
       " 'liveescortreviews',\n",
       " 'massagetroll',\n",
       " 'missingkids',\n",
       " 'myproviderguide',\n",
       " 'naughtyreviews',\n",
       " 'redbook',\n",
       " 'rubads',\n",
       " 'sipsap'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2014-12-13T04:51:31',\n",
       " '2014-12-13T23:40:32',\n",
       " '2014-12-13T00:46:43',\n",
       " '2014-12-13T23:40:34',\n",
       " '2014-12-13T16:48:31',\n",
       " '2014-12-13T06:05:32',\n",
       " '2014-12-13T19:16:31',\n",
       " '2014-12-13T00:49:34',\n",
       " '2014-12-13T00:49:41',\n",
       " '2014-12-13T23:41:37',\n",
       " '2014-12-13T04:21:31',\n",
       " '2014-12-13T15:32:31',\n",
       " '2014-12-13T00:48:34',\n",
       " '2014-12-13T20:49:31',\n",
       " '2014-12-13T10:07:39',\n",
       " '2014-12-13T11:18:32',\n",
       " '2014-12-13T18:20:34',\n",
       " '2014-12-13T00:44:49',\n",
       " '2014-12-13T12:35:32',\n",
       " '2014-12-13T12:46:32',\n",
       " '2014-12-13T00:46:39',\n",
       " '2014-12-13T12:13:39',\n",
       " '2014-12-13T21:59:33',\n",
       " '2014-12-13T00:44:46',\n",
       " '2014-12-13T00:42:40',\n",
       " '2014-12-13T00:41:36',\n",
       " '2014-12-13T00:45:35',\n",
       " '2014-12-13T00:46:36',\n",
       " '2014-12-13T00:45:33',\n",
       " '2014-12-13T00:41:34',\n",
       " '2014-12-13T15:24:31',\n",
       " '2014-12-13T04:31:32',\n",
       " '2014-12-13T00:46:41',\n",
       " '2014-12-13T12:13:32',\n",
       " '2014-12-13T00:28:32',\n",
       " '2014-12-13T00:18:31',\n",
       " '2014-12-13T21:59:40',\n",
       " '2014-12-13T05:27:31',\n",
       " '2014-12-13T07:57:32',\n",
       " '2014-12-13T07:40:31',\n",
       " '2014-12-13T10:07:32',\n",
       " '2014-12-13T12:13:40',\n",
       " '2014-12-13T00:43:33',\n",
       " '2014-12-13T19:13:31',\n",
       " '2014-12-13T12:13:38',\n",
       " '2014-12-13T00:42:36',\n",
       " '2014-12-13T06:05:33',\n",
       " '2014-12-13T23:59:36',\n",
       " '2014-12-13T00:19:31',\n",
       " '2014-12-13T23:59:35']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_ts_list = json_pd['timestamp'].tolist()[-50:]\n",
    "json_ts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_labeled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0e2f45bc3b4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'location'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mload_chtap_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLocation_Extraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mannotator_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_labeled' is not defined"
     ]
    }
   ],
   "source": [
    "# Adding gold labels to database\n",
    "session.rollback()\n",
    "target = 'location'\n",
    "\n",
    "load_chtap_labels(session, Location_Extraction, df_labeled, target ,annotator_name='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Set Balance: 0.00 Percent Positive\n",
      "Test Set Balance: 0.00 Percent Positive\n"
     ]
    }
   ],
   "source": [
    "# Check class balance on dev/test\n",
    "from fonduer.snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "print('Dev Set Balance: %0.2f Percent Positive' % (100*np.sum(L_gold_dev == 1)/L_gold_dev.shape[0]))\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "print('Test Set Balance: %0.2f Percent Positive' % (100*np.sum(L_gold_test == 1)/L_gold_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Creating LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " cand_dev = session.query(Location_Extraction).filter(Location_Extraction.split == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for true/false/abstain\n",
    "TRUE,FALSE,ABSTAIN = 1,-1,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.lf_helpers import *\n",
    "import re\n",
    "# Defining LFs\n",
    "# LF API is here: http://web.stanford.edu/~lwhsiao/api/\n",
    "\n",
    "def LF_in_breadcrumbs_1(c):\n",
    "    parent_text = c.get_parent().text\n",
    "    return FALSE if '>' in parent_text else ABSTAIN\n",
    "\n",
    "def LF_long_candidate(c):\n",
    "    parent_text = c.get_parent().text\n",
    "    return FALSE if len(parent_text) > 1000 else ABSTAIN\n",
    "\n",
    "def LF_common_real_words(c):\n",
    "    reg_pos = re.compile('other cities|since|day|escorts',re.IGNORECASE)\n",
    "    if reg_pos.search(c.get_parent().text):\n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "#def LF_in_breadcrumbs_2(c):\n",
    "#    attributes = list(get_attributes(c))\n",
    "#    return TRUE if ('class=breadcrumbs'in attributes) or ('class=inside_scroll' in attributes) else ABSTAIN\n",
    "\n",
    "def LF_head_in_tag(c):\n",
    "    tags = list(get_ancestor_tag_names(c))\n",
    "    return FALSE if 'head' in tags else TRUE\n",
    "\n",
    "def LF_body_in_tag(c):\n",
    "    tags = list(get_ancestor_tag_names(c))\n",
    "    return TRUE if 'body' in tags else FALSE\n",
    "\n",
    "def LF_table_in_tag(c):\n",
    "    tags = list(get_ancestor_tag_names(c))\n",
    "    return TRUE if 'table' in tags else ABSTAIN\n",
    "\n",
    "def LF_to_left(c):\n",
    "    return TRUE if overlap(\n",
    "      ['location','locall','outcall','stay','live','available','female escort'], \n",
    "        get_left_ngrams(c, window=3)) else FALSE\n",
    "\n",
    "def LF_to_right(c):\n",
    "    return TRUE if overlap(\n",
    "      ['escorts','incall','outcall','stay','live','available','female escort'], \n",
    "        list(get_right_ngrams(c, window=5))) else ABSTAIN\n",
    "# Need more of these...can check tutorials for inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<function LF_in_breadcrumbs_1 at 0x7fc9dc0c1378>, <function LF_head_in_tag at 0x7fc9dbef0620>, <function LF_to_right at 0x7fc9dbef0ae8>, <function LF_table_in_tag at 0x7fc9dbef0a60>, <function LF_long_candidate at 0x7fc9db48b158>, <function LF_common_real_words at 0x7fc9db48bea0>]\n"
     ]
    }
   ],
   "source": [
    "# Collect LFs in list\n",
    "lfs_location = [LF_in_breadcrumbs_1,\n",
    "                #LF_in_breadcrumbs_2,\n",
    "                LF_head_in_tag,\n",
    "                #LF_body_in_tag,\n",
    "                LF_to_right,\n",
    "                #LF_to_left,\n",
    "                LF_table_in_tag,\n",
    "                LF_long_candidate,\n",
    "                LF_common_real_words]\n",
    "print (lfs_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Running Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying location_extraction_label to postgres\n",
      "b'COPY 7\\n'\n",
      "CPU times: user 76 ms, sys: 672 ms, total: 748 ms\n",
      "Wall time: 3.93 s\n",
      "Copying location_extraction_label_updates to postgres\n",
      "b'COPY 6\\n'\n",
      "CPU times: user 76 ms, sys: 632 ms, total: 708 ms\n",
      "Wall time: 3.91 s\n",
      "Copying location_extraction_label_updates to postgres\n",
      "b'COPY 12\\n'\n",
      "CPU times: user 76 ms, sys: 636 ms, total: 712 ms\n",
      "Wall time: 3.91 s\n",
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "from fonduer import BatchLabelAnnotator\n",
    "\n",
    "# Annotating candidats using LFs (clear=True replaced existing)\n",
    "labeler = BatchLabelAnnotator(Location_Extraction, lfs=lfs_location)\n",
    "%time L_train = labeler.apply(split=0, clear=True, parallelism=cfg['parallel'],update_keys =True)\n",
    "%time L_dev = labeler.apply(split=1, clear=True, parallelism=cfg['parallel'],update_keys =True)\n",
    "%time L_test = labeler.apply(split=2, clear=True, parallelism=cfg['parallel'],update_keys =True)\n",
    "print(L_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a)Computing Individual LF Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fonduer.snorkel.lf_helpers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-0fadcb60f77a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfonduer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnorkel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlf_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest_LF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlfs_location\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_LF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotator_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fonduer.snorkel.lf_helpers'"
     ]
    }
   ],
   "source": [
    "from fonduer.snorkel.lf_helpers import test_LF\n",
    "for lf in lfs_location:\n",
    "    print(lf.__name__)\n",
    "    tp, fp, tn, fn = test_LF(session, lf, split=1, annotator_name='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_extraction(Span(\"b'\\xef\\xb8\\x8f'\", sentence=1163, chars=[0,0], words=[0,0]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing a candidate from dev set\n",
    "ind = 0\n",
    "print(L_dev.get_candidate(session, ind))\n",
    "print(L_gold_dev[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6,) (0,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/fonduer/async_annotations.py\u001b[0m in \u001b[0;36mlf_stats\u001b[0;34m(self, labels, est_accs)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mcol_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Empirical Acc.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_tp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_fp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/fonduer/snorkel/utils.py\u001b[0m in \u001b[0;36mmatrix_tp\u001b[0;34m(L, labels)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmatrix_tp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return np.ravel([\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     ])\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/fonduer/snorkel/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmatrix_tp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return np.ravel([\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     ])\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6,) (0,) "
     ]
    }
   ],
   "source": [
    "# Loading assessing LF performance vs. gold labels\n",
    "from fonduer.snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "%time L_dev.lf_stats(L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10 ¬µs\n",
      "============================================================\n",
      "[1] Testing epochs = 1.00e+02, step_size = 1.00e-03, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "[GenerativeModel] Model saved as <GenerativeModel_0>.\n",
      "[GenerativeModel] Model saved as <GenerativeModel_best>.\n",
      "============================================================\n",
      "[2] Testing epochs = 5.00e+01, step_size = 1.00e-02, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[3] Testing epochs = 1.00e+02, step_size = 1.00e-02, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[4] Testing epochs = 1.00e+02, step_size = 1.00e-03, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[5] Testing epochs = 2.00e+01, step_size = 1.00e-05, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "[GenerativeModel] Model saved as <GenerativeModel_4>.\n",
      "[GenerativeModel] Model saved as <GenerativeModel_best>.\n",
      "============================================================\n",
      "[6] Testing epochs = 1.00e+02, step_size = 1.00e-06, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[7] Testing epochs = 1.00e+02, step_size = 1.00e-02, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[8] Testing epochs = 5.00e+01, step_size = 1.00e-02, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[9] Testing epochs = 1.00e+02, step_size = 1.00e-06, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[10] Testing epochs = 5.00e+01, step_size = 1.00e-03, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[11] Testing epochs = 1.00e+02, step_size = 1.00e-05, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[12] Testing epochs = 5.00e+01, step_size = 1.00e-04, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[13] Testing epochs = 2.00e+01, step_size = 1.00e-06, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[14] Testing epochs = 5.00e+01, step_size = 1.00e-04, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[15] Testing epochs = 1.00e+02, step_size = 1.00e-06, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "[GenerativeModel] Model <GenerativeModel_4> loaded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>step_size</th>\n",
       "      <th>decay</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>Rec.</th>\n",
       "      <th>F-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  step_size  decay     Prec.      Rec.       F-1\n",
       "4       20   0.000010   1.00  0.877246  0.983221  0.927215\n",
       "5      100   0.000001   1.00  0.877246  0.983221  0.927215\n",
       "8      100   0.000001   0.95  0.877246  0.983221  0.927215\n",
       "10     100   0.000010   0.90  0.877246  0.983221  0.927215\n",
       "11      50   0.000100   0.90  0.877246  0.983221  0.927215\n",
       "12      20   0.000001   0.90  0.877246  0.983221  0.927215\n",
       "13      50   0.000100   0.95  0.877246  0.983221  0.927215\n",
       "14     100   0.000001   1.00  0.877246  0.983221  0.927215\n",
       "0      100   0.001000   1.00  0.863333  0.869128  0.866221\n",
       "1       50   0.010000   0.95  0.863333  0.869128  0.866221\n",
       "2      100   0.010000   1.00  0.863333  0.869128  0.866221\n",
       "3      100   0.001000   0.95  0.863333  0.869128  0.866221\n",
       "6      100   0.010000   1.00  0.863333  0.869128  0.866221\n",
       "7       50   0.010000   0.90  0.863333  0.869128  0.866221\n",
       "9       50   0.001000   0.95  0.863333  0.869128  0.866221"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "from snorkel.learning import RandomSearch\n",
    "\n",
    "param_ranges = {\n",
    "    'step_size' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "    'decay' : [1.0, 0.95, 0.9],\n",
    "    'epochs' : [20, 50, 100]\n",
    "}\n",
    "\n",
    "searcher = RandomSearch(GenerativeModel, param_ranges, L_train, n=15)\n",
    "\n",
    "%time\n",
    "gen_model, run_stats = searcher.fit(L_dev, L_gold_dev)\n",
    "run_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e18e3ad50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing and plotting training marginals\n",
    "train_marginals = gen_model.marginals(L_train)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84921912, 1.07673239, 0.85064543, 0.94957098, 0.85142461,\n",
       "       0.85070572, 0.83974904, 0.84747935, 0.97313617, 0.84885964])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing LF accuracies\n",
    "gen_model.weights.lf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "      <th>Learned Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_in_breadcrumbs</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.849219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_head_in_tag</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.765306</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>259</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>249</td>\n",
       "      <td>0.863946</td>\n",
       "      <td>1.076732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_body_in_tag</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.850645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_to_right</th>\n",
       "      <td>3</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>0.057823</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_in_breadcrumbs_2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.851425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_table_in_tag</th>\n",
       "      <td>5</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_in_breadcrumbs_1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>249</td>\n",
       "      <td>0.984190</td>\n",
       "      <td>0.839749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_to_left</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.847479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_real_words</th>\n",
       "      <td>8</td>\n",
       "      <td>0.263605</td>\n",
       "      <td>0.263605</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>141</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909677</td>\n",
       "      <td>0.973136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_long_candidate</th>\n",
       "      <td>9</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>249</td>\n",
       "      <td>0.984190</td>\n",
       "      <td>0.848860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      j  Coverage  Overlaps  Conflicts   TP  FP  FN   TN  \\\n",
       "LF_in_breadcrumbs     0  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_head_in_tag        1  1.000000  0.765306   0.059524  259  41  39  249   \n",
       "LF_body_in_tag        2  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_to_right           3  0.108844  0.108844   0.057823   64   0   0    0   \n",
       "LF_in_breadcrumbs_2   4  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_table_in_tag       5  0.040816  0.040816   0.000000   24   0   0    0   \n",
       "LF_in_breadcrumbs_1   6  0.430272  0.430272   0.000000    0   0   4  249   \n",
       "LF_to_left            7  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_common_real_words  8  0.263605  0.263605   0.059524  141  14   0    0   \n",
       "LF_long_candidate     9  0.430272  0.430272   0.000000    0   0   4  249   \n",
       "\n",
       "                      Empirical Acc.  Learned Acc.  \n",
       "LF_in_breadcrumbs                NaN      0.849219  \n",
       "LF_head_in_tag              0.863946      1.076732  \n",
       "LF_body_in_tag                   NaN      0.850645  \n",
       "LF_to_right                 1.000000      0.949571  \n",
       "LF_in_breadcrumbs_2              NaN      0.851425  \n",
       "LF_table_in_tag             1.000000      0.850706  \n",
       "LF_in_breadcrumbs_1         0.984190      0.839749  \n",
       "LF_to_left                       NaN      0.847479  \n",
       "LF_common_real_words        0.909677      0.973136  \n",
       "LF_long_candidate           0.984190      0.848860  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pringint LF stats post-learning\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "prec, rec, f1 = gen_model.score(L_dev, L_gold_dev)\n",
    "L_dev.lf_stats(L_gold_dev, gen_model.weights.lf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[1] Testing dropout = 0.00e+00, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.06s)\tAverage loss=0.788678\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 25 (0.38s)\tAverage loss=0.480607\tDev F1=69.80\n",
      "[SparseLogisticRegression] Epoch 50 (0.97s)\tAverage loss=0.355863\tDev F1=67.99\n",
      "[SparseLogisticRegression] Epoch 75 (1.27s)\tAverage loss=0.311128\tDev F1=71.17\n",
      "[SparseLogisticRegression] Epoch 100 (1.59s)\tAverage loss=0.292562\tDev F1=67.70\n",
      "[SparseLogisticRegression] Epoch 125 (1.90s)\tAverage loss=0.283264\tDev F1=64.73\n",
      "[SparseLogisticRegression] Epoch 150 (2.20s)\tAverage loss=0.277872\tDev F1=63.38\n",
      "[SparseLogisticRegression] Epoch 175 (2.50s)\tAverage loss=0.274410\tDev F1=63.70\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.87s)\tAverage loss=0.272111\tDev F1=63.86\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (3.00s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-199\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.638596491228\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_0>\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_best>\n",
      "============================================================\n",
      "[2] Testing dropout = 5.00e-01, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 25 (0.36s)\tAverage loss=0.480607\tDev F1=69.80\n",
      "[SparseLogisticRegression] Epoch 50 (0.66s)\tAverage loss=0.355863\tDev F1=67.99\n",
      "[SparseLogisticRegression] Epoch 75 (0.98s)\tAverage loss=0.311128\tDev F1=71.17\n",
      "[SparseLogisticRegression] Epoch 100 (1.29s)\tAverage loss=0.292562\tDev F1=67.70\n",
      "[SparseLogisticRegression] Epoch 125 (1.59s)\tAverage loss=0.283264\tDev F1=64.73\n",
      "[SparseLogisticRegression] Epoch 150 (1.90s)\tAverage loss=0.277872\tDev F1=63.38\n",
      "[SparseLogisticRegression] Epoch 175 (2.22s)\tAverage loss=0.274410\tDev F1=63.70\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.60s)\tAverage loss=0.272111\tDev F1=63.86\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (2.74s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-199\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.638596491228\n",
      "============================================================\n",
      "[3] Testing dropout = 0.00e+00, lr = 1.00e-02\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 25 (0.75s)\tAverage loss=0.275965\tDev F1=65.90\n",
      "[SparseLogisticRegression] Epoch 50 (1.07s)\tAverage loss=0.264111\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 75 (1.37s)\tAverage loss=0.263621\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 100 (1.67s)\tAverage loss=0.263494\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 125 (1.98s)\tAverage loss=0.263446\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 150 (2.28s)\tAverage loss=0.263417\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 175 (2.59s)\tAverage loss=0.263397\tDev F1=66.97\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.96s)\tAverage loss=0.263385\tDev F1=66.97\n",
      "[SparseLogisticRegression] Training done (3.01s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.669714285714\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_2>\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_best>\n",
      "============================================================\n",
      "[4] Testing dropout = 0.00e+00, lr = 1.00e-02\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 25 (0.36s)\tAverage loss=0.275965\tDev F1=65.90\n",
      "[SparseLogisticRegression] Epoch 50 (0.67s)\tAverage loss=0.264111\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 75 (0.97s)\tAverage loss=0.263621\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 100 (1.28s)\tAverage loss=0.263494\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 125 (1.59s)\tAverage loss=0.263446\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 150 (1.90s)\tAverage loss=0.263417\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 175 (2.21s)\tAverage loss=0.263397\tDev F1=66.97\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.59s)\tAverage loss=0.263385\tDev F1=66.97\n",
      "[SparseLogisticRegression] Training done (2.64s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.669714285714\n",
      "============================================================\n",
      "[5] Testing dropout = 0.00e+00, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 25 (0.36s)\tAverage loss=0.480607\tDev F1=69.80\n",
      "[SparseLogisticRegression] Epoch 50 (0.67s)\tAverage loss=0.355863\tDev F1=67.99\n",
      "[SparseLogisticRegression] Epoch 75 (0.97s)\tAverage loss=0.311128\tDev F1=71.17\n",
      "[SparseLogisticRegression] Epoch 100 (1.55s)\tAverage loss=0.292562\tDev F1=67.70\n",
      "[SparseLogisticRegression] Epoch 125 (1.86s)\tAverage loss=0.283264\tDev F1=64.73\n",
      "[SparseLogisticRegression] Epoch 150 (2.17s)\tAverage loss=0.277872\tDev F1=63.38\n",
      "[SparseLogisticRegression] Epoch 175 (2.48s)\tAverage loss=0.274410\tDev F1=63.70\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.84s)\tAverage loss=0.272111\tDev F1=63.86\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (2.97s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-199\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.638596491228\n",
      "============================================================\n",
      "[6] Testing dropout = 0.00e+00, lr = 1.00e-05\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.36s)\tAverage loss=0.784480\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 50 (0.67s)\tAverage loss=0.780311\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 75 (0.97s)\tAverage loss=0.776173\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 100 (1.28s)\tAverage loss=0.772065\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 125 (1.58s)\tAverage loss=0.767988\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 150 (1.89s)\tAverage loss=0.763942\tDev F1=63.09\n",
      "[SparseLogisticRegression] Epoch 175 (2.22s)\tAverage loss=0.759925\tDev F1=63.09\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseLogisticRegression] Epoch 199 (2.58s)\tAverage loss=0.756098\tDev F1=63.09\n",
      "[SparseLogisticRegression] Training done (2.63s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.630872483221\n",
      "============================================================\n",
      "[7] Testing dropout = 0.00e+00, lr = 1.00e-06\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.36s)\tAverage loss=0.788257\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 50 (0.66s)\tAverage loss=0.787837\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 75 (0.96s)\tAverage loss=0.787417\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 100 (1.27s)\tAverage loss=0.786998\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 125 (1.58s)\tAverage loss=0.786578\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 150 (1.89s)\tAverage loss=0.786159\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 175 (2.42s)\tAverage loss=0.785740\tDev F1=62.16\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.80s)\tAverage loss=0.785338\tDev F1=62.16\n",
      "[SparseLogisticRegression] Training done (2.85s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.621621621622\n",
      "============================================================\n",
      "[8] Testing dropout = 5.00e-01, lr = 1.00e-02\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.06s)\tAverage loss=0.788678\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 25 (0.40s)\tAverage loss=0.275965\tDev F1=65.90\n",
      "[SparseLogisticRegression] Epoch 50 (0.70s)\tAverage loss=0.264111\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 75 (1.02s)\tAverage loss=0.263621\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 100 (1.32s)\tAverage loss=0.263494\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 125 (1.63s)\tAverage loss=0.263446\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 150 (1.93s)\tAverage loss=0.263417\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 175 (2.23s)\tAverage loss=0.263397\tDev F1=66.97\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.61s)\tAverage loss=0.263385\tDev F1=66.97\n",
      "[SparseLogisticRegression] Training done (2.65s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.669714285714\n",
      "============================================================\n",
      "[9] Testing dropout = 5.00e-01, lr = 1.00e-02\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 25 (0.38s)\tAverage loss=0.275965\tDev F1=65.90\n",
      "[SparseLogisticRegression] Epoch 50 (0.69s)\tAverage loss=0.264111\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 75 (0.99s)\tAverage loss=0.263621\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 100 (1.30s)\tAverage loss=0.263494\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 125 (1.60s)\tAverage loss=0.263446\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 150 (1.91s)\tAverage loss=0.263417\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 175 (2.25s)\tAverage loss=0.263397\tDev F1=66.97\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.61s)\tAverage loss=0.263385\tDev F1=66.97\n",
      "[SparseLogisticRegression] Training done (2.66s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.669714285714\n",
      "============================================================\n",
      "[10] Testing dropout = 0.00e+00, lr = 1.00e-06\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.37s)\tAverage loss=0.788257\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 50 (0.68s)\tAverage loss=0.787837\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 75 (0.99s)\tAverage loss=0.787417\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 100 (1.30s)\tAverage loss=0.786998\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 125 (1.60s)\tAverage loss=0.786578\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 150 (1.91s)\tAverage loss=0.786159\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 175 (2.21s)\tAverage loss=0.785740\tDev F1=62.16\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.59s)\tAverage loss=0.785338\tDev F1=62.16\n",
      "[SparseLogisticRegression] Training done (2.64s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.621621621622\n",
      "============================================================\n",
      "[11] Testing dropout = 5.00e-01, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 25 (0.37s)\tAverage loss=0.480607\tDev F1=69.80\n",
      "[SparseLogisticRegression] Epoch 50 (0.68s)\tAverage loss=0.355863\tDev F1=67.99\n",
      "[SparseLogisticRegression] Epoch 75 (0.98s)\tAverage loss=0.311128\tDev F1=71.17\n",
      "[SparseLogisticRegression] Epoch 100 (1.29s)\tAverage loss=0.292562\tDev F1=67.70\n",
      "[SparseLogisticRegression] Epoch 125 (1.60s)\tAverage loss=0.283264\tDev F1=64.73\n",
      "[SparseLogisticRegression] Epoch 150 (1.91s)\tAverage loss=0.277872\tDev F1=63.38\n",
      "[SparseLogisticRegression] Epoch 175 (2.21s)\tAverage loss=0.274410\tDev F1=63.70\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.59s)\tAverage loss=0.272111\tDev F1=63.86\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (2.72s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-199\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.638596491228\n",
      "============================================================\n",
      "[12] Testing dropout = 5.00e-01, lr = 1.00e-05\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.37s)\tAverage loss=0.784480\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 50 (0.68s)\tAverage loss=0.780311\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 75 (1.00s)\tAverage loss=0.776173\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 100 (1.30s)\tAverage loss=0.772065\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 125 (1.62s)\tAverage loss=0.767988\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 150 (1.93s)\tAverage loss=0.763942\tDev F1=63.09\n",
      "[SparseLogisticRegression] Epoch 175 (2.24s)\tAverage loss=0.759925\tDev F1=63.09\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.62s)\tAverage loss=0.756098\tDev F1=63.09\n",
      "[SparseLogisticRegression] Training done (2.67s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.630872483221\n",
      "============================================================\n",
      "[13] Testing dropout = 0.00e+00, lr = 1.00e-04\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.06s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.38s)\tAverage loss=0.747784\tDev F1=63.55\n",
      "[SparseLogisticRegression] Epoch 50 (0.70s)\tAverage loss=0.709641\tDev F1=64.23\n",
      "[SparseLogisticRegression] Epoch 75 (1.00s)\tAverage loss=0.674383\tDev F1=64.36\n",
      "[SparseLogisticRegression] Epoch 100 (1.32s)\tAverage loss=0.641896\tDev F1=65.80\n",
      "[SparseLogisticRegression] Epoch 125 (1.63s)\tAverage loss=0.612017\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 150 (1.95s)\tAverage loss=0.584581\tDev F1=66.34\n",
      "[SparseLogisticRegression] Epoch 175 (2.26s)\tAverage loss=0.559421\tDev F1=66.34\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.63s)\tAverage loss=0.537254\tDev F1=66.34\n",
      "[SparseLogisticRegression] Training done (2.68s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.663430420712\n",
      "============================================================\n",
      "[14] Testing dropout = 5.00e-01, lr = 1.00e-06\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.37s)\tAverage loss=0.788257\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 50 (0.89s)\tAverage loss=0.787837\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 75 (1.20s)\tAverage loss=0.787417\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 100 (1.52s)\tAverage loss=0.786998\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 125 (1.84s)\tAverage loss=0.786578\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 150 (2.17s)\tAverage loss=0.786159\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 175 (2.48s)\tAverage loss=0.785740\tDev F1=62.16\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.85s)\tAverage loss=0.785338\tDev F1=62.16\n",
      "[SparseLogisticRegression] Training done (2.90s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.621621621622\n",
      "============================================================\n",
      "[15] Testing dropout = 0.00e+00, lr = 1.00e-04\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.37s)\tAverage loss=0.747784\tDev F1=63.55\n",
      "[SparseLogisticRegression] Epoch 50 (0.68s)\tAverage loss=0.709641\tDev F1=64.23\n",
      "[SparseLogisticRegression] Epoch 75 (0.98s)\tAverage loss=0.674383\tDev F1=64.36\n",
      "[SparseLogisticRegression] Epoch 100 (1.29s)\tAverage loss=0.641896\tDev F1=65.80\n",
      "[SparseLogisticRegression] Epoch 125 (1.60s)\tAverage loss=0.612017\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 150 (1.91s)\tAverage loss=0.584581\tDev F1=66.34\n",
      "[SparseLogisticRegression] Epoch 175 (2.22s)\tAverage loss=0.559421\tDev F1=66.34\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.62s)\tAverage loss=0.537254\tDev F1=66.34\n",
      "[SparseLogisticRegression] Training done (2.67s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.663430420712\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression_2/SparseLogisticRegression_2-0\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression_2>\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "\n",
    "disc_model = SparseLogisticRegression\n",
    "param_ranges = {\n",
    "    'lr' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "    'dropout' : [0.0, 0.5]\n",
    "}\n",
    "\n",
    "model_hyperparams = {\n",
    "    'n_epochs' : 200,\n",
    "    'rebalance' : 0.5,\n",
    "    'print_freq' : 25\n",
    "}\n",
    "\n",
    "# We now add a session and probabilistic labels, as well as pass in the candidates\n",
    "# instead of the label matrix\n",
    "searcher = RandomSearch(disc_model, param_ranges, F_train, Y_train=train_marginals, n=15,\n",
    "    model_hyperparams=model_hyperparams)\n",
    "\n",
    "# We now pass in the development candidates and the gold development labels\n",
    "trained_model, run_stats = searcher.fit(F_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate discriminative on test set \n",
    "L_gold_test = np.array(load_gold_labels(session, annotator_name='gold', split=2).todense()).squeeze()\n",
    "# Get candidates, discriminative model outputs, and discriminative model predicts\n",
    "test_candidates = [F_test.get_candidate(session, i) for i in range(F_test.shape[0])]\n",
    "test_score = np.array(trained_model.predictions(F_test))\n",
    "true_pred = [test_candidates[_] for _ in np.nditer(np.where(test_score > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5475113122171946\n"
     ]
    }
   ],
   "source": [
    "# L_gold_test\n",
    "corr = [ test_score[i] == L_gold_test[i] for i in range(len(test_score))]\n",
    "acc = np.sum(corr)/len(corr)\n",
    "print (acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.918552036199095\n"
     ]
    }
   ],
   "source": [
    "# Assessing values \n",
    "gen_model_preds = (gen_model.marginals(L_test)>0.5)*2-1\n",
    "gen_corr = [ gen_model_preds[i] == L_gold_test[i] for i in range(len(gen_model_preds))]\n",
    "gen_acc = np.sum(gen_corr)/len(gen_corr)\n",
    "print(gen_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9217687074829932\n"
     ]
    }
   ],
   "source": [
    "# Assessing values\n",
    "L_gold_dev = np.array(load_gold_labels(session, annotator_name='gold', split=1).todense()).squeeze()\n",
    "gen_model_preds = (gen_model.marginals(L_dev)>0.5)*2-1\n",
    "gen_corr = [ gen_model_preds[i] == L_gold_dev[i] for i in range(len(gen_model_preds))]\n",
    "gen_acc = np.sum(gen_corr)/len(gen_corr)\n",
    "print(gen_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Creating and Saving Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#getting google place and geocoding APIs\n",
    "import googlemaps as gm\n",
    "import gmaps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import MultiPoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "maps_api_key = 'AIzaSyA0Veo5Lc6JOwDjNgQvPEhQB4AiZcrYQGI'\n",
    "gmaps.configure(api_key=maps_api_key)\n",
    "\n",
    "def get_possible_locations(plc):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    plc: string describing place to match\n",
    "\n",
    "    OUTPUTS\n",
    "    qo: full json structure returned from API call\n",
    "    cl: list of candidate location strings\n",
    "    \"\"\" \n",
    "    api_key = 'AIzaSyDbk3lLZHuQVKDRBN99_oz-p4AJjIzhA0w'\n",
    "    gms = gm.Client(key=api_key)\n",
    "    qo = gm.places.places_autocomplete(gms,plc)\n",
    "    cl = [a['description'] for a in qo]\n",
    "    return qo,cl\n",
    "\n",
    "def get_geocode(plc):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    plc: string describing place to match\n",
    "\n",
    "    OUTPUTS\n",
    "    qo full json structure returned from API call\n",
    "    (lat,lon): lat-lon tuple\n",
    "    \"\"\"\n",
    "    api_key = 'AIzaSyBlLyOaasYMgMxFGUh2jJyxIG0_pZFF_jM'\n",
    "    gms = gm.Client(key=api_key)\n",
    "    qo = gm.geocoding.geocode(gms,plc)\n",
    "    lat = qo[0]['geometry']['location']['lat']\n",
    "    lng = qo[0]['geometry']['location']['lng']\n",
    "    return qo,(lat,lng)\n",
    "\n",
    "def slice_pd_by_cont(dfm,col,val,pres=True,lower=False,union=False):\n",
    "    \"\"\"\n",
    "    Returns dataframe where column values include/exclude values in provided list\n",
    "    \n",
    "    INPUTS:\n",
    "    dfm: dataframe\n",
    "    col: column header\n",
    "    val: list of strings to include/ignore\n",
    "    pres: true to include, false to exclude\n",
    "    union: include union of these values\n",
    "    \"\"\"\n",
    "    if union:\n",
    "        val = ['|'.join(val)]\n",
    "    for vl in val:\n",
    "        if ~lower:\n",
    "            if pres:\n",
    "                dfm = dfm.loc[dfm[col].str.contains(vl,na=False)]\n",
    "            else:\n",
    "                dfm = dfm.loc[~dfm[col].str.contains(vl,na=False)]\n",
    "        else:\n",
    "            if pres:\n",
    "                dfm = dfm.loc[dfm[col].str.lower().str.contains(vl,na=False)]\n",
    "            else:\n",
    "                dfm = dfm.loc[~dfm[col].str.lower().str.contains(vl,na=False)]\n",
    "    return dfm\n",
    "\n",
    "def map_candidates_and_centroid(dfm):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    dfm: dataframe containing at least latitude, longitude\n",
    "    \n",
    "    OUTPUT\n",
    "    centroid: np array of lat/lon of location centroid\n",
    "    \"\"\"\n",
    "    df_cans = dfm\n",
    "    df_cans_map = dfm[['latitude','longitude']]\n",
    "    df_cans['lat_long'] = df_cans[['latitude', 'longitude']].apply(tuple, axis=1)\n",
    "    point_tup_lst = df_cans['lat_long'].tolist()\n",
    "    points = MultiPoint(point_tup_lst)\n",
    "    cent = np.array(points.centroid)\n",
    "    cent_df = pd.DataFrame([cent]) #this is a rough centroid estimate\n",
    "    fig = gmaps.Map()\n",
    "    can_layer = gmaps.symbol_layer(\n",
    "    df_cans_map, fill_color=\"green\", stroke_color=\"green\", scale=2)\n",
    "    cent_layer = gmaps.symbol_layer(\n",
    "    cent_df, fill_color=\"red\", stroke_color=\"red\", scale=2)\n",
    "    fig.add_layer(can_layer)\n",
    "    fig.add_layer(cent_layer)\n",
    "    fig\n",
    "    return cent,fig\n",
    "\n",
    "def get_attr(obj):\n",
    "    out = [a for a in dir(obj) if not a.startswith('__') and not callable(getattr(obj,a))]\n",
    "    return out\n",
    "\n",
    "def most_common(lt):\n",
    "    data = Counter(lt)\n",
    "    return data.most_common(1)[0][0]\n",
    "\n",
    "def get_common_country(lt):\n",
    "    country_lst = []\n",
    "    country_els = []\n",
    "    for it in lt:\n",
    "        try:\n",
    "            country = pycountry.countries.lookup(it.lower())\n",
    "            country_lst.append(country.alpha_3)\n",
    "            country_els.append(it)\n",
    "        except:\n",
    "            country = None \n",
    "    if country_lst == []:\n",
    "        return 'none',[],[]\n",
    "    return most_common(country_lst),country_lst, country_els\n",
    "\n",
    "def get_common_state(lt):\n",
    "    state_lst = []\n",
    "    state_els = []\n",
    "    for it in lt:\n",
    "        sts = [a.lower() for a in state_add_dict.keys()]\n",
    "        abbs = [a.lower() for a in state_add_dict.values()]\n",
    "        if it in sts:\n",
    "            state_lst.append(it)\n",
    "            state_els.append(it)\n",
    "        elif it in abbs:\n",
    "            state_lst.append(state_dict[it])\n",
    "            state_els.append(it)\n",
    "    if state_lst == []:\n",
    "        return 'none',[],[]\n",
    "    else:\n",
    "        return most_common(state_lst), state_lst, state_els\n",
    "\n",
    "def get_possible_locale(lt,cn,st,cn_lst,st_lst):\n",
    "    locale_list = []\n",
    "    a = [b for b in lt if b not in cn_lst and b not in st_lst]\n",
    "    for b in a:\n",
    "        locales = get_possible_locations(b)\n",
    "        locales = [c for c in locales if cn in b and st in b]\n",
    "        locale_list.append(locales)\n",
    "    return locale_list\n",
    "\n",
    "# Need to unify this!\n",
    "def lookup_state_abbrev(cn):\n",
    "    try:\n",
    "        out = state_add_dict[cn]\n",
    "    except:\n",
    "        out = 'no state'\n",
    "    return out\n",
    "\n",
    "state_dict = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "}\n",
    "state_add_dict = {v: k for k, v in state_dict.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_extractions = defaultdict(list)\n",
    "num_test_cands = F_test.shape[0]\n",
    "test_cand_preds = (gen_model.marginals(L_test)>0.5)*2-1\n",
    "for ind in range(num_test_cands):\n",
    "    cand = F_test.get_candidate(session,ind)\n",
    "    parent = cand.get_parent()\n",
    "    doc_name = parent.document.name\n",
    "    # Initializing key if it doesn't exist\n",
    "    doc_extractions[doc_name]\n",
    "    loc = cand.location.get_span().lower()\n",
    "    if test_cand_preds[ind] == 1:\n",
    "        doc_extractions[doc_name].append(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_extractions = defaultdict(set)\n",
    "for doc_name, extract_list in doc_extractions.items():\n",
    "    out_extractions[doc_name] = list(set(extract_list))\n",
    "df_out = pd.DataFrame()\n",
    "df_out = df_labeled[df_labeled['file name'].isin(list(out_extractions.keys()))]\n",
    "df_out['extracted_location'] = df_out.apply(lambda row: out_extractions[row['file name']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv('../output/location_extractions.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'austin', u'trishia austin', u'austin']\n",
      "Checking Locale 0 of 2\n",
      "> <ipython-input-170-2d31084fc3c4>(31)<module>()\n",
      "-> if lookup_country_name(probable_country).lower() in spl:\n",
      "(Pdb) spl\n",
      "['tricia cove', 'hutto', 'tx', 'usa']\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(48)<module>()\n",
      "-> count = count+1\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(26)<module>()\n",
      "-> while not_exact and count<len(locales):\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(27)<module>()\n",
      "-> print('Checking Locale %d of %d' %(count,len(locales)))\n",
      "(Pdb) n\n",
      "Checking Locale 1 of 2\n",
      "> <ipython-input-170-2d31084fc3c4>(28)<module>()\n",
      "-> c = locales[count]\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(29)<module>()\n",
      "-> spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(29)<module>()\n",
      "-> spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
      "(Pdb) spl\n",
      "['tricia cove', 'hutto', 'tx', 'usa']\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(29)<module>()\n",
      "-> spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
      "(Pdb) spl\n",
      "['tricia cove', 'hutto', 'tx', 'usa']\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(29)<module>()\n",
      "-> spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(29)<module>()\n",
      "-> spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(30)<module>()\n",
      "-> import pdb; pdb.set_trace()\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(31)<module>()\n",
      "-> if lookup_country_name(probable_country).lower() in spl:\n",
      "(Pdb) spl\n",
      "['tricia lane', 'hutto', 'tx', 'usa']\n",
      "(Pdb) count\n",
      "1\n",
      "(Pdb) spl\n",
      "['tricia lane', 'hutto', 'tx', 'usa']\n",
      "(Pdb) c\n",
      "Checking Locale 0 of 5\n",
      "> <ipython-input-170-2d31084fc3c4>(30)<module>()\n",
      "-> import pdb; pdb.set_trace()\n",
      "(Pdb) spl\n",
      "['austin', 'tx', 'usa']\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(31)<module>()\n",
      "-> if lookup_country_name(probable_country).lower() in spl:\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(48)<module>()\n",
      "-> count = count+1\n",
      "(Pdb) lookup_country_name(probable_country).lower() in spl\n",
      "False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-2d31084fc3c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-170-2d31084fc3c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/pdb.pyc\u001b[0m in \u001b[0;36muser_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_mainpyfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbp_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbp_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/pdb.pyc\u001b[0m in \u001b[0;36minteraction\u001b[0;34m(self, frame, traceback)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_stack_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/cmd.pyc\u001b[0m in \u001b[0;36mcmdloop\u001b[0;34m(self, intro)\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_rawinput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EOF'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out_locales = defaultdict(list)\n",
    "for doc_name, extract_list in doc_extractions.items():\n",
    "    \n",
    "    # Getting country names\n",
    "    probable_country,country_list, country_els = get_common_country(extract_list)\n",
    "    if lookup_country_alpha3(probable_country) == 'USA' and len(extract_list) >1:\n",
    "        # Getting state names\n",
    "        probable_state,state_list,state_els = get_common_state(extract_list)\n",
    "    else:\n",
    "        probable_state,state_list,state_els = 'none',[],[]\n",
    "    \n",
    "    locale_list = []\n",
    "    a = [b for b in extract_list if b not in country_els and b not in state_els] #need lookup here\n",
    "    print(a)\n",
    "    if a == []:\n",
    "        if probable_state != 'none' and probable_country != 'none' and a == []:\n",
    "            locale_list = ['none,none,'+state_add_dict[probable_state]+','+probable_country]\n",
    "    else:\n",
    "        most_common_locale = most_common(a)\n",
    "        aset = list(set(a))\n",
    "        for b in aset:\n",
    "                locale_tmp = []\n",
    "                qo,locales = get_possible_locations(b)\n",
    "                not_exact = 1\n",
    "                count = 0\n",
    "                while not_exact and count<len(locales):\n",
    "                    print('Checking Locale %d of %d' %(count,len(locales)))\n",
    "                    c = locales[count]\n",
    "                    spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
    "                    import pdb; pdb.set_trace()\n",
    "                    if lookup_country_name(probable_country).lower() in spl:\n",
    "                        if lookup_state_abbrev(probable_state).lower() in spl: \n",
    "                            if spl[0].lower() == most_common_locale.lower() and len(spl) == 3:\n",
    "                                locale_list = ['none']+spl\n",
    "                                locale_list = [','.join(locale_list)]\n",
    "                                not_exact = 0\n",
    "                                print('Exact City Found')\n",
    "                            elif spl[0].lower() == most_common_locale.lower() and len(spl) == 4:\n",
    "                                locale_list = [','.join(spl)]\n",
    "                                not_exact = 0\n",
    "                                print('Exact Location Found')\n",
    "                            else:             \n",
    "                                locale_list.append(','.join(spl))  \n",
    "                                count = count+1\n",
    "                        else:\n",
    "                            count = count+1         \n",
    "                    else:\n",
    "                        count = count+1\n",
    "        \n",
    "    import pdb; pdb.set_trace()\n",
    "    #reformatting for labeling comparison\n",
    "    locale_list_out = []\n",
    "    for c in locale_list:\n",
    "        b = c.split(',')\n",
    "        print(b)\n",
    "        b[-1] = str(lookup_country_alpha3(b[-1]).lower())\n",
    "        b[-2] = state_dict[b[-2].upper()].lower()\n",
    "        locale_list_out.append(','.join(b)) \n",
    "    out_locales[doc_name] = locale_list_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ii in out_locales.keys():\n",
    "    if out_locales[ii] == []:\n",
    "        out_locales[ii] = ['none','none','none','none']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
