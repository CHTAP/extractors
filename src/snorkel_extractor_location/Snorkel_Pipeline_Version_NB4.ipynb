{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluate Saved Discriminative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is ensure that modules are auto-reloaded at runtime to allow for development in other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set the Snorkel database location and start and connect to it.  By default, we use a PosgreSQL database backend, which can be created using `createdb DB_NAME` once psql is installed.  Note that Snorkel does *not* currently support parallel database processing with a SQLite backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Snorkel DB location\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#For PostgreSQL\n",
    "postgres_location = 'postgresql://jdunnmon:123@localhost:5432'\n",
    "postgres_db_name = 'loc_jd_1M'\n",
    "os.environ['SNORKELDB'] = os.path.join(postgres_location,postgres_db_name)\n",
    "\n",
    "#For local PostgreSQL\n",
    "#os.environ['SNORKELDB'] = 'postgres:///es_locs_small'\n",
    "\n",
    "# Adding path above for utils\n",
    "sys.path.append('../utils')\n",
    "\n",
    "# For SQLite\n",
    "#db_location = '.'\n",
    "#db_name = \"es_locs_small.db\"\n",
    "#os.environ['SNORKELDB'] = '{0}:///{1}/{2}'.format(\"sqlite\", db_location, db_name)\n",
    "\n",
    "# Start Snorkel session\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Setting parallelism\n",
    "parallelism = 72\n",
    "\n",
    "# Setting random seed\n",
    "seed = 1701\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parallelized loader\n"
     ]
    }
   ],
   "source": [
    "from dataset_utils import set_preprocessor, combine_dedupe\n",
    "\n",
    "# Set data source: options are 'content.tsv', 'memex_jsons', 'es'\n",
    "data_source = 'es'\n",
    "\n",
    "# Setting max number of docs to ingest\n",
    "max_docs = 100000\n",
    "\n",
    "# Setting location of data source\n",
    "\n",
    "# For ES:\n",
    "data_loc = '/dfs/scratch0/jdunnmon/data/memex-data/tsvs/output_all_b'\n",
    "\n",
    "# Optional: add tsv with additional documents to create combined tsv without duplicates\n",
    "#data_loc = combine_dedupe(data_loc, 'output_location.tsv', 'combined.tsv')\n",
    "\n",
    "# If memex_raw_content is a content_field, uses term as a regex in raw data in addition to getting title and body\n",
    "term = r'\\b[Ll]ocation:|\\b[cC]ity:'\n",
    "\n",
    "# Doc length in characters, remove to have no max\n",
    "max_doc_length=2000\n",
    "\n",
    "# Setting preprocessor\n",
    "doc_preprocessor = set_preprocessor(data_source, data_loc, max_docs=max_docs, verbose=True, clean_docs=True,\n",
    "                                    content_fields=['raw_content', 'url'], term=term, max_doc_length=max_doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 1000 docs...\n",
      "Parsed 2000 docs...\n",
      "Parsed 3000 docs...\n",
      "Parsed 4000 docs...\n",
      "Parsed 5000 docs...\n",
      "Parsed 6000 docs...\n",
      "Parsed 7000 docs...\n",
      "Parsed 8000 docs...\n",
      "Parsed 9000 docs...\n",
      "Parsed 10000 docs...\n",
      "Parsed 11000 docs...\n",
      "Parsed 12000 docs...\n",
      "Malformatted Line!\n",
      "Parsed 13000 docs...\n",
      "Parsed 14000 docs...\n",
      "Parsed 15000 docs...\n",
      "Parsed 16000 docs...\n",
      "Parsed 17000 docs...\n",
      "Parsed 18000 docs...\n",
      "Parsed 19000 docs...\n",
      "Parsed 20000 docs...\n",
      "Parsed 21000 docs...\n",
      "Parsed 22000 docs...\n",
      "Parsed 23000 docs...\n",
      "Parsed 24000 docs...\n",
      "Parsed 25000 docs...\n",
      "Malformatted Line!\n",
      "Parsed 26000 docs...\n",
      "Parsed 27000 docs...\n",
      "Parsed 28000 docs...\n",
      "Parsed 29000 docs...\n",
      "Parsed 30000 docs...\n",
      "Parsed 31000 docs...\n",
      "Parsed 32000 docs...\n",
      "Parsed 33000 docs...\n",
      "Parsed 34000 docs...\n",
      "Parsed 35000 docs...\n",
      "Parsed 36000 docs...\n",
      "Parsed 37000 docs...\n",
      "Parsed 38000 docs...\n",
      "Malformatted Line!\n",
      "Parsed 39000 docs...\n",
      "Parsed 40000 docs...\n",
      "Parsed 41000 docs...\n",
      "Parsed 42000 docs...\n",
      "Parsed 43000 docs...\n",
      "Parsed 44000 docs...\n",
      "Parsed 45000 docs...\n",
      "Parsed 46000 docs...\n",
      "Parsed 47000 docs...\n",
      "Parsed 48000 docs...\n",
      "Parsed 49000 docs...\n",
      "Parsed 50000 docs...\n",
      "Parsed 51000 docs...\n",
      "Malformatted Line!\n",
      "Parsed 52000 docs...\n",
      "Parsed 53000 docs...\n",
      "Parsed 54000 docs...\n",
      "Parsed 55000 docs...\n",
      "Parsed 56000 docs...\n",
      "Parsed 57000 docs...\n",
      "Parsed 58000 docs...\n",
      "Parsed 59000 docs...\n",
      "Parsed 60000 docs...\n",
      "Parsed 61000 docs...\n",
      "Parsed 62000 docs...\n",
      "Parsed 63000 docs...\n",
      "Malformatted Line!\n",
      "Parsed 64000 docs...\n",
      "Parsed 65000 docs...\n",
      "Parsed 66000 docs...\n",
      "Parsed 67000 docs...\n",
      "Parsed 68000 docs...\n",
      "Parsed 69000 docs...\n",
      "Parsed 70000 docs...\n",
      "Parsed 71000 docs...\n",
      "Parsed 72000 docs...\n",
      "Parsed 73000 docs...\n",
      "Parsed 74000 docs...\n",
      "Parsed 75000 docs...\n",
      "Parsed 76000 docs...\n",
      "Parsed 77000 docs...\n",
      "Parsed 78000 docs...\n",
      "Malformatted Line!\n",
      "Parsed 79000 docs...\n",
      "Parsed 80000 docs...\n",
      "Parsed 81000 docs...\n",
      "Parsed 82000 docs...\n",
      "Parsed 83000 docs...\n",
      "Parsed 84000 docs...\n",
      "Parsed 85000 docs...\n",
      "Parsed 86000 docs...\n",
      "Parsed 87000 docs...\n",
      "Parsed 88000 docs...\n",
      "Parsed 89000 docs...\n",
      "Parsed 90000 docs...\n",
      "Malformatted Line!\n",
      "Parsed 91000 docs...\n",
      "Parsed 92000 docs...\n",
      "Parsed 93000 docs...\n",
      "Parsed 94000 docs...\n",
      "Parsed 95000 docs...\n",
      "Parsed 96000 docs...\n",
      "Parsed 97000 docs...\n",
      "Parsed 98000 docs...\n",
      "Parsed 99000 docs...\n",
      "Parsed 100000 docs...\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[==============================          ] 74%"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "\n",
    "# Applying corpus parser\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(list(doc_preprocessor), parallelism=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 10000\n",
      "Sentences: 66299\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "# Printing number of docs/sentences\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all documents parsed by Snorkel\n",
    "sents = session.query(Sentence).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create candidate subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams\n",
    "from snorkel.candidates import CandidateExtractor\n",
    "from dataset_utils import create_candidate_class, LocationMatcher, city_index\n",
    "from snorkel.matchers import Union, LambdaFunctionMatcher\n",
    "\n",
    "# Setting extraction type -- should be a subfield in your data source extractions field!\n",
    "extraction_type = 'location'\n",
    "\n",
    "# Creating candidate class\n",
    "candidate_class, candidate_class_name = create_candidate_class(extraction_type)\n",
    "\n",
    "# Defining ngrams for candidates\n",
    "location_ngrams = Ngrams(n_max=3)\n",
    "\n",
    "# Define matchers\n",
    "cities = city_index('../utils/data/cities15000.txt')\n",
    "geo_location_matcher = LambdaFunctionMatcher(func=cities.fast_loc)\n",
    "# spacy_location_matcher = LocationMatcher(longest_match_only=True)\n",
    "\n",
    "# Union matchers and create candidate extractor\n",
    "location_matcher = Union(geo_location_matcher)\n",
    "cand_extractor   = CandidateExtractor(candidate_class, [location_ngrams], [location_matcher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 12 s, sys: 8.24 s, total: 20.2 s\n",
      "Wall time: 56.6 s\n",
      "Number of candidates: 30111\n"
     ]
    }
   ],
   "source": [
    "# Applying candidate extractor to each split\n",
    "%time cand_extractor.apply(sents, split=0, parallelism=parallelism)\n",
    "print(\"Number of candidates:\", session.query(candidate_class).filter(candidate_class.split == 0).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting candidates for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30111 candidates...\n"
     ]
    }
   ],
   "source": [
    "# Split to pull eval candidates from\n",
    "eval_split = 0\n",
    "\n",
    "# Executing query for eval candidates\n",
    "eval_cands = session.query(candidate_class).filter(candidate_class.split == eval_split).order_by(candidate_class.id).all()\n",
    "print(f'Loaded {len(eval_cands)} candidates...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading trained discriminative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Number of learning epochs:      5\n",
      "Learning rate:                  1e-05\n",
      "Use attention:                  False\n",
      "LSTM hidden dimension:          128\n",
      "Dropout:                        0.5\n",
      "Checkpoint Patience:            2\n",
      "Batch size:                     128\n",
      "Rebalance:                      0.5\n",
      "Load pre-trained embedding:     False\n",
      "Host device:                    gpu\n",
      "Word embedding size:            300\n",
      "Word embedding:                 None\n",
      "===============================================\n",
      "[LSTM] Loaded model <location_lstm>, only_param=False\n"
     ]
    }
   ],
   "source": [
    "from dm_utils import LSTM\n",
    "\n",
    "# defining model\n",
    "lstm = LSTM(n_threads=parallelism)\n",
    "\n",
    "# defining saved weights directory and name\n",
    "\n",
    "model_name = 'location_lstm' # this was provided when the model was saved!\n",
    "save_dir = 'checkpoints' # this was provided when the model was saved!\n",
    "\n",
    "# loading\n",
    "lstm.load(model_name=model_name, save_dir=save_dir, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting and plotting discriminative model marginals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../utils/dm_utils.py:133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  output_word, state_word = self.word_lstm(x_emb, state_word)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%time eval_marginals = lstm.marginals(eval_cands)\n",
    "# Plotting eval marginals\n",
    "plt.hist(eval_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating extractions from discriminative model marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gm_utils import create_extractions_dict\n",
    "\n",
    "# Enter googlemaps api key to get geocodes, leave blank to just use extracted locations\n",
    "geocode_key = None\n",
    "# geocode_key = 'AIzaSyBlLyOaasYMgMxFGUh2jJyxIG0_pZFF_jM'\n",
    "\n",
    "doc_extractions = create_extractions_dict(session, eval_cands, eval_marginals, extractions=[extraction_type],\n",
    "                                          dummy=False, geocode_key=geocode_key)\n",
    "\n",
    "# Uncomment to inspecting extractions dict to check format\n",
    "#doc_extractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving extractions to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Setting filename\n",
    "out_filename = \"loc_ext_test_discriminative_eval.jsonl\"\n",
    "\n",
    "# Saving file to jsonl in extractions format\n",
    "with open(out_filename, 'w') as outfile:\n",
    "    for k,v in doc_extractions.items():\n",
    "        v['id'] = k\n",
    "        print(json.dumps(v), file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Extracted String: date', ('Date', '12', 'JP', 35301, 42.46806, 140.86806)],\n",
       " ['Extracted String: goes', ('Goes', '10', 'NL', 36931, 51.50417, 3.88889)],\n",
       " ['Extracted String: name', ('Namur', 'WAL', 'BE', 106284, 50.4669, 4.86746)],\n",
       " ['Extracted String: aurora',\n",
       "  ('Aurora', 'IL', 'US', 200661, 41.76058, -88.32007),\n",
       "  ('Aurora', 'CO', 'US', 359407, 39.72943, -104.83192),\n",
       "  ('Aurora', '40', 'PH', 16178, 13.3476, 122.5195),\n",
       "  ('Aurora', 'OH', 'US', 15838, 41.31755, -81.34539)],\n",
       " ['Extracted String: new york',\n",
       "  ('New York City', 'NY', 'US', 8175133, 40.71427, -74.00597)],\n",
       " ['Extracted String: queens',\n",
       "  ('Queens', 'NY', 'US', 2272771, 40.68149, -73.83652),\n",
       "  ('Queens Village', 'NY', 'US', 51919, 40.72677, -73.74152)],\n",
       " ['Extracted String: much', ('Much', '07', 'DE', 15231, 50.90383, 7.40306)]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_extractions[\"b'3B0BD327C38D7578DA50E19F8BD68873B5C0E9CD63000BF37AEE120D7901AA06'\"]['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4445"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_extractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
