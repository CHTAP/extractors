{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Write Labeling Functions and Train Generative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is ensure that modules are auto-reloaded at runtime to allow for development in other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set the Snorkel database location and start and connect to it.  By default, we use a PosgreSQL database backend, which can be created using `createdb DB_NAME` once psql is installed.  Note that Snorkel does *not* currently support parallel database processing with a SQLite backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Snorkel DB location\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#For network PostgreSQL\n",
    "postgres_location = 'postgresql://jdufault:123@localhost:5432'\n",
    "postgres_db_name = 'es_locs_1M'\n",
    "os.environ['SNORKELDB'] = os.path.join(postgres_location,postgres_db_name)\n",
    "\n",
    "#For local PostgreSQL\n",
    "#os.environ['SNORKELDB'] = 'postgres:///es_locs_small'\n",
    "\n",
    "# Adding path above for utils\n",
    "sys.path.append('../utils')\n",
    "\n",
    "# For SQLite\n",
    "#db_location = '.'\n",
    "#db_name = \"es_locs_small.db\"\n",
    "#os.environ['SNORKELDB'] = '{0}:///{1}/{2}'.format(\"sqlite\", db_location, db_name)\n",
    "\n",
    "# Start Snorkel session\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Setting parallelism\n",
    "parallelism = 32\n",
    "\n",
    "# Setting random seed\n",
    "seed = 1701\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create candidate subclass and get dev set candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Candidates: 907\n"
     ]
    }
   ],
   "source": [
    "from dataset_utils import create_candidate_class\n",
    "\n",
    "# Setting extraction type -- should be a subfield in your data source extractions field!\n",
    "extraction_type = 'location'\n",
    "\n",
    "# Creating candidate class\n",
    "candidate_class, candidate_class_name  = create_candidate_class(extraction_type)\n",
    "\n",
    "# Getting dev set and printing length\n",
    "cands_dev = session.query(candidate_class).filter(candidate_class.split == 1).order_by(candidate_class.id).all()\n",
    "print(f'Dev Candidates: {len(cands_dev)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Labeling Functions (LFs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fonduer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-80959e3de722>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfonduer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlf_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_left_ngrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_right_ngrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_between_ngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msnorkel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlf_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tagged_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeotext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fonduer'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from fonduer.lf_helpers import get_left_ngrams, get_right_ngrams, get_between_ngrams\n",
    "from snorkel.lf_helpers import get_tagged_text\n",
    "\n",
    "import geotext\n",
    "import geograpy\n",
    "from geograpy import extraction\n",
    "\n",
    "from gm_utils import *\n",
    "from dataset_utils import lookup_state_name\n",
    "from nltk.corpus import words\n",
    "\n",
    "def lf_geograpy_entity_neg(c):\n",
    "    txt = c.location.get_span()\n",
    "    sent = c.get_parent().text\n",
    "    e = extraction.Extractor(text=sent)\n",
    "    e.find_entities()\n",
    "    places = [p.lower() for p in e.places]\n",
    "    if txt not in places:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lf_geograpy_country(c):\n",
    "    txt = c.location.get_span()\n",
    "    sent = c.get_parent().text\n",
    "    places = geograpy.get_place_context(text=sent)\n",
    "    if places.countries:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lf_geograpy_region(c):\n",
    "    txt = c.location.get_span()\n",
    "    sent = c.get_parent().text\n",
    "    places = geograpy.get_place_context(text=sent)\n",
    "    if places.regions:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lf_state(c):\n",
    "    txt = c.location.get_span()\n",
    "    if lookup_state_name(txt) != 'no state' and len(txt) > 4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def lf_geograpy_city(c):\n",
    "    txt = c.location.get_span()\n",
    "    sent = c.get_parent().text\n",
    "    places = geograpy.get_place_context(text=sent)\n",
    "    if places.cities:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lf_geograpy_other(c):\n",
    "    txt = c.location.get_span()\n",
    "    sent = c.get_parent().text\n",
    "    places = geograpy.get_place_context(text=sent)\n",
    "    if places.other:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lf_preceding_title(c):\n",
    "    preceding_words = ['title']\n",
    "    return 1 if overlap(\n",
    "      preceding_words,\n",
    "      get_left_ngrams(c, window=20)) else 0\n",
    "\n",
    "def lf_preceding_body(c):\n",
    "    preceding_words = ['in', 'to', 'north', 'south', 'east',\n",
    "                       'west', 'located', 'en', 'you', 'visit',\n",
    "                       'visits', 'escort', 'escorts', 'escortes']\n",
    "    return 1 if overlap(\n",
    "      preceding_words,\n",
    "      get_left_ngrams(c, window=3)) else 0\n",
    "\n",
    "def lf_preceding_search(c):\n",
    "    preceding_words = ['location', 'city', 'search']\n",
    "    return 1 if overlap(\n",
    "      preceding_words,\n",
    "      get_left_ngrams(c, window=4)) else 0\n",
    "\n",
    "def lf_preceding_url(c):\n",
    "    preceding_words = ['url']\n",
    "    return 1 if overlap(\n",
    "      preceding_words,\n",
    "      get_left_ngrams(c, window=4)) else 0\n",
    "\n",
    "def lf_many_locations(c):\n",
    "    txt = c.location.get_span().lower()\n",
    "    sent = c.get_parent().text\n",
    "    e = extraction.Extractor(text=sent)\n",
    "    e.find_entities()\n",
    "    thresh = 4\n",
    "    return -1 if len(e.places)>thresh else 0\n",
    "\n",
    "def lf_following_body(c):\n",
    "    preceding_words = ['escort', 'escorts', 'escortes']\n",
    "    return 1 if overlap(\n",
    "      preceding_words,\n",
    "      get_right_ngrams(c, window=4)) else 0\n",
    "\n",
    "def lf_repeated(c):\n",
    "    loc = [c.location.get_span().lower()]\n",
    "    return 1 if (overlap(loc, get_left_ngrams(c, window=4))\n",
    "                 or overlap(loc, get_right_ngrams(c, window=4))) else 0\n",
    "    \n",
    "def lf_nonletter(c):\n",
    "    txt = c.location.get_span().lower()\n",
    "    reg = re.compile(r'[^a-z ,]')\n",
    "    if reg.search(txt):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lf_english_word(c):\n",
    "    txt = c.location.get_span().lower()\n",
    "    return -1 if txt in words.words() else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf_loc(c):\n",
    "    return lf_geograpy_region(c) or lf_geograpy_city(c) or lf_state(c)\n",
    "    \n",
    "def lf_nonloc(c):\n",
    "    return -1 if not lf_loc(c) and not lf_geograpy_other(c) else 0\n",
    "\n",
    "def lf_from_title(c):\n",
    "    return lf_preceding_title(c) and lf_loc(c)\n",
    "\n",
    "def lf_from_title(c):\n",
    "    return lf_preceding_search(c) and lf_loc(c)\n",
    "\n",
    "def lf_from_body(c):\n",
    "    return (lf_preceding_body(c) or lf_following_body(c)) and lf_loc(c)\n",
    "\n",
    "def lf_from_url(c):\n",
    "    return lf_preceding_url(c) and lf_loc(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating list of LFs to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFs = [\n",
    "    lf_many_locations,\n",
    "    lf_preceding_title,\n",
    "    lf_preceding_search,\n",
    "    lf_preceding_body,\n",
    "    lf_preceding_url,\n",
    "    lf_following_body,\n",
    "    lf_from_title,\n",
    "    lf_from_body,\n",
    "    lf_from_url,\n",
    "    lf_repeated,\n",
    "    lf_english_word,\n",
    "    lf_geograpy_country,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading gold dev set labels from database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating labeling functions on dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /afs/cs.stanford.edu/u/jdufault/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /afs/cs.stanford.edu/u/jdufault/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /afs/cs.stanford.edu/u/jdufault/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /afs/cs.stanford.edu/u/jdufault/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only run once\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  snorkel.annotations import LabelAnnotator\n",
    "import numpy as np\n",
    "labeler = LabelAnnotator(lfs=LFs)\n",
    "\n",
    "%time L_dev = labeler.apply(split=1, parallelism=parallelism)\n",
    "L_dev.lf_stats(session, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating viewer to assist in LF development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# Can insert function here to select candidates based on arbitary criteria\n",
    "\n",
    "#Creating viewer for dev candidates\n",
    "sv = SentenceNgramViewer(cands_dev[:20], session)\n",
    "sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sv.get_selected()\n",
    "c.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once LFs are performing well, apply to entire database.  Applying to unlabeled data can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from  snorkel.annotations import LabelAnnotator\n",
    "import numpy as np\n",
    "labeler = LabelAnnotator(lfs=LFs)\n",
    "\n",
    "%time L_train = labeler.apply(split=0, parallelism=parallelism)\n",
    "%time L_test = labeler.apply(split=2, parallelism=parallelism)\n",
    "\n",
    "# can also load with:\n",
    "#%time L_train = labeler.load_matrix(session, split=0).astype(int)\n",
    "#%time L_dev = labeler.load_matrix(session, split=1).astype(int)\n",
    "#%time L_test = labeler.load_matrix(session, split=2).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "from snorkel.learning import RandomSearch\n",
    "\n",
    "# Setting parameter ranges for search\n",
    "param_ranges = {\n",
    "    'step_size' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "    'decay' : [1.0, 0.95, 0.9],\n",
    "    'epochs' : [20, 50, 100]\n",
    "}\n",
    "\n",
    "# Creating generative model\n",
    "gen_model = GenerativeModel()\n",
    "\n",
    "# Creating searcher over hyperparameters-- n is the number of models to train\n",
    "searcher = RandomSearch(GenerativeModel, param_ranges, L_train, n=5)\n",
    "\n",
    "# Searching model\n",
    "%time gen_model, run_stats = searcher.fit(L_dev, L_gold_dev, n_threads=parallelism)\n",
    "\n",
    "# Printing results of model search\n",
    "run_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing learned LF accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.weights.lf_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error analysis for generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = gen_model.error_analysis(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = SentenceNgramViewer(fp, session)\n",
    "sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting marginals, plotting training marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "L_eval = L_test\n",
    "eval_marginals = gen_model.marginals(L_eval)\n",
    "training_marginals = gen_model.marginals(L_train)\n",
    "\n",
    "# Plotting training marignals\n",
    "plt.hist(training_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.save(model_name='Loc_Gen_20K', save_dir='checkpoints', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dictionary of extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gm_utils import create_extractions_dict\n",
    "\n",
    "# Enter googlemaps api key to get geocodes, leave blank to just use extracted locations\n",
    "geocode_key = None\n",
    "# geocode_key = 'AIzaSyBlLyOaasYMgMxFGUh2jJyxIG0_pZFF_jM'\n",
    "\n",
    "doc_extractions = create_extractions_dict(session, L_eval, eval_marginals, extractions=[extraction_type],\n",
    "                                          dummy=False, geocode_key=geocode_key)\n",
    "\n",
    "# Uncomment to inspecting extractions dict to check format\n",
    "# doc_extractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Saving extractions to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Setting filename\n",
    "out_filename = \"loc_ext_test_generative.jsonl\"\n",
    "\n",
    "# Saving file to jsonl in extractions format\n",
    "with open(out_filename, 'w') as outfile:\n",
    "    for k,v in doc_extractions.items():\n",
    "        v['url'] = k\n",
    "        print(json.dumps(v), file=outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving training marginals for use with discriminative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import save_marginals\n",
    "%time save_marginals(session, L_train, training_marginals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
