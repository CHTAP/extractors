{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Parsing Files, Adding Candidates and Labels to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saeideh.shahrokh/anaconda/envs/fonduer_env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/saeideh.shahrokh/anaconda/envs/fonduer_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from os.path import join\n",
    "# Loading config\n",
    "with open(\"run_config_memex.json\") as fl:\n",
    "    cfg = json.load(fl)\n",
    "cfg_params = cfg['parameters']\n",
    "\n",
    "# Setting snorkel path and output root\n",
    "\n",
    "output_root = join(cfg_params['output_path'],cfg_params['experiment_name'])\n",
    "\n",
    "# Old import grammar\n",
    "os.environ['FONDUERDBNAME'] = cfg['postgres_db_name']\n",
    "os.environ['SNORKELDB'] = join(cfg['postgres_location'],os.environ['FONDUERDBNAME'])\n",
    "\n",
    "# For loading input files\n",
    "import pandas as pd\n",
    "\n",
    "# For running Snorkel\n",
    "from fonduer import SnorkelSession\n",
    "from fonduer.models import candidate_subclass\n",
    "from fonduer import HTMLPreprocessor, OmniParser\n",
    "#from fonduer import Meta\n",
    "from utils import MEMEXJsonPreprocessor, HTMLListPreprocessor\n",
    "\n",
    "#old snorkel imports\n",
    "#from snorkel.contrib.fonduer import SnorkelSession\n",
    "#from snorkel.contrib.fonduer.models import candidate_subclass\n",
    "#from snorkel.contrib.fonduer import HTMLPreprocessor, OmniParser\n",
    "#from utils import HTMLListPreprocessor, MEMEXJsonPreprocessor\n",
    "\n",
    "#from sqlalchemy import create_engine\n",
    "#snorkeldb = create_engine(os.environ['SNORKELDB'], isolation_level=\"AUTOCOMMIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up jsons\n",
    "# Load up content.tsv for gold labels?\n",
    "# Run doc preprocessor on jsons to get raw content\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "with open(\"run_config_memex.json\") as fl:\n",
    "    cfg = json.load(fl)\n",
    "#Creating path to labeled data\n",
    "pth_labeled = \"../../data\"#cfg['source_data_dir']\n",
    "\n",
    "# Getting labeled data file name\n",
    "fl_labeled = cfg['labeled_data_path']\n",
    "\n",
    "# Loading labeled data into dataframe\n",
    "df_labeled = pd.read_csv(join(pth_labeled,fl_labeled),names=['number_1','number_2','index','website_name','add_type','url','text','info_extracted','gold_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://pune.backpage.com/FemaleEscorts/good-looking-south-and-noth-indian-collage-girls-in-pune-escorts-service/19568623'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_labeled.head()\n",
    "df_labeled['url'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load html data from json files\n",
    "fl_unlabeled = cfg['unlabeled_data_path']\n",
    "path_list = os.listdir(os.path.join(cfg['data_path'],fl_unlabeled))\n",
    "\n",
    "# Start snorkel session and creating location subclass\n",
    "session = SnorkelSession()     \n",
    "#session = Meta.init(\"postgres://localhost:5432/\" + cfg['postgres_db_name']).SnorkelSession()\n",
    "Location_Extraction = candidate_subclass('location_extraction',\\\n",
    "                          [\"location\"])\n",
    "Phone_Extraction = candidate_subclass('location_extraction',\\\n",
    "                          [\"location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://pune.backpage.com/FemaleEscorts/good-looking-south-and-noth-indian-collage-girls-in-pune-escorts-service/19568623'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled.head(5)\n",
    "df_labeled['url'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting parameter for max number of docs to load from labeled/unlabeled\n",
    "#max_docs = cfg['max_docs']\n",
    "max_docs = 100\n",
    "\n",
    "# Setting location for raw data\n",
    "data_loc = join(cfg['data_path'],cfg['unlabeled_data_path'])\n",
    "\n",
    "# Setting jsons to load\n",
    "path_list = path_list\n",
    "\n",
    "# Preprocessing documents from path_list\n",
    "doc_preprocessor = MEMEXJsonPreprocessor(data_loc,\\\n",
    "                                file_list=path_list,max_docs=max_docs,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 21s, sys: 23.3 s, total: 5min 45s\n",
      "Wall time: 5min 5s\n"
     ]
    }
   ],
   "source": [
    "# Ingest data into Fonduer via parser\n",
    "corpus_parser = OmniParser(structural=True, lingual=True, visual=False)\n",
    "%time corpus_parser.apply(doc_preprocessor)#, parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 100\n",
      "Phrases: 31025\n"
     ]
    }
   ],
   "source": [
    "from fonduer.models import Document, Phrase\n",
    "\n",
    "# Checking database contents\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Phrases:\", session.query(Phrase).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599984"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_url_list = df_labeled['url'].tolist()\n",
    "labeled_url_set = set(labeled_url_list)\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "x= docs[5]\n",
    "x.name\n",
    "len(labeled_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \"pune', ' india\"']\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "pst\n",
      "kll\n",
      "km,m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Dividing into Test/Train, Extracting Features, Throttling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 80\n",
      "dev: 0\n",
      "test: 0\n"
     ]
    }
   ],
   "source": [
    "# Getting all documents parsed by Fonduer\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "dev_set_sz = np.round(ld*0.1)\n",
    "test_set_sz = np.round(ld*0.1)\n",
    "train_set_sz = ld - dev_set_sz - test_set_sz\n",
    "\n",
    "# Setting up train, dev, and test sets\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "\n",
    "# Creating list of (document name, Fonduer document object) tuples\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "shuffle(data)\n",
    "\n",
    "# Adding unlabeled data to train set, \n",
    "# labaled data to dev/test sets in alternating fashion\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    # Searching for the training data-- labeled_url_set is a set of all urls in the gold file\n",
    "    #print(doc_name)\n",
    "    if  (doc_name in labeled_url_set):\n",
    "        print(\"hi\")\n",
    "        if len(dev_docs)<=len(test_docs):\n",
    "            dev_docs.add(doc)\n",
    "        else:\n",
    "            test_docs.add(doc)\n",
    "    else:\n",
    "        if (i<train_set_sz):\n",
    "            train_docs.add(doc)\n",
    "#Printing length of train/test/dev sets\n",
    "print(\"train:\",len(train_docs))\n",
    "print(\"dev:\" ,len(dev_docs))\n",
    "print(\"test:\",len(test_docs))\n",
    "\n",
    "#Printing some filenames \n",
    "from pprint import pprint\n",
    "# pprint([x.name for x in train_docs])\n",
    "dev_list=set([x.name for x in dev_docs])\n",
    "test_list=set([x.name for x in test_docs])\n",
    "# pprint([x.name for x in test_docs])\n",
    "url_int = labeled_url_set & test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing matchers module and defining LocationMatcher\n",
    "from fonduer.snorkel.matchers import *\n",
    "location_matcher = LocationMatcher(longest_match_only=False) \n",
    "\n",
    "#importing NGrams and defining location_ngrams \n",
    "from fonduer.candidates import OmniNgrams\n",
    "location_ngrams = OmniNgrams(n_max=10, split_tokens=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.lf_helpers import *\n",
    "import re\n",
    "\n",
    "# Creating filter to eliminate mentions of currency  \n",
    "def location_currencies_filter(location):\n",
    "    list_currencies = [ \"dollar\", \"dollars\", \"lira\",\"kwacha\",\"rials\",\"rial\",\"dong\",\"dongs\",\"fuerte\",\"euro\",\n",
    "                       \"euros\",\"vatu\",\"som\",\"peso\",\"sterling\",\"sterlings\",\"soms\",\"pestos\",\n",
    "                       \"pounds\", \n",
    "                  \"pound\",\"dirham\",\"dirhams\",\"hryvnia\",\"manat\",\"manats\",\"liras\",\"lira\",\n",
    "                       \"dinar\",\"dinars\",\"pa'anga\",\"franc\",\"baht\",\"schilling\",\n",
    "                  \"somoni\",\"krona\",\"lilangeni\",\"rupee\",\"rand\",\"shilling\",\"leone\",\"riyal\",\"dobra\",\n",
    "                  \"tala\",\"ruble\",\"zloty\",\"peso\",\"sol\",\"quarani\",\"kina\",\"guinean\",\"balboa\",\"krone\",\"naira\",\n",
    "                  \"cordoba\",\"kyat\",\"metical\",\"togrog\",\"leu\",\"ouguiya\",\"rufiyaa\",\"ringgit\",\"kwacha\",\n",
    "                  \"ariary\",\"denar\",\"litas\",\"loti\",\"lats\",\"kip\",\"som\",\"won\",\"tenge\",\"yen\",\"shekel\",\"rupiah\",\n",
    "                  \"forint\",\"lempira\",\"gourde\",\"quetzal\",\"cedi\",\"lari\",\"dalasi\",\"cfp\",\"birr\",\"kroon\",\"nakfa\",\n",
    "                  \"cfa\",\"Peso\",\"koruna\",\"croatian\",\"colon\",\"yuan\",\"escudo\",\"cape\",\"riel\",\"lev\",\"real\"\n",
    "                  ,\"real\",\"mark\",\"boliviano\",\"ngultrum\",\"taka\",\"manat\",\"dram\",\"kwanza\",\"lek\",\"afghani\",\"renminbi\"]\n",
    "\n",
    "    \n",
    "    cand_right_tokens = list(get_right_ngrams(location,window=2))\n",
    "    for cand in cand_right_tokens:\n",
    "        if cand not in list_currencies:\n",
    "            return location\n",
    "\n",
    "# Setting candidate filter to location_currencies_filter\n",
    "candidate_filter = location_currencies_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================] 100%\n",
      "CPU times: user 11.9 s, sys: 88.6 ms, total: 12 s\n",
      "Wall time: 12.8 s\n",
      "Number of candidates: 45\n",
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 7.87 µs\n",
      "0\n",
      "[========================================] 100%\n",
      "Number of candidates: 11\n",
      "1\n",
      "[========================================] 100%\n",
      "Number of candidates: 6\n"
     ]
    }
   ],
   "source": [
    "from fonduer.candidates import CandidateExtractor\n",
    "\n",
    "# Defining candidate extractor\n",
    "candidate_extractor = CandidateExtractor(Location_Extraction,\n",
    "                                         [location_ngrams], [location_matcher],\n",
    "                                         candidate_filter=candidate_filter)\n",
    "\n",
    "# Extracting candidates from each split\n",
    "%time candidate_extractor.apply(train_docs, split=0)#, parallelism=4)\n",
    "print(\"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == 0).count())\n",
    "%time\n",
    "for i, docs in enumerate([dev_docs, test_docs]):\n",
    "    print(i)\n",
    "    candidate_extractor.apply(docs, split=i+1)#, parallelism=4)\n",
    "    print(\"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == i+1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fonduer.models import Sentences\n",
    "# cands_test = session.query(Location_Extraction).filter(Location_Extraction.split == 2).all()\n",
    "# len(cands_test)\n",
    "# y=cands_test[2]\n",
    "# y.name\n",
    "# # docs = session.query(Document).order_by(Document.name).all()\n",
    "# # x= docs[5]\n",
    "# # x.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_extraction(Span(\"b'Let\\\\'\", sentence=9439, chars=[0,3], words=[0,0]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Phrase (Doc: b'http://losangeles.backpage.com/FemaleEscorts/new-year-specials-super-sexy-cutie-with-a-bootie-freaky-waterf-ll-grip/90625227', Index: 252, Text: b\"Let\\\\'s get freaky\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 0\n",
    "print(cands_test[ind])\n",
    "cands_test[ind].get_parent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=79450, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=81167, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'Softest'\", sentence=91155, chars=[96,102], words=[23,23])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=90533, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=96218, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=85608, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'Area'\", sentence=82932, chars=[16,19], words=[4,4])),\n",
       " location_extraction(Span(\"b'Let\\\\'\", sentence=95972, chars=[0,3], words=[0,0])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=93863, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=66190, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'Optional - South'\", sentence=72244, chars=[0,15], words=[0,2])),\n",
       " location_extraction(Span(\"b'-'\", sentence=72244, chars=[9,9], words=[1,1])),\n",
       " location_extraction(Span(\"b'Optional - South Carolina'\", sentence=72244, chars=[0,24], words=[0,3])),\n",
       " location_extraction(Span(\"b'- South'\", sentence=72244, chars=[9,15], words=[1,2])),\n",
       " location_extraction(Span(\"b'South'\", sentence=72244, chars=[11,15], words=[2,2])),\n",
       " location_extraction(Span(\"b'- South Carolina'\", sentence=72244, chars=[9,24], words=[1,3])),\n",
       " location_extraction(Span(\"b'Carolina'\", sentence=72244, chars=[17,24], words=[3,3])),\n",
       " location_extraction(Span(\"b'South Carolina'\", sentence=72244, chars=[11,24], words=[2,3])),\n",
       " location_extraction(Span(\"b'Optional'\", sentence=72244, chars=[0,7], words=[0,0])),\n",
       " location_extraction(Span(\"b'Optional -'\", sentence=72244, chars=[0,9], words=[0,1])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=67753, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=92420, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'\\\\nNo Rushed'\", sentence=73177, chars=[67,77], words=[13,14])),\n",
       " location_extraction(Span(\"b'Rushed'\", sentence=73177, chars=[72,77], words=[14,14])),\n",
       " location_extraction(Span(\"b'\\\\nNo Rushed Session'\", sentence=73177, chars=[67,85], words=[13,15])),\n",
       " location_extraction(Span(\"b'Rushed Session'\", sentence=73177, chars=[72,85], words=[14,15])),\n",
       " location_extraction(Span(\"b'Session'\", sentence=73177, chars=[79,85], words=[15,15])),\n",
       " location_extraction(Span(\"b'\\\\nNo'\", sentence=73177, chars=[67,70], words=[13,13])),\n",
       " location_extraction(Span(\"b'Atlantic'\", sentence=64735, chars=[50,57], words=[8,8])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=88108, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=74653, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=77066, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=85144, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'Yourself'\", sentence=81749, chars=[28,35], words=[5,5])),\n",
       " location_extraction(Span(\"b'Treat'\", sentence=81749, chars=[22,26], words=[4,4])),\n",
       " location_extraction(Span(\"b'Treat Yourself'\", sentence=81749, chars=[22,35], words=[4,5])),\n",
       " location_extraction(Span(\"b'Valley New'\", sentence=76434, chars=[13,22], words=[3,4])),\n",
       " location_extraction(Span(\"b'New'\", sentence=76434, chars=[20,22], words=[4,4])),\n",
       " location_extraction(Span(\"b'Valley'\", sentence=76440, chars=[13,18], words=[3,3])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=76302, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'Valley'\", sentence=76434, chars=[13,18], words=[3,3])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=89448, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=89003, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=80399, chars=[32,56], words=[5,5])),\n",
       " location_extraction(Span(\"b'bocoperations@gmail.com.\\\\'\", sentence=84633, chars=[32,56], words=[5,5]))]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.query(Location_Extraction).filter(Location_Extraction.split == 0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 13.4 µs\n",
      "Copying location_extraction_feature to postgres\n",
      "b'COPY 106\\n'\n",
      "(106, 1914)\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 11.9 µs\n",
      "Copying location_extraction_feature_updates to postgres\n",
      "b'COPY 3\\n'\n",
      "(3, 1914)\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 12.6 µs\n",
      "Copying location_extraction_feature_updates to postgres\n",
      "b'COPY 4\\n'\n",
      "(4, 1914)\n"
     ]
    }
   ],
   "source": [
    "# Applying the featurizer (to get feature vector describing the input)\n",
    "from fonduer import BatchFeatureAnnotator\n",
    "session.rollback()\n",
    "featurizer = BatchFeatureAnnotator(Location_Extraction)\n",
    "# Running for train set -- replace_key_set = True!\n",
    "%time\n",
    "F_train = featurizer.apply(split=0, replace_key_set=True, parallelism=cfg['parallel'])\n",
    "session.rollback()\n",
    "print(F_train.shape)\n",
    "# Running for dev set -- replace_key_set = False! Uses same featuers as dev set\n",
    "%time \n",
    "F_dev = featurizer.apply(split=1, clear=True, replace_key_set=False, parallelism=cfg['parallel'])\n",
    "session.rollback()\n",
    "print(F_dev.shape)\n",
    "%time \n",
    "F_test = featurizer.apply(split=2, clear=True, replace_key_set=False, parallelism=cfg['parallel'])\n",
    "session.rollback()\n",
    "print(F_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Adding Gold Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import range\n",
    "import csv\n",
    "import codecs\n",
    "import pycountry\n",
    "#import us\n",
    "import editdistance\n",
    "\n",
    "from fonduer.snorkel.utils import ProgressBar\n",
    "from fonduer.snorkel.models import GoldLabel, GoldLabelKey\n",
    "\n",
    "# Defining function for getting gold labels\n",
    "# Could go in utils file later!\n",
    "\n",
    "def lookup_country_name(cn):\n",
    "    try:\n",
    "        out = pycountry.countries.lookup(cn).name\n",
    "    except:\n",
    "        out = 'no country'\n",
    "    return out\n",
    "\n",
    "def lookup_country_alpha3(cn):\n",
    "    try:\n",
    "        out = pycountry.countries.lookup(cn).alpha_3\n",
    "    except:\n",
    "        out = 'no country'\n",
    "    return out\n",
    "\n",
    "def lookup_country_alpha2(cn):\n",
    "    try:\n",
    "        out = pycountry.countries.lookup(cn).alpha_2\n",
    "    except:\n",
    "        out = 'no country'\n",
    "    return out\n",
    "\n",
    "def lookup_state_name(cn):\n",
    "    try:\n",
    "        out = us.states.lookup(val).name\n",
    "    except:\n",
    "        out = 'no state'\n",
    "    return out\n",
    "\n",
    "def lookup_state_abbr(cn):\n",
    "    try:\n",
    "        out = us.states.lookup(val).abbr\n",
    "    except:\n",
    "        out = 'no state'\n",
    "    return out\n",
    "\n",
    "def check_editdistance(val,targets):\n",
    "    for tgt in targets:\n",
    "        if editdistance.eval(val,tgt)<=3:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def match_val_targets(val,targets):\n",
    "    if val in targets: return True\n",
    "    if lookup_country_name(val).lower() in targets: return True\n",
    "    if lookup_country_alpha2(val).lower() in targets: return True\n",
    "    if lookup_country_alpha3(val).lower() in targets: return True\n",
    "    if lookup_state_name(val).lower() in targets: return True\n",
    "    if lookup_state_abbr(val).lower() in targets: return True\n",
    "    if any([a in val for a in targets]): return True\n",
    "    if check_editdistance(val,targets): return True\n",
    "    return False\n",
    "### function for cleaning gold labels\n",
    "def cleaning_gold_value(value):\n",
    "    y= value.replace('\"',\"\")  \n",
    "    z =y.replace(\"'\",\"\")\n",
    "    x = z.replace(\" \",\"\")\n",
    "    return x\n",
    "    \n",
    "def load_chtap_labels(session, candidate_class, df, target, annotator_name='gold'):\n",
    "    \n",
    "    # Database nonsense to make sure that there is a \"gold\" annotator \n",
    "    ak = session.query(GoldLabelKey).filter(GoldLabelKey.name == annotator_name).first()\n",
    "    if ak is None:\n",
    "        ak = GoldLabelKey(name=annotator_name)\n",
    "        session.add(ak)\n",
    "        session.commit()   \n",
    "    \n",
    "    # Getting all candidates from dev/test set only (splits 1 and 2)\n",
    "    candidates = session.query(candidate_class).filter(candidate_class.split != 0).all()\n",
    "    cand_total = len(candidates)\n",
    "    print('Loading', cand_total, 'candidate labels')\n",
    "    pb = ProgressBar(cand_total)\n",
    "    labels=[]\n",
    "\n",
    "    # For each candidate, add appropriate gold label\n",
    "    for i, c in enumerate(candidates):\n",
    "        pb.bar(i)\n",
    "        # Get document name for candidate\n",
    "        doc = c[0].sentence.document.name\n",
    "        \n",
    "        # Get text span for candidate\n",
    "        val = c[0].get_span().lower()\n",
    "        print(\"i am doc\")\n",
    "        print (doc)\n",
    "        #print(df['url'][i])\n",
    "        # Get location label from labeled dataframe (input)\n",
    "        target_strings = df[df['url']==doc][target].tolist()\n",
    "        print(\"hi\",target_strings)\n",
    "        # Handling location extraction\n",
    "        if target == 'gold_location':\n",
    "#             targets = target_strings[0].lower().split(',')\n",
    "#             print(targets)\n",
    "#             targets = [a.strip() for a in targets]\n",
    "                \n",
    "            if target_strings == []:\n",
    "                targets = ''\n",
    "    ## Cleaning the target value -- needs to be test when the dev and test set are created. it may need for loop.                #print (target_strings)\n",
    "            else:\n",
    "                targets = cleaning_gold_value(target_strings[0].lower())#.split(',')\n",
    "                print(targets)\n",
    "#                 targets = [a.strip() for a in targets]\n",
    "\n",
    "        #Keeping this in comments...don't know what it was for\n",
    "        #context_stable_ids = '~~'.join([i.stable_id for i in c.get_contexts()])\n",
    "        label = session.query(GoldLabel).filter(GoldLabel.key == ak).filter(GoldLabel.candidate == c).first()\n",
    "        if label is None:\n",
    "            # Matching target label string to extract span, adding TRUE label if found, FALSE if not\n",
    "            # This conditional could be improved (use regex, etc.)\n",
    "            if match_val_targets(val,targets):\n",
    "                label = GoldLabel(candidate=c, key=ak, value=1)\n",
    "            else:\n",
    "                label = GoldLabel(candidate=c, key=ak, value=-1)\n",
    "            session.add(label)\n",
    "            labels.append(label)\n",
    "    session.commit()\n",
    "    pb.close()\n",
    "    print(\"AnnotatorLabels created: %s\" % (len(labels),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-323-234966a618e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget_strings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_labeled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_labeled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'http://www.eroticmugshots.com/augusta-escorts/706-250-6723/?pid=49834552'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gold_location'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_strings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#                     targets = [a.strip() for a in targets]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "'''Testing Gold lable function'''\n",
    "import re\n",
    "target_strings=df_labeled[df_labeled['url']=='http://www.eroticmugshots.com/augusta-escorts/706-250-6723/?pid=49834552']['gold_location'].tolist()\n",
    "print(target_strings)\n",
    "targets = target_strings[0].lower().split(',')\n",
    "#                     targets = [a.strip() for a in targets]\n",
    "print (targets)\n",
    "#targets = [a.strip() for a in targets]\n",
    "# if 'pune'in targets[0]:\n",
    "#     print (\"hi\")\n",
    "#junker = re.compile('[[\\\" ` \\/]]')\n",
    "#junker_1 = re.compile('\"|/'')\n",
    "rslt_1 = targets[0].replace(\" \",\"\")\n",
    "#rslt_1= junker.sub('',targets[0])\n",
    "rsl= rslt_1.replace('\"',\"\")\n",
    "#rslt_1= junker_1.sub('',targets[0])\n",
    "rsl\n",
    "lst =[\"pst'\",'\"kll ', \"  km' \"]\n",
    "for i in lst:\n",
    "    y= i.replace('\"',\"\")\n",
    "    \n",
    "    \n",
    "    z =x.replace(\"'\",\"\")\n",
    "    x = y.replace(\" \",\"\")\n",
    "    print (type(z))\n",
    "def cleaning_gold_value(value):\n",
    "    y= value.replace('\"',\"\")  \n",
    "    z =y.replace(\"'\",\"\")\n",
    "    x = z.replace(\" \",\"\")\n",
    "    \n",
    "    return x\n",
    "for i in lst:\n",
    "    print(cleaning_gold_value(i))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' \"pune'\n",
    "def check_editdistance(val,targets):\n",
    "    for tgt in targets:\n",
    "        if editdistance.eval(val,tgt)<=3:\n",
    "            return True\n",
    "    return False\n",
    "check_editdistance('pune', 'pune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 17 candidate labels\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'location_extraction' object has no attribute 'sentence'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-339-a5e07791be9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gold_location'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mload_chtap_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLocation_Extraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mannotator_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-322-4a37af2e047f>\u001b[0m in \u001b[0;36mload_chtap_labels\u001b[0;34m(session, candidate_class, df, target, annotator_name)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# Get document name for candidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# Get text span for candidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'location_extraction' object has no attribute 'sentence'"
     ]
    }
   ],
   "source": [
    "# Adding gold labels to database\n",
    "session.rollback()\n",
    "target = 'gold_location'\n",
    "\n",
    "load_chtap_labels(session, Location_Extraction, df_labeled, target ,annotator_name='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t-1\n",
      "  (1, 0)\t-1\n",
      "  (2, 0)\t-1\n",
      "  (3, 0)\t-1\n",
      "  (4, 0)\t-1\n",
      "  (5, 0)\t-1\n",
      "  (6, 0)\t-1\n",
      "  (7, 0)\t-1\n",
      "  (8, 0)\t-1\n",
      "  (9, 0)\t-1\n",
      "  (10, 0)\t-1\n",
      "Dev Set Balance: 0.00 Percent Positive\n",
      "Test Set Balance: 0.00 Percent Positive\n",
      "  (0, 0)\t-1\n",
      "  (1, 0)\t-1\n",
      "  (2, 0)\t-1\n",
      "  (3, 0)\t-1\n",
      "  (4, 0)\t-1\n",
      "  (5, 0)\t-1\n"
     ]
    }
   ],
   "source": [
    "from fonduer.snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "print (L_gold_dev)\n",
    "print('Dev Set Balance: %0.2f Percent Positive' % (100*np.sum(L_gold_dev == 1)/L_gold_dev.shape[0]))\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "print('Test Set Balance: %0.2f Percent Positive' % (100*np.sum(L_gold_test == 1)/L_gold_test.shape[0]))\n",
    "print (L_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a single json file\n",
    "json_files = ['0000.flat.json','0001.flat.json','0002.flat.json','0003.flat.json','0004.flat.json']\n",
    "json_pd = pd.DataFrame()\n",
    "for fl in json_files:\n",
    "    pd_temp = doc_preprocessor._read_content_file(join(cfg['data_path'],cfg['unlabeled_data_path'],fl))\n",
    "    json_pd = json_pd.append(pd_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Records: 77329263\n",
      "Number of JSON Records 34503\n"
     ]
    }
   ],
   "source": [
    "# Checking number of files in csv: 77329263\n",
    "source_list = df_labeled['source'].tolist()\n",
    "source_set = set(source_list)\n",
    "url_list = df_labeled['url'].tolist()\n",
    "url_set = set(url_list)\n",
    "\n",
    "# Checking number of files in json\n",
    "json_url_list = json_pd['url'].tolist()\n",
    "json_url_set = set(json_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CSV Records: 66176483\n",
      "Number of JSON Records 34338\n",
      "Number of intersections: 102\n"
     ]
    }
   ],
   "source": [
    "url_int = json_url_set & url_set\n",
    "print('Number of CSV Records: %d' % len(url_set))\n",
    "print('Number of JSON Records %d' % len(json_url_set))\n",
    "print('Number of intersections: %d' % len(url_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adultsearch',\n",
       " 'anunico',\n",
       " 'asexyservice',\n",
       " 'backpage',\n",
       " 'cityvibe',\n",
       " 'cityxguide',\n",
       " 'classivox',\n",
       " 'craigslist',\n",
       " 'eroticmugshots',\n",
       " 'escortadsxxx',\n",
       " 'escortphonelist',\n",
       " 'escortsinca',\n",
       " 'escortsincollege',\n",
       " 'escortsintheus',\n",
       " 'happymassage',\n",
       " 'liveescortreviews',\n",
       " 'massagetroll',\n",
       " 'missingkids',\n",
       " 'myproviderguide',\n",
       " 'naughtyreviews',\n",
       " 'redbook',\n",
       " 'rubads',\n",
       " 'sipsap'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017-01-01T17:49:58',\n",
       " '2017-01-01T17:38:43',\n",
       " '2017-01-01T17:57:10',\n",
       " '2017-01-01T17:30:15',\n",
       " '2017-01-01T17:17:35',\n",
       " '2017-01-01T17:34:05',\n",
       " '2017-01-01T17:03:14',\n",
       " '2017-01-01T17:51:42',\n",
       " '2017-01-01T17:49:38',\n",
       " '2017-01-01T17:01:27',\n",
       " '2017-01-01T17:17:29',\n",
       " '2017-01-01T17:16:18',\n",
       " '2017-01-01T17:49:04',\n",
       " '2017-01-01T17:24:25',\n",
       " '2017-01-01T17:28:59',\n",
       " '2017-01-01T17:28:59',\n",
       " '2017-01-01T17:10:11',\n",
       " '2017-01-01T17:43:38',\n",
       " '2017-01-01T17:30:13',\n",
       " '2017-01-01T17:43:31',\n",
       " '2017-01-01T17:11:32',\n",
       " '2017-01-01T17:28:49',\n",
       " '2017-01-01T17:28:43',\n",
       " '2017-01-01T17:29:11',\n",
       " '2017-01-01T17:59:42',\n",
       " '2017-01-01T17:05:24',\n",
       " '2017-01-01T17:05:24',\n",
       " '2017-01-01T17:24:09',\n",
       " '2017-01-01T17:54:41',\n",
       " '2017-01-01T17:41:46',\n",
       " '2017-01-01T17:57:14',\n",
       " '2017-01-01T17:58:30',\n",
       " '2017-01-01T17:43:32',\n",
       " '2017-01-01T17:58:32',\n",
       " '2017-01-01T17:41:52',\n",
       " '2017-01-01T17:37:41',\n",
       " '2017-01-01T17:56:08',\n",
       " '2017-01-01T17:38:36',\n",
       " '2017-01-01T17:37:29',\n",
       " '2017-01-01T17:54:33',\n",
       " '2017-01-01T17:21:33',\n",
       " '2017-01-01T17:56:44',\n",
       " '2017-01-01T17:22:36',\n",
       " '2017-01-01T17:56:40',\n",
       " '2017-01-01T17:58:09',\n",
       " '2017-01-01T17:24:13',\n",
       " '2017-01-01T17:08:39',\n",
       " '2017-01-01T17:39:12',\n",
       " '2017-01-01T17:04:37',\n",
       " '2017-01-01T17:05:12']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_ts_list = json_pd['timestamp'].tolist()[-50:]\n",
    "json_ts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_labeled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0e2f45bc3b4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'location'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mload_chtap_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLocation_Extraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mannotator_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_labeled' is not defined"
     ]
    }
   ],
   "source": [
    "# Adding gold labels to database\n",
    "session.rollback()\n",
    "target = 'location'\n",
    "\n",
    "load_chtap_labels(session, Location_Extraction, df_labeled, target ,annotator_name='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Set Balance: 0.00 Percent Positive\n",
      "Test Set Balance: 0.00 Percent Positive\n"
     ]
    }
   ],
   "source": [
    "# Check class balance on dev/test\n",
    "from fonduer.snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "print('Dev Set Balance: %0.2f Percent Positive' % (100*np.sum(L_gold_dev == 1)/L_gold_dev.shape[0]))\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "print('Test Set Balance: %0.2f Percent Positive' % (100*np.sum(L_gold_test == 1)/L_gold_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Creating LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " cand_dev = session.query(Location_Extraction).filter(Location_Extraction.split == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for true/false/abstain\n",
    "TRUE,FALSE,ABSTAIN = 1,-1,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.lf_helpers import *\n",
    "import re\n",
    "# Defining LFs\n",
    "# LF API is here: http://web.stanford.edu/~lwhsiao/api/\n",
    "\n",
    "def LF_in_breadcrumbs_1(c):\n",
    "    parent_text = c.get_parent().text\n",
    "    return FALSE if '>' in parent_text else ABSTAIN\n",
    "\n",
    "def LF_long_candidate(c):\n",
    "    parent_text = c.get_parent().text\n",
    "    return FALSE if len(parent_text) > 1000 else ABSTAIN\n",
    "\n",
    "def LF_common_real_words(c):\n",
    "    reg_pos = re.compile('other cities|since|day|escorts',re.IGNORECASE)\n",
    "    if reg_pos.search(c.get_parent().text):\n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "#def LF_in_breadcrumbs_2(c):\n",
    "#    attributes = list(get_attributes(c))\n",
    "#    return TRUE if ('class=breadcrumbs'in attributes) or ('class=inside_scroll' in attributes) else ABSTAIN\n",
    "\n",
    "def LF_head_in_tag(c):\n",
    "    tags = list(get_ancestor_tag_names(c))\n",
    "    return FALSE if 'head' in tags else TRUE\n",
    "\n",
    "def LF_body_in_tag(c):\n",
    "    tags = list(get_ancestor_tag_names(c))\n",
    "    return TRUE if 'body' in tags else FALSE\n",
    "\n",
    "def LF_table_in_tag(c):\n",
    "    tags = list(get_ancestor_tag_names(c))\n",
    "    return TRUE if 'table' in tags else ABSTAIN\n",
    "\n",
    "def LF_to_left(c):\n",
    "    return TRUE if overlap(\n",
    "      ['location','locall','outcall','stay','live','available','female escort'], \n",
    "        get_left_ngrams(c, window=3)) else FALSE\n",
    "\n",
    "def LF_to_right(c):\n",
    "    return TRUE if overlap(\n",
    "      ['escorts','incall','outcall','stay','live','available','female escort'], \n",
    "        list(get_right_ngrams(c, window=5))) else ABSTAIN\n",
    "# Need more of these...can check tutorials for inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<function LF_in_breadcrumbs_1 at 0x7fc9dc0c1378>, <function LF_head_in_tag at 0x7fc9dbef0620>, <function LF_to_right at 0x7fc9dbef0ae8>, <function LF_table_in_tag at 0x7fc9dbef0a60>, <function LF_long_candidate at 0x7fc9db48b158>, <function LF_common_real_words at 0x7fc9db48bea0>]\n"
     ]
    }
   ],
   "source": [
    "# Collect LFs in list\n",
    "lfs_location = [LF_in_breadcrumbs_1,\n",
    "                #LF_in_breadcrumbs_2,\n",
    "                LF_head_in_tag,\n",
    "                #LF_body_in_tag,\n",
    "                LF_to_right,\n",
    "                #LF_to_left,\n",
    "                LF_table_in_tag,\n",
    "                LF_long_candidate,\n",
    "                LF_common_real_words]\n",
    "print (lfs_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Running Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying location_extraction_label to postgres\n",
      "b'COPY 7\\n'\n",
      "CPU times: user 76 ms, sys: 672 ms, total: 748 ms\n",
      "Wall time: 3.93 s\n",
      "Copying location_extraction_label_updates to postgres\n",
      "b'COPY 6\\n'\n",
      "CPU times: user 76 ms, sys: 632 ms, total: 708 ms\n",
      "Wall time: 3.91 s\n",
      "Copying location_extraction_label_updates to postgres\n",
      "b'COPY 12\\n'\n",
      "CPU times: user 76 ms, sys: 636 ms, total: 712 ms\n",
      "Wall time: 3.91 s\n",
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "from fonduer import BatchLabelAnnotator\n",
    "\n",
    "# Annotating candidats using LFs (clear=True replaced existing)\n",
    "labeler = BatchLabelAnnotator(Location_Extraction, lfs=lfs_location)\n",
    "%time L_train = labeler.apply(split=0, clear=True, parallelism=cfg['parallel'],update_keys =True)\n",
    "%time L_dev = labeler.apply(split=1, clear=True, parallelism=cfg['parallel'],update_keys =True)\n",
    "%time L_test = labeler.apply(split=2, clear=True, parallelism=cfg['parallel'],update_keys =True)\n",
    "print(L_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a)Computing Individual LF Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fonduer.snorkel.lf_helpers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-0fadcb60f77a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfonduer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnorkel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlf_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest_LF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlfs_location\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_LF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotator_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fonduer.snorkel.lf_helpers'"
     ]
    }
   ],
   "source": [
    "from fonduer.snorkel.lf_helpers import test_LF\n",
    "for lf in lfs_location:\n",
    "    print(lf.__name__)\n",
    "    tp, fp, tn, fn = test_LF(session, lf, split=1, annotator_name='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_extraction(Span(\"b'\\xef\\xb8\\x8f'\", sentence=1163, chars=[0,0], words=[0,0]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing a candidate from dev set\n",
    "ind = 0\n",
    "print(L_dev.get_candidate(session, ind))\n",
    "print(L_gold_dev[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6,) (0,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/fonduer/async_annotations.py\u001b[0m in \u001b[0;36mlf_stats\u001b[0;34m(self, labels, est_accs)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mcol_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Empirical Acc.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_tp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_fp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/fonduer/snorkel/utils.py\u001b[0m in \u001b[0;36mmatrix_tp\u001b[0;34m(L, labels)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmatrix_tp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return np.ravel([\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     ])\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/fonduer/snorkel/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmatrix_tp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return np.ravel([\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     ])\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6,) (0,) "
     ]
    }
   ],
   "source": [
    "# Loading assessing LF performance vs. gold labels\n",
    "from fonduer.snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "%time L_dev.lf_stats(L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10 µs\n",
      "============================================================\n",
      "[1] Testing epochs = 1.00e+02, step_size = 1.00e-03, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "[GenerativeModel] Model saved as <GenerativeModel_0>.\n",
      "[GenerativeModel] Model saved as <GenerativeModel_best>.\n",
      "============================================================\n",
      "[2] Testing epochs = 5.00e+01, step_size = 1.00e-02, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[3] Testing epochs = 1.00e+02, step_size = 1.00e-02, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[4] Testing epochs = 1.00e+02, step_size = 1.00e-03, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[5] Testing epochs = 2.00e+01, step_size = 1.00e-05, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "[GenerativeModel] Model saved as <GenerativeModel_4>.\n",
      "[GenerativeModel] Model saved as <GenerativeModel_best>.\n",
      "============================================================\n",
      "[6] Testing epochs = 1.00e+02, step_size = 1.00e-06, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[7] Testing epochs = 1.00e+02, step_size = 1.00e-02, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[8] Testing epochs = 5.00e+01, step_size = 1.00e-02, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[9] Testing epochs = 1.00e+02, step_size = 1.00e-06, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[10] Testing epochs = 5.00e+01, step_size = 1.00e-03, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[11] Testing epochs = 1.00e+02, step_size = 1.00e-05, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[12] Testing epochs = 5.00e+01, step_size = 1.00e-04, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[13] Testing epochs = 2.00e+01, step_size = 1.00e-06, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[14] Testing epochs = 5.00e+01, step_size = 1.00e-04, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[15] Testing epochs = 1.00e+02, step_size = 1.00e-06, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "[GenerativeModel] Model <GenerativeModel_4> loaded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>step_size</th>\n",
       "      <th>decay</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>Rec.</th>\n",
       "      <th>F-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  step_size  decay     Prec.      Rec.       F-1\n",
       "4       20   0.000010   1.00  0.877246  0.983221  0.927215\n",
       "5      100   0.000001   1.00  0.877246  0.983221  0.927215\n",
       "8      100   0.000001   0.95  0.877246  0.983221  0.927215\n",
       "10     100   0.000010   0.90  0.877246  0.983221  0.927215\n",
       "11      50   0.000100   0.90  0.877246  0.983221  0.927215\n",
       "12      20   0.000001   0.90  0.877246  0.983221  0.927215\n",
       "13      50   0.000100   0.95  0.877246  0.983221  0.927215\n",
       "14     100   0.000001   1.00  0.877246  0.983221  0.927215\n",
       "0      100   0.001000   1.00  0.863333  0.869128  0.866221\n",
       "1       50   0.010000   0.95  0.863333  0.869128  0.866221\n",
       "2      100   0.010000   1.00  0.863333  0.869128  0.866221\n",
       "3      100   0.001000   0.95  0.863333  0.869128  0.866221\n",
       "6      100   0.010000   1.00  0.863333  0.869128  0.866221\n",
       "7       50   0.010000   0.90  0.863333  0.869128  0.866221\n",
       "9       50   0.001000   0.95  0.863333  0.869128  0.866221"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "from snorkel.learning import RandomSearch\n",
    "\n",
    "param_ranges = {\n",
    "    'step_size' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "    'decay' : [1.0, 0.95, 0.9],\n",
    "    'epochs' : [20, 50, 100]\n",
    "}\n",
    "\n",
    "searcher = RandomSearch(GenerativeModel, param_ranges, L_train, n=15)\n",
    "\n",
    "%time\n",
    "gen_model, run_stats = searcher.fit(L_dev, L_gold_dev)\n",
    "run_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e18e3ad50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing and plotting training marginals\n",
    "train_marginals = gen_model.marginals(L_train)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84921912, 1.07673239, 0.85064543, 0.94957098, 0.85142461,\n",
       "       0.85070572, 0.83974904, 0.84747935, 0.97313617, 0.84885964])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing LF accuracies\n",
    "gen_model.weights.lf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "      <th>Learned Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_in_breadcrumbs</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.849219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_head_in_tag</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.765306</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>259</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>249</td>\n",
       "      <td>0.863946</td>\n",
       "      <td>1.076732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_body_in_tag</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.850645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_to_right</th>\n",
       "      <td>3</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>0.057823</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_in_breadcrumbs_2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.851425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_table_in_tag</th>\n",
       "      <td>5</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_in_breadcrumbs_1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>249</td>\n",
       "      <td>0.984190</td>\n",
       "      <td>0.839749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_to_left</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.847479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_real_words</th>\n",
       "      <td>8</td>\n",
       "      <td>0.263605</td>\n",
       "      <td>0.263605</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>141</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909677</td>\n",
       "      <td>0.973136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_long_candidate</th>\n",
       "      <td>9</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>249</td>\n",
       "      <td>0.984190</td>\n",
       "      <td>0.848860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      j  Coverage  Overlaps  Conflicts   TP  FP  FN   TN  \\\n",
       "LF_in_breadcrumbs     0  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_head_in_tag        1  1.000000  0.765306   0.059524  259  41  39  249   \n",
       "LF_body_in_tag        2  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_to_right           3  0.108844  0.108844   0.057823   64   0   0    0   \n",
       "LF_in_breadcrumbs_2   4  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_table_in_tag       5  0.040816  0.040816   0.000000   24   0   0    0   \n",
       "LF_in_breadcrumbs_1   6  0.430272  0.430272   0.000000    0   0   4  249   \n",
       "LF_to_left            7  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_common_real_words  8  0.263605  0.263605   0.059524  141  14   0    0   \n",
       "LF_long_candidate     9  0.430272  0.430272   0.000000    0   0   4  249   \n",
       "\n",
       "                      Empirical Acc.  Learned Acc.  \n",
       "LF_in_breadcrumbs                NaN      0.849219  \n",
       "LF_head_in_tag              0.863946      1.076732  \n",
       "LF_body_in_tag                   NaN      0.850645  \n",
       "LF_to_right                 1.000000      0.949571  \n",
       "LF_in_breadcrumbs_2              NaN      0.851425  \n",
       "LF_table_in_tag             1.000000      0.850706  \n",
       "LF_in_breadcrumbs_1         0.984190      0.839749  \n",
       "LF_to_left                       NaN      0.847479  \n",
       "LF_common_real_words        0.909677      0.973136  \n",
       "LF_long_candidate           0.984190      0.848860  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pringint LF stats post-learning\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "prec, rec, f1 = gen_model.score(L_dev, L_gold_dev)\n",
    "L_dev.lf_stats(L_gold_dev, gen_model.weights.lf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[1] Testing dropout = 0.00e+00, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.06s)\tAverage loss=0.788678\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 25 (0.38s)\tAverage loss=0.480607\tDev F1=69.80\n",
      "[SparseLogisticRegression] Epoch 50 (0.97s)\tAverage loss=0.355863\tDev F1=67.99\n",
      "[SparseLogisticRegression] Epoch 75 (1.27s)\tAverage loss=0.311128\tDev F1=71.17\n",
      "[SparseLogisticRegression] Epoch 100 (1.59s)\tAverage loss=0.292562\tDev F1=67.70\n",
      "[SparseLogisticRegression] Epoch 125 (1.90s)\tAverage loss=0.283264\tDev F1=64.73\n",
      "[SparseLogisticRegression] Epoch 150 (2.20s)\tAverage loss=0.277872\tDev F1=63.38\n",
      "[SparseLogisticRegression] Epoch 175 (2.50s)\tAverage loss=0.274410\tDev F1=63.70\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.87s)\tAverage loss=0.272111\tDev F1=63.86\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (3.00s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-199\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.638596491228\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_0>\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_best>\n",
      "============================================================\n",
      "[2] Testing dropout = 5.00e-01, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 25 (0.36s)\tAverage loss=0.480607\tDev F1=69.80\n",
      "[SparseLogisticRegression] Epoch 50 (0.66s)\tAverage loss=0.355863\tDev F1=67.99\n",
      "[SparseLogisticRegression] Epoch 75 (0.98s)\tAverage loss=0.311128\tDev F1=71.17\n",
      "[SparseLogisticRegression] Epoch 100 (1.29s)\tAverage loss=0.292562\tDev F1=67.70\n",
      "[SparseLogisticRegression] Epoch 125 (1.59s)\tAverage loss=0.283264\tDev F1=64.73\n",
      "[SparseLogisticRegression] Epoch 150 (1.90s)\tAverage loss=0.277872\tDev F1=63.38\n",
      "[SparseLogisticRegression] Epoch 175 (2.22s)\tAverage loss=0.274410\tDev F1=63.70\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.60s)\tAverage loss=0.272111\tDev F1=63.86\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (2.74s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-199\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.638596491228\n",
      "============================================================\n",
      "[3] Testing dropout = 0.00e+00, lr = 1.00e-02\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 25 (0.75s)\tAverage loss=0.275965\tDev F1=65.90\n",
      "[SparseLogisticRegression] Epoch 50 (1.07s)\tAverage loss=0.264111\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 75 (1.37s)\tAverage loss=0.263621\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 100 (1.67s)\tAverage loss=0.263494\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 125 (1.98s)\tAverage loss=0.263446\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 150 (2.28s)\tAverage loss=0.263417\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 175 (2.59s)\tAverage loss=0.263397\tDev F1=66.97\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.96s)\tAverage loss=0.263385\tDev F1=66.97\n",
      "[SparseLogisticRegression] Training done (3.01s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.669714285714\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_2>\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_best>\n",
      "============================================================\n",
      "[4] Testing dropout = 0.00e+00, lr = 1.00e-02\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 25 (0.36s)\tAverage loss=0.275965\tDev F1=65.90\n",
      "[SparseLogisticRegression] Epoch 50 (0.67s)\tAverage loss=0.264111\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 75 (0.97s)\tAverage loss=0.263621\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 100 (1.28s)\tAverage loss=0.263494\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 125 (1.59s)\tAverage loss=0.263446\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 150 (1.90s)\tAverage loss=0.263417\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 175 (2.21s)\tAverage loss=0.263397\tDev F1=66.97\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.59s)\tAverage loss=0.263385\tDev F1=66.97\n",
      "[SparseLogisticRegression] Training done (2.64s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.669714285714\n",
      "============================================================\n",
      "[5] Testing dropout = 0.00e+00, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 25 (0.36s)\tAverage loss=0.480607\tDev F1=69.80\n",
      "[SparseLogisticRegression] Epoch 50 (0.67s)\tAverage loss=0.355863\tDev F1=67.99\n",
      "[SparseLogisticRegression] Epoch 75 (0.97s)\tAverage loss=0.311128\tDev F1=71.17\n",
      "[SparseLogisticRegression] Epoch 100 (1.55s)\tAverage loss=0.292562\tDev F1=67.70\n",
      "[SparseLogisticRegression] Epoch 125 (1.86s)\tAverage loss=0.283264\tDev F1=64.73\n",
      "[SparseLogisticRegression] Epoch 150 (2.17s)\tAverage loss=0.277872\tDev F1=63.38\n",
      "[SparseLogisticRegression] Epoch 175 (2.48s)\tAverage loss=0.274410\tDev F1=63.70\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.84s)\tAverage loss=0.272111\tDev F1=63.86\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (2.97s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-199\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.638596491228\n",
      "============================================================\n",
      "[6] Testing dropout = 0.00e+00, lr = 1.00e-05\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.36s)\tAverage loss=0.784480\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 50 (0.67s)\tAverage loss=0.780311\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 75 (0.97s)\tAverage loss=0.776173\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 100 (1.28s)\tAverage loss=0.772065\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 125 (1.58s)\tAverage loss=0.767988\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 150 (1.89s)\tAverage loss=0.763942\tDev F1=63.09\n",
      "[SparseLogisticRegression] Epoch 175 (2.22s)\tAverage loss=0.759925\tDev F1=63.09\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseLogisticRegression] Epoch 199 (2.58s)\tAverage loss=0.756098\tDev F1=63.09\n",
      "[SparseLogisticRegression] Training done (2.63s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.630872483221\n",
      "============================================================\n",
      "[7] Testing dropout = 0.00e+00, lr = 1.00e-06\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.36s)\tAverage loss=0.788257\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 50 (0.66s)\tAverage loss=0.787837\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 75 (0.96s)\tAverage loss=0.787417\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 100 (1.27s)\tAverage loss=0.786998\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 125 (1.58s)\tAverage loss=0.786578\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 150 (1.89s)\tAverage loss=0.786159\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 175 (2.42s)\tAverage loss=0.785740\tDev F1=62.16\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.80s)\tAverage loss=0.785338\tDev F1=62.16\n",
      "[SparseLogisticRegression] Training done (2.85s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.621621621622\n",
      "============================================================\n",
      "[8] Testing dropout = 5.00e-01, lr = 1.00e-02\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.06s)\tAverage loss=0.788678\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 25 (0.40s)\tAverage loss=0.275965\tDev F1=65.90\n",
      "[SparseLogisticRegression] Epoch 50 (0.70s)\tAverage loss=0.264111\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 75 (1.02s)\tAverage loss=0.263621\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 100 (1.32s)\tAverage loss=0.263494\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 125 (1.63s)\tAverage loss=0.263446\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 150 (1.93s)\tAverage loss=0.263417\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 175 (2.23s)\tAverage loss=0.263397\tDev F1=66.97\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.61s)\tAverage loss=0.263385\tDev F1=66.97\n",
      "[SparseLogisticRegression] Training done (2.65s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.669714285714\n",
      "============================================================\n",
      "[9] Testing dropout = 5.00e-01, lr = 1.00e-02\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 25 (0.38s)\tAverage loss=0.275965\tDev F1=65.90\n",
      "[SparseLogisticRegression] Epoch 50 (0.69s)\tAverage loss=0.264111\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 75 (0.99s)\tAverage loss=0.263621\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 100 (1.30s)\tAverage loss=0.263494\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 125 (1.60s)\tAverage loss=0.263446\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 150 (1.91s)\tAverage loss=0.263417\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 175 (2.25s)\tAverage loss=0.263397\tDev F1=66.97\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.61s)\tAverage loss=0.263385\tDev F1=66.97\n",
      "[SparseLogisticRegression] Training done (2.66s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.669714285714\n",
      "============================================================\n",
      "[10] Testing dropout = 0.00e+00, lr = 1.00e-06\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.37s)\tAverage loss=0.788257\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 50 (0.68s)\tAverage loss=0.787837\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 75 (0.99s)\tAverage loss=0.787417\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 100 (1.30s)\tAverage loss=0.786998\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 125 (1.60s)\tAverage loss=0.786578\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 150 (1.91s)\tAverage loss=0.786159\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 175 (2.21s)\tAverage loss=0.785740\tDev F1=62.16\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.59s)\tAverage loss=0.785338\tDev F1=62.16\n",
      "[SparseLogisticRegression] Training done (2.64s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.621621621622\n",
      "============================================================\n",
      "[11] Testing dropout = 5.00e-01, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 25 (0.37s)\tAverage loss=0.480607\tDev F1=69.80\n",
      "[SparseLogisticRegression] Epoch 50 (0.68s)\tAverage loss=0.355863\tDev F1=67.99\n",
      "[SparseLogisticRegression] Epoch 75 (0.98s)\tAverage loss=0.311128\tDev F1=71.17\n",
      "[SparseLogisticRegression] Epoch 100 (1.29s)\tAverage loss=0.292562\tDev F1=67.70\n",
      "[SparseLogisticRegression] Epoch 125 (1.60s)\tAverage loss=0.283264\tDev F1=64.73\n",
      "[SparseLogisticRegression] Epoch 150 (1.91s)\tAverage loss=0.277872\tDev F1=63.38\n",
      "[SparseLogisticRegression] Epoch 175 (2.21s)\tAverage loss=0.274410\tDev F1=63.70\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.59s)\tAverage loss=0.272111\tDev F1=63.86\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (2.72s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-199\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.638596491228\n",
      "============================================================\n",
      "[12] Testing dropout = 5.00e-01, lr = 1.00e-05\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.37s)\tAverage loss=0.784480\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 50 (0.68s)\tAverage loss=0.780311\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 75 (1.00s)\tAverage loss=0.776173\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 100 (1.30s)\tAverage loss=0.772065\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 125 (1.62s)\tAverage loss=0.767988\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 150 (1.93s)\tAverage loss=0.763942\tDev F1=63.09\n",
      "[SparseLogisticRegression] Epoch 175 (2.24s)\tAverage loss=0.759925\tDev F1=63.09\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.62s)\tAverage loss=0.756098\tDev F1=63.09\n",
      "[SparseLogisticRegression] Training done (2.67s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.630872483221\n",
      "============================================================\n",
      "[13] Testing dropout = 0.00e+00, lr = 1.00e-04\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.06s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.38s)\tAverage loss=0.747784\tDev F1=63.55\n",
      "[SparseLogisticRegression] Epoch 50 (0.70s)\tAverage loss=0.709641\tDev F1=64.23\n",
      "[SparseLogisticRegression] Epoch 75 (1.00s)\tAverage loss=0.674383\tDev F1=64.36\n",
      "[SparseLogisticRegression] Epoch 100 (1.32s)\tAverage loss=0.641896\tDev F1=65.80\n",
      "[SparseLogisticRegression] Epoch 125 (1.63s)\tAverage loss=0.612017\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 150 (1.95s)\tAverage loss=0.584581\tDev F1=66.34\n",
      "[SparseLogisticRegression] Epoch 175 (2.26s)\tAverage loss=0.559421\tDev F1=66.34\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.63s)\tAverage loss=0.537254\tDev F1=66.34\n",
      "[SparseLogisticRegression] Training done (2.68s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.663430420712\n",
      "============================================================\n",
      "[14] Testing dropout = 5.00e-01, lr = 1.00e-06\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.37s)\tAverage loss=0.788257\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 50 (0.89s)\tAverage loss=0.787837\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 75 (1.20s)\tAverage loss=0.787417\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 100 (1.52s)\tAverage loss=0.786998\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 125 (1.84s)\tAverage loss=0.786578\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 150 (2.17s)\tAverage loss=0.786159\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 175 (2.48s)\tAverage loss=0.785740\tDev F1=62.16\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.85s)\tAverage loss=0.785338\tDev F1=62.16\n",
      "[SparseLogisticRegression] Training done (2.90s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.621621621622\n",
      "============================================================\n",
      "[15] Testing dropout = 0.00e+00, lr = 1.00e-04\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.37s)\tAverage loss=0.747784\tDev F1=63.55\n",
      "[SparseLogisticRegression] Epoch 50 (0.68s)\tAverage loss=0.709641\tDev F1=64.23\n",
      "[SparseLogisticRegression] Epoch 75 (0.98s)\tAverage loss=0.674383\tDev F1=64.36\n",
      "[SparseLogisticRegression] Epoch 100 (1.29s)\tAverage loss=0.641896\tDev F1=65.80\n",
      "[SparseLogisticRegression] Epoch 125 (1.60s)\tAverage loss=0.612017\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 150 (1.91s)\tAverage loss=0.584581\tDev F1=66.34\n",
      "[SparseLogisticRegression] Epoch 175 (2.22s)\tAverage loss=0.559421\tDev F1=66.34\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.62s)\tAverage loss=0.537254\tDev F1=66.34\n",
      "[SparseLogisticRegression] Training done (2.67s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.663430420712\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression_2/SparseLogisticRegression_2-0\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression_2>\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "\n",
    "disc_model = SparseLogisticRegression\n",
    "param_ranges = {\n",
    "    'lr' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "    'dropout' : [0.0, 0.5]\n",
    "}\n",
    "\n",
    "model_hyperparams = {\n",
    "    'n_epochs' : 200,\n",
    "    'rebalance' : 0.5,\n",
    "    'print_freq' : 25\n",
    "}\n",
    "\n",
    "# We now add a session and probabilistic labels, as well as pass in the candidates\n",
    "# instead of the label matrix\n",
    "searcher = RandomSearch(disc_model, param_ranges, F_train, Y_train=train_marginals, n=15,\n",
    "    model_hyperparams=model_hyperparams)\n",
    "\n",
    "# We now pass in the development candidates and the gold development labels\n",
    "trained_model, run_stats = searcher.fit(F_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate discriminative on test set \n",
    "L_gold_test = np.array(load_gold_labels(session, annotator_name='gold', split=2).todense()).squeeze()\n",
    "# Get candidates, discriminative model outputs, and discriminative model predicts\n",
    "test_candidates = [F_test.get_candidate(session, i) for i in range(F_test.shape[0])]\n",
    "test_score = np.array(trained_model.predictions(F_test))\n",
    "true_pred = [test_candidates[_] for _ in np.nditer(np.where(test_score > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5475113122171946\n"
     ]
    }
   ],
   "source": [
    "# L_gold_test\n",
    "corr = [ test_score[i] == L_gold_test[i] for i in range(len(test_score))]\n",
    "acc = np.sum(corr)/len(corr)\n",
    "print (acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.918552036199095\n"
     ]
    }
   ],
   "source": [
    "# Assessing values \n",
    "gen_model_preds = (gen_model.marginals(L_test)>0.5)*2-1\n",
    "gen_corr = [ gen_model_preds[i] == L_gold_test[i] for i in range(len(gen_model_preds))]\n",
    "gen_acc = np.sum(gen_corr)/len(gen_corr)\n",
    "print(gen_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9217687074829932\n"
     ]
    }
   ],
   "source": [
    "# Assessing values\n",
    "L_gold_dev = np.array(load_gold_labels(session, annotator_name='gold', split=1).todense()).squeeze()\n",
    "gen_model_preds = (gen_model.marginals(L_dev)>0.5)*2-1\n",
    "gen_corr = [ gen_model_preds[i] == L_gold_dev[i] for i in range(len(gen_model_preds))]\n",
    "gen_acc = np.sum(gen_corr)/len(gen_corr)\n",
    "print(gen_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Creating and Saving Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#getting google place and geocoding APIs\n",
    "import googlemaps as gm\n",
    "import gmaps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import MultiPoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "maps_api_key = 'AIzaSyA0Veo5Lc6JOwDjNgQvPEhQB4AiZcrYQGI'\n",
    "gmaps.configure(api_key=maps_api_key)\n",
    "\n",
    "def get_possible_locations(plc):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    plc: string describing place to match\n",
    "\n",
    "    OUTPUTS\n",
    "    qo: full json structure returned from API call\n",
    "    cl: list of candidate location strings\n",
    "    \"\"\" \n",
    "    api_key = 'AIzaSyDbk3lLZHuQVKDRBN99_oz-p4AJjIzhA0w'\n",
    "    gms = gm.Client(key=api_key)\n",
    "    qo = gm.places.places_autocomplete(gms,plc)\n",
    "    cl = [a['description'] for a in qo]\n",
    "    return qo,cl\n",
    "\n",
    "def get_geocode(plc):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    plc: string describing place to match\n",
    "\n",
    "    OUTPUTS\n",
    "    qo full json structure returned from API call\n",
    "    (lat,lon): lat-lon tuple\n",
    "    \"\"\"\n",
    "    api_key = 'AIzaSyBlLyOaasYMgMxFGUh2jJyxIG0_pZFF_jM'\n",
    "    gms = gm.Client(key=api_key)\n",
    "    qo = gm.geocoding.geocode(gms,plc)\n",
    "    lat = qo[0]['geometry']['location']['lat']\n",
    "    lng = qo[0]['geometry']['location']['lng']\n",
    "    return qo,(lat,lng)\n",
    "\n",
    "def slice_pd_by_cont(dfm,col,val,pres=True,lower=False,union=False):\n",
    "    \"\"\"\n",
    "    Returns dataframe where column values include/exclude values in provided list\n",
    "    \n",
    "    INPUTS:\n",
    "    dfm: dataframe\n",
    "    col: column header\n",
    "    val: list of strings to include/ignore\n",
    "    pres: true to include, false to exclude\n",
    "    union: include union of these values\n",
    "    \"\"\"\n",
    "    if union:\n",
    "        val = ['|'.join(val)]\n",
    "    for vl in val:\n",
    "        if ~lower:\n",
    "            if pres:\n",
    "                dfm = dfm.loc[dfm[col].str.contains(vl,na=False)]\n",
    "            else:\n",
    "                dfm = dfm.loc[~dfm[col].str.contains(vl,na=False)]\n",
    "        else:\n",
    "            if pres:\n",
    "                dfm = dfm.loc[dfm[col].str.lower().str.contains(vl,na=False)]\n",
    "            else:\n",
    "                dfm = dfm.loc[~dfm[col].str.lower().str.contains(vl,na=False)]\n",
    "    return dfm\n",
    "\n",
    "def map_candidates_and_centroid(dfm):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    dfm: dataframe containing at least latitude, longitude\n",
    "    \n",
    "    OUTPUT\n",
    "    centroid: np array of lat/lon of location centroid\n",
    "    \"\"\"\n",
    "    df_cans = dfm\n",
    "    df_cans_map = dfm[['latitude','longitude']]\n",
    "    df_cans['lat_long'] = df_cans[['latitude', 'longitude']].apply(tuple, axis=1)\n",
    "    point_tup_lst = df_cans['lat_long'].tolist()\n",
    "    points = MultiPoint(point_tup_lst)\n",
    "    cent = np.array(points.centroid)\n",
    "    cent_df = pd.DataFrame([cent]) #this is a rough centroid estimate\n",
    "    fig = gmaps.Map()\n",
    "    can_layer = gmaps.symbol_layer(\n",
    "    df_cans_map, fill_color=\"green\", stroke_color=\"green\", scale=2)\n",
    "    cent_layer = gmaps.symbol_layer(\n",
    "    cent_df, fill_color=\"red\", stroke_color=\"red\", scale=2)\n",
    "    fig.add_layer(can_layer)\n",
    "    fig.add_layer(cent_layer)\n",
    "    fig\n",
    "    return cent,fig\n",
    "\n",
    "def get_attr(obj):\n",
    "    out = [a for a in dir(obj) if not a.startswith('__') and not callable(getattr(obj,a))]\n",
    "    return out\n",
    "\n",
    "def most_common(lt):\n",
    "    data = Counter(lt)\n",
    "    return data.most_common(1)[0][0]\n",
    "\n",
    "def get_common_country(lt):\n",
    "    country_lst = []\n",
    "    country_els = []\n",
    "    for it in lt:\n",
    "        try:\n",
    "            country = pycountry.countries.lookup(it.lower())\n",
    "            country_lst.append(country.alpha_3)\n",
    "            country_els.append(it)\n",
    "        except:\n",
    "            country = None \n",
    "    if country_lst == []:\n",
    "        return 'none',[],[]\n",
    "    return most_common(country_lst),country_lst, country_els\n",
    "\n",
    "def get_common_state(lt):\n",
    "    state_lst = []\n",
    "    state_els = []\n",
    "    for it in lt:\n",
    "        sts = [a.lower() for a in state_add_dict.keys()]\n",
    "        abbs = [a.lower() for a in state_add_dict.values()]\n",
    "        if it in sts:\n",
    "            state_lst.append(it)\n",
    "            state_els.append(it)\n",
    "        elif it in abbs:\n",
    "            state_lst.append(state_dict[it])\n",
    "            state_els.append(it)\n",
    "    if state_lst == []:\n",
    "        return 'none',[],[]\n",
    "    else:\n",
    "        return most_common(state_lst), state_lst, state_els\n",
    "\n",
    "def get_possible_locale(lt,cn,st,cn_lst,st_lst):\n",
    "    locale_list = []\n",
    "    a = [b for b in lt if b not in cn_lst and b not in st_lst]\n",
    "    for b in a:\n",
    "        locales = get_possible_locations(b)\n",
    "        locales = [c for c in locales if cn in b and st in b]\n",
    "        locale_list.append(locales)\n",
    "    return locale_list\n",
    "\n",
    "# Need to unify this!\n",
    "def lookup_state_abbrev(cn):\n",
    "    try:\n",
    "        out = state_add_dict[cn]\n",
    "    except:\n",
    "        out = 'no state'\n",
    "    return out\n",
    "\n",
    "state_dict = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "}\n",
    "state_add_dict = {v: k for k, v in state_dict.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_extractions = defaultdict(list)\n",
    "num_test_cands = F_test.shape[0]\n",
    "test_cand_preds = (gen_model.marginals(L_test)>0.5)*2-1\n",
    "for ind in range(num_test_cands):\n",
    "    cand = F_test.get_candidate(session,ind)\n",
    "    parent = cand.get_parent()\n",
    "    doc_name = parent.document.name\n",
    "    # Initializing key if it doesn't exist\n",
    "    doc_extractions[doc_name]\n",
    "    loc = cand.location.get_span().lower()\n",
    "    if test_cand_preds[ind] == 1:\n",
    "        doc_extractions[doc_name].append(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_extractions = defaultdict(set)\n",
    "for doc_name, extract_list in doc_extractions.items():\n",
    "    out_extractions[doc_name] = list(set(extract_list))\n",
    "df_out = pd.DataFrame()\n",
    "df_out = df_labeled[df_labeled['file name'].isin(list(out_extractions.keys()))]\n",
    "df_out['extracted_location'] = df_out.apply(lambda row: out_extractions[row['file name']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv('../output/location_extractions.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'austin', u'trishia austin', u'austin']\n",
      "Checking Locale 0 of 2\n",
      "> <ipython-input-170-2d31084fc3c4>(31)<module>()\n",
      "-> if lookup_country_name(probable_country).lower() in spl:\n",
      "(Pdb) spl\n",
      "['tricia cove', 'hutto', 'tx', 'usa']\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(48)<module>()\n",
      "-> count = count+1\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(26)<module>()\n",
      "-> while not_exact and count<len(locales):\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(27)<module>()\n",
      "-> print('Checking Locale %d of %d' %(count,len(locales)))\n",
      "(Pdb) n\n",
      "Checking Locale 1 of 2\n",
      "> <ipython-input-170-2d31084fc3c4>(28)<module>()\n",
      "-> c = locales[count]\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(29)<module>()\n",
      "-> spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(29)<module>()\n",
      "-> spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
      "(Pdb) spl\n",
      "['tricia cove', 'hutto', 'tx', 'usa']\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(29)<module>()\n",
      "-> spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
      "(Pdb) spl\n",
      "['tricia cove', 'hutto', 'tx', 'usa']\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(29)<module>()\n",
      "-> spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(29)<module>()\n",
      "-> spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(30)<module>()\n",
      "-> import pdb; pdb.set_trace()\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(31)<module>()\n",
      "-> if lookup_country_name(probable_country).lower() in spl:\n",
      "(Pdb) spl\n",
      "['tricia lane', 'hutto', 'tx', 'usa']\n",
      "(Pdb) count\n",
      "1\n",
      "(Pdb) spl\n",
      "['tricia lane', 'hutto', 'tx', 'usa']\n",
      "(Pdb) c\n",
      "Checking Locale 0 of 5\n",
      "> <ipython-input-170-2d31084fc3c4>(30)<module>()\n",
      "-> import pdb; pdb.set_trace()\n",
      "(Pdb) spl\n",
      "['austin', 'tx', 'usa']\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(31)<module>()\n",
      "-> if lookup_country_name(probable_country).lower() in spl:\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(48)<module>()\n",
      "-> count = count+1\n",
      "(Pdb) lookup_country_name(probable_country).lower() in spl\n",
      "False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-2d31084fc3c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-170-2d31084fc3c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/pdb.pyc\u001b[0m in \u001b[0;36muser_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_mainpyfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbp_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbp_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/pdb.pyc\u001b[0m in \u001b[0;36minteraction\u001b[0;34m(self, frame, traceback)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_stack_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/cmd.pyc\u001b[0m in \u001b[0;36mcmdloop\u001b[0;34m(self, intro)\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_rawinput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EOF'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out_locales = defaultdict(list)\n",
    "for doc_name, extract_list in doc_extractions.items():\n",
    "    \n",
    "    # Getting country names\n",
    "    probable_country,country_list, country_els = get_common_country(extract_list)\n",
    "    if lookup_country_alpha3(probable_country) == 'USA' and len(extract_list) >1:\n",
    "        # Getting state names\n",
    "        probable_state,state_list,state_els = get_common_state(extract_list)\n",
    "    else:\n",
    "        probable_state,state_list,state_els = 'none',[],[]\n",
    "    \n",
    "    locale_list = []\n",
    "    a = [b for b in extract_list if b not in country_els and b not in state_els] #need lookup here\n",
    "    print(a)\n",
    "    if a == []:\n",
    "        if probable_state != 'none' and probable_country != 'none' and a == []:\n",
    "            locale_list = ['none,none,'+state_add_dict[probable_state]+','+probable_country]\n",
    "    else:\n",
    "        most_common_locale = most_common(a)\n",
    "        aset = list(set(a))\n",
    "        for b in aset:\n",
    "                locale_tmp = []\n",
    "                qo,locales = get_possible_locations(b)\n",
    "                not_exact = 1\n",
    "                count = 0\n",
    "                while not_exact and count<len(locales):\n",
    "                    print('Checking Locale %d of %d' %(count,len(locales)))\n",
    "                    c = locales[count]\n",
    "                    spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
    "                    import pdb; pdb.set_trace()\n",
    "                    if lookup_country_name(probable_country).lower() in spl:\n",
    "                        if lookup_state_abbrev(probable_state).lower() in spl: \n",
    "                            if spl[0].lower() == most_common_locale.lower() and len(spl) == 3:\n",
    "                                locale_list = ['none']+spl\n",
    "                                locale_list = [','.join(locale_list)]\n",
    "                                not_exact = 0\n",
    "                                print('Exact City Found')\n",
    "                            elif spl[0].lower() == most_common_locale.lower() and len(spl) == 4:\n",
    "                                locale_list = [','.join(spl)]\n",
    "                                not_exact = 0\n",
    "                                print('Exact Location Found')\n",
    "                            else:             \n",
    "                                locale_list.append(','.join(spl))  \n",
    "                                count = count+1\n",
    "                        else:\n",
    "                            count = count+1         \n",
    "                    else:\n",
    "                        count = count+1\n",
    "        \n",
    "    import pdb; pdb.set_trace()\n",
    "    #reformatting for labeling comparison\n",
    "    locale_list_out = []\n",
    "    for c in locale_list:\n",
    "        b = c.split(',')\n",
    "        print(b)\n",
    "        b[-1] = str(lookup_country_alpha3(b[-1]).lower())\n",
    "        b[-2] = state_dict[b[-2].upper()].lower()\n",
    "        locale_list_out.append(','.join(b)) \n",
    "    out_locales[doc_name] = locale_list_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ii in out_locales.keys():\n",
    "    if out_locales[ii] == []:\n",
    "        out_locales[ii] = ['none','none','none','none']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
