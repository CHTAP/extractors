{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Parsing Files, Adding Candidates and Labels to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Loading config\n",
    "with open(\"run_config.json\") as fl:\n",
    "    cfg = json.load(fl)\n",
    "cfg_params = cfg['parameters']\n",
    "\n",
    "# Setting snorkel path and output root\n",
    "import os\n",
    "from os.path import join\n",
    "output_root = join(cfg_params['output_path'],cfg_params['experiment_name'])\n",
    "os.environ['FONDUERDBNAME'] = cfg['postgres_db_name']\n",
    "os.environ['SNORKELDB'] = join(cfg['postgres_location'],os.environ['FONDUERDBNAME'])\n",
    "\n",
    "# For loading input files\n",
    "import pandas as pd\n",
    "\n",
    "# For running Snorkel\n",
    "from snorkel.contrib.fonduer import SnorkelSession\n",
    "from snorkel.contrib.fonduer.models import candidate_subclass\n",
    "from snorkel.contrib.fonduer import HTMLPreprocessor, OmniParser\n",
    "from utils import HTMLListPreprocessor\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "snorkeldb = create_engine(os.environ['SNORKELDB'], isolation_level=\"AUTOCOMMIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled data from tsv\n",
    "pth_labeled = join(cfg['data_path'],'labels_and_splits')\n",
    "fl_labeled = cfg['labeled_data_file']\n",
    "df_labeled = pd.read_csv(join(pth_labeled,fl_labeled),sep='\\t')\n",
    "path_list_labeled = [_+'.html' for _ in df_labeled['file name'].tolist()]\n",
    "\n",
    "#Load unlabeled data from tsv\n",
    "fl_unlabeled = cfg['unlabeled_data_file']\n",
    "df_unlabeled = pd.read_csv(join(pth_labeled,fl_unlabeled),sep='\\t')\n",
    "path_list_unlabeled = [_+'.html' for _ in df_unlabeled['file name'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning, empty document 018563ac-eb50-4d26-8507-31e9cf836999 passed to CoreNLP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 596 ms, sys: 144 ms, total: 740 ms\n",
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "# Start snorkel session and creating location subclass\n",
    "session = SnorkelSession()\n",
    "Location_Extraction = candidate_subclass('location_extraction',\\\n",
    "                          [\"location\"])\n",
    "\n",
    "# Parsing documents \n",
    "max_docs = cfg['max_docs']\n",
    "data_loc = join(cfg['data_path'],'raw_data')\n",
    "path_list = path_list_labeled[:max_docs]+path_list_unlabeled[:max_docs]\n",
    "doc_preprocessor = HTMLListPreprocessor(data_loc,\\\n",
    "                                file_list=path_list)\n",
    "corpus_parser = OmniParser(structural=True, lingual=True, visual=False)\n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=cfg['parallel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Documents:', 8L)\n",
      "('Phrases:', 2388L)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.models import Document, Phrase\n",
    "\n",
    "# Checking database contents\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Phrases:\", session.query(Phrase).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing into Test/Train, Extracting Features, Throttling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 4\n",
      "dev: 2\n",
      "test: 2\n",
      "[u'001a5f8b-82c5-4428-b539-0c8a0f2f87c4',\n",
      " u'005dd27d-91c5-4569-b285-489391dcff4f',\n",
      " u'0069a7dd-9a03-4240-9073-77744c10b467',\n",
      " u'0034ff21-5d7a-4edf-9150-e22c5188dde1']\n"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "data = [(doc.name+'.html', doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if doc_name in path_list_unlabeled:\n",
    "        train_docs.add(doc)\n",
    "    else:\n",
    "        if len(dev_docs)<=len(test_docs):\n",
    "            dev_docs.add(doc)\n",
    "        else:\n",
    "            test_docs.add(doc)\n",
    "\n",
    "print \"train:\",len(train_docs)\n",
    "print \"dev:\" ,len(dev_docs)\n",
    "print \"test:\",len(test_docs)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint([x.name for x in train_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.matchers import *\n",
    "location_matcher = LocationMatcher(longest_match_only=True) \n",
    "\n",
    "from snorkel.contrib.fonduer.fonduer.candidates import OmniNgrams\n",
    "location_ngrams = OmniNgrams(n_max=6, split_tokens=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.lf_helpers import *\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "def location_currencies_filter(location):\n",
    "    list_currencies = [ \"dollar\", \"dollars\", \"lira\",\"kwacha\",\"rials\",\"rial\",\"dong\",\"dongs\",\"fuerte\",\"euro\",\n",
    "                       \"euros\",\"vatu\",\"som\",\"peso\",\"sterling\",\"sterlings\",\"soms\",\"pestos\",\n",
    "                       \"pounds\", \n",
    "                  \"pound\",\"dirham\",\"dirhams\",\"hryvnia\",\"manat\",\"manats\",\"liras\",\"lira\",\n",
    "                       \"dinar\",\"dinars\",\"pa'anga\",\"franc\",\"baht\",\"schilling\",\n",
    "                  \"somoni\",\"krona\",\"lilangeni\",\"rupee\",\"rand\",\"shilling\",\"leone\",\"riyal\",\"dobra\",\n",
    "                  \"tala\",\"ruble\",\"zloty\",\"peso\",\"sol\",\"quarani\",\"kina\",\"guinean\",\"balboa\",\"krone\",\"naira\",\n",
    "                  \"cordoba\",\"kyat\",\"metical\",\"togrog\",\"leu\",\"ouguiya\",\"rufiyaa\",\"ringgit\",\"kwacha\",\n",
    "                  \"ariary\",\"denar\",\"litas\",\"loti\",\"lats\",\"kip\",\"som\",\"won\",\"tenge\",\"yen\",\"shekel\",\"rupiah\",\n",
    "                  \"forint\",\"lempira\",\"gourde\",\"quetzal\",\"cedi\",\"lari\",\"dalasi\",\"cfp\",\"birr\",\"kroon\",\"nakfa\",\n",
    "                  \"cfa\",\"Peso\",\"koruna\",\"croatian\",\"colon\",\"yuan\",\"escudo\",\"cape\",\"riel\",\"lev\",\"real\"\n",
    "                  ,\"real\",\"mark\",\"boliviano\",\"ngultrum\",\"taka\",\"manat\",\"dram\",\"kwanza\",\"lek\",\"afghani\",\"renminbi\"]\n",
    "\n",
    "    \n",
    "    cand_right_tokens = list(get_right_ngrams(location,window=2))\n",
    "    for cand in cand_right_tokens:\n",
    "        if cand not in list_currencies:\n",
    "            return location\n",
    "    \n",
    "candidate_filter = location_currencies_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 44 ms, sys: 264 ms, total: 308 ms\n",
      "Wall time: 4.02 s\n",
      "('Number of candidates:', 12L)\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.06 Âµs\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "('Number of candidates:', 26L)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "('Number of candidates:', 12L)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.candidates import CandidateExtractor\n",
    "\n",
    "candidate_extractor = CandidateExtractor(Location_Extraction,\n",
    "                                         [location_ngrams], [location_matcher],\n",
    "                                         candidate_filter=candidate_filter)\n",
    "\n",
    "%time candidate_extractor.apply(train_docs, split=0, parallelism=cfg['parallel'])\n",
    "print(\"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == i+1).count())\n",
    "%time\n",
    "for i, docs in enumerate([dev_docs, test_docs]):\n",
    "    candidate_extractor.apply(docs, split=i+1, parallelism=cfg['parallel'])\n",
    "    print(\"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == i+1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True (Span(\"Dallas\", sentence=36703, chars=[0,5], words=[0,0]),) Span(\"Dallas\", sentence=36703, chars=[0,5], words=[0,0])\n",
      "Phrase (Doc: 0397de89-5130-4f56-8a46-3e533d393d8d, Index: 126, Text: Dallas Escorts)\n",
      "0397de89-5130-4f56-8a46-3e533d393d8d\n",
      "Dallas\n"
     ]
    }
   ],
   "source": [
    "cands = session.query(Location_Extraction).filter(Location_Extraction.split == 1).order_by(Location_Extraction.id).all()\n",
    "cand = cands[0]\n",
    "args = cand.get_contexts()\n",
    "span = args[0]\n",
    "c = span.sentence.is_lingual()\n",
    "a = span.get_parent()\n",
    "b = cand[0]\n",
    "print c, args, span\n",
    "print a\n",
    "print cand[0].sentence.document.name\n",
    "print cand[0].get_span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "Copying location_extraction_feature_updates to postgres\n",
      "COPY 9\n",
      "\n",
      "CPU times: user 44 ms, sys: 216 ms, total: 260 ms\n",
      "Wall time: 3.78 s\n",
      "(9, 463)\n",
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process BatchAnnotatorUDF-129:\n",
      "Traceback (most recent call last):\n",
      "  File \"/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/lfs/raiders2/0/jdunnmon/chtap/snorkel/snorkel/udf.py\", line 161, in run\n",
      "    for y in self.apply(x, **self.apply_kwargs):\n",
      "  File \"/lfs/raiders2/0/jdunnmon/chtap/snorkel/snorkel/contrib/fonduer/fonduer/async_annotations.py\", line 215, in apply\n",
      "    for id, k, v in self.anno_generator(list(candidates)):\n",
      "  File \"/lfs/raiders2/0/jdunnmon/chtap/snorkel/snorkel/contrib/fonduer/fonduer/features/features.py\", line 11, in get_all_feats\n",
      "    for id, f, v in get_content_feats(candidates):\n",
      "  File \"/lfs/raiders2/0/jdunnmon/chtap/snorkel/snorkel/contrib/fonduer/fonduer/features/content_features.py\", line 37, in get_content_feats\n",
      "    xmltree = corenlp_to_xmltree(sent)\n",
      "  File \"/lfs/raiders2/0/jdunnmon/chtap/snorkel/treedlib/treedlib/structs.py\", line 88, in corenlp_to_xmltree\n",
      "    root = corenlp_to_xmltree_sub(s, dep_parents, 0)\n",
      "  File \"/lfs/raiders2/0/jdunnmon/chtap/snorkel/treedlib/treedlib/structs.py\", line 120, in corenlp_to_xmltree_sub\n",
      "    root.append(corenlp_to_xmltree_sub(s, dep_parents, i+1))\n",
      "  File \"/lfs/raiders2/0/jdunnmon/chtap/snorkel/treedlib/treedlib/structs.py\", line 120, in corenlp_to_xmltree_sub\n",
      "    root.append(corenlp_to_xmltree_sub(s, dep_parents, i+1))\n",
      "  File \"/lfs/raiders2/0/jdunnmon/chtap/snorkel/treedlib/treedlib/structs.py\", line 120, in corenlp_to_xmltree_sub\n",
      "    root.append(corenlp_to_xmltree_sub(s, dep_parents, i+1))\n",
      "  File \"/lfs/raiders2/0/jdunnmon/chtap/snorkel/treedlib/treedlib/structs.py\", line 120, in corenlp_to_xmltree_sub\n",
      "    root.append(corenlp_to_xmltree_sub(s, dep_parents, i+1))\n",
      "  File \"/lfs/raiders2/0/jdunnmon/chtap/snorkel/treedlib/treedlib/structs.py\", line 110, in corenlp_to_xmltree_sub\n",
      "    attrib[singular(k)] = ''.join(c for c in str(v[i]) if ord(c) < 128)\n",
      "UnicodeEncodeError: 'ascii' codec can't encode character u'\\u2022' in position 0: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying location_extraction_feature_updates to postgres\n",
      "COPY 0\n",
      "\n",
      "CPU times: user 76 ms, sys: 124 ms, total: 200 ms\n",
      "Wall time: 3.33 s\n",
      "(26, 463)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "Copying location_extraction_feature_updates to postgres\n",
      "COPY 12\n",
      "\n",
      "CPU times: user 52 ms, sys: 148 ms, total: 200 ms\n",
      "Wall time: 3.69 s\n",
      "(12, 463)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer import BatchFeatureAnnotator\n",
    "featurizer = BatchFeatureAnnotator(Location_Extraction)\n",
    "%time F_train = featurizer.apply(split=0, replace_key_set=True, parallelism=cfg['parallel'])\n",
    "print(F_train.shape)\n",
    "%time F_dev = featurizer.apply(split=1, replace_key_set=False, parallelism=cfg['parallel'])\n",
    "print(F_dev.shape)\n",
    "%time F_test = featurizer.apply(split=2, replace_key_set=False, parallelism=cfg['parallel'])\n",
    "print(F_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Gold Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from builtins import range\n",
    "import csv\n",
    "import codecs\n",
    "\n",
    "from snorkel.utils import ProgressBar\n",
    "\n",
    "from snorkel.models import GoldLabel, GoldLabelKey\n",
    "\n",
    "def load_chtap_labels(session, candidate_class, df, target, annotator_name='gold'):\n",
    "    ak = session.query(GoldLabelKey).filter(GoldLabelKey.name == annotator_name).first()\n",
    "    if ak is None:\n",
    "        ak = GoldLabelKey(name=annotator_name)\n",
    "        session.add(ak)\n",
    "        session.commit()   \n",
    "        \n",
    "    candidates = session.query(candidate_class).all()\n",
    "    cand_total = len(candidates)\n",
    "    print('Loading', cand_total, 'candidate labels')\n",
    "    pb = ProgressBar(cand_total)\n",
    "    labels=[]\n",
    "    for i, c in enumerate(candidates):\n",
    "        pb.bar(i)\n",
    "        doc = c[0].sentence.document.name\n",
    "        val = c[0].get_span().lower()\n",
    "        target_strings = df[df['file name']==doc][target].tolist()\n",
    "        if target == 'location':\n",
    "                if target_strings == []:\n",
    "                    targets = ''\n",
    "                else:\n",
    "                    targets = target_strings[0].lower().split(',')\n",
    "        #context_stable_ids = '~~'.join([i.stable_id for i in c.get_contexts()])\n",
    "        label = session.query(GoldLabel).filter(GoldLabel.key == ak).filter(GoldLabel.candidate == c).first()\n",
    "        if label is None:\n",
    "            #this conditional could be improved (use regex, etc.)\n",
    "            if val in targets or any([a in val for a in targets]):\n",
    "                label = GoldLabel(candidate=c, key=ak, value=1)\n",
    "            else:\n",
    "                label = GoldLabel(candidate=c, key=ak, value=-1)\n",
    "            session.add(label)\n",
    "            labels.append(label)\n",
    "    session.commit()\n",
    "    pb.close()\n",
    "\n",
    "    session.commit()\n",
    "    print(\"AnnotatorLabels created: %s\" % (len(labels),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 47 candidate labels\n",
      "[========================================] 100%\n",
      "AnnotatorLabels created: 0\n"
     ]
    }
   ],
   "source": [
    "target = 'location'\n",
    "load_chtap_labels(session, Location_Extraction, df_labeled, target ,annotator_name='gold')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
