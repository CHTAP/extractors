{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Parsing Files, Adding Candidates and Labels to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Loading config\n",
    "with open(\"run_config.json\") as fl:\n",
    "    cfg = json.load(fl)\n",
    "cfg_params = cfg['parameters']\n",
    "\n",
    "# Setting snorkel path and output root\n",
    "import os\n",
    "from os.path import join\n",
    "output_root = join(cfg_params['output_path'],cfg_params['experiment_name'])\n",
    "os.environ['FONDUERDBNAME'] = cfg['postgres_db_name']\n",
    "os.environ['SNORKELDB'] = join(cfg['postgres_location'],os.environ['FONDUERDBNAME'])\n",
    "\n",
    "# For loading input files\n",
    "import pandas as pd\n",
    "\n",
    "# For running Snorkel\n",
    "from snorkel.contrib.fonduer import SnorkelSession\n",
    "from snorkel.contrib.fonduer.models import candidate_subclass\n",
    "from snorkel.contrib.fonduer import HTMLPreprocessor, OmniParser\n",
    "from utils import HTMLListPreprocessor\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "snorkeldb = create_engine(os.environ['SNORKELDB'], isolation_level=\"AUTOCOMMIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled data from tsv\n",
    "pth_labeled = join(cfg['data_path'],'labels_and_splits')\n",
    "fl_labeled = cfg['labeled_data_file']\n",
    "df_labeled = pd.read_csv(join(pth_labeled,fl_labeled),sep='\\t')\n",
    "path_list_labeled = [_+'.html' for _ in df_labeled['file name'].tolist()]\n",
    "\n",
    "#Load unlabeled data from tsv\n",
    "fl_unlabeled = cfg['unlabeled_data_file']\n",
    "df_unlabeled = pd.read_csv(join(pth_labeled,fl_unlabeled),sep='\\t')\n",
    "path_list_unlabeled = [_+'.html' for _ in df_unlabeled['file name'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning, empty document 018563ac-eb50-4d26-8507-31e9cf836999 passed to CoreNLP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 532 ms, sys: 164 ms, total: 696 ms\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "# Start snorkel session and creating location subclass\n",
    "session = SnorkelSession()\n",
    "Location_Extraction = candidate_subclass('location_extraction',\\\n",
    "                          [\"location\"])\n",
    "\n",
    "# Parsing documents \n",
    "max_docs = cfg['max_docs']\n",
    "data_loc = join(cfg['data_path'],'raw_data')\n",
    "path_list = path_list_labeled[:max_docs]+path_list_unlabeled[:max_docs]\n",
    "doc_preprocessor = HTMLListPreprocessor(data_loc,\\\n",
    "                                file_list=path_list)\n",
    "corpus_parser = OmniParser(structural=True, lingual=True, visual=False)\n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=cfg['parallel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Documents:', 8L)\n",
      "('Phrases:', 2388L)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.models import Document, Phrase\n",
    "\n",
    "# Checking database contents\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Phrases:\", session.query(Phrase).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing into Test/Train, Extracting Features, Throttling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 4\n",
      "dev: 2\n",
      "test: 2\n",
      "[u'005dd27d-91c5-4569-b285-489391dcff4f',\n",
      " u'0069a7dd-9a03-4240-9073-77744c10b467',\n",
      " u'001a5f8b-82c5-4428-b539-0c8a0f2f87c4',\n",
      " u'0034ff21-5d7a-4edf-9150-e22c5188dde1']\n"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "data = [(doc.name+'.html', doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if doc_name in path_list_unlabeled:\n",
    "        train_docs.add(doc)\n",
    "    else:\n",
    "        if len(dev_docs)<=len(test_docs):\n",
    "            dev_docs.add(doc)\n",
    "        else:\n",
    "            test_docs.add(doc)\n",
    "\n",
    "print \"train:\",len(train_docs)\n",
    "print \"dev:\" ,len(dev_docs)\n",
    "print \"test:\",len(test_docs)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint([x.name for x in train_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.matchers import *\n",
    "location_matcher = LocationMatcher(longest_match_only=True) \n",
    "\n",
    "from snorkel.contrib.fonduer.fonduer.candidates import OmniNgrams\n",
    "location_ngrams = OmniNgrams(n_max=6, split_tokens=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.lf_helpers import *\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "def location_currencies_filter(location):\n",
    "    list_currencies = [ \"dollar\", \"dollars\", \"lira\",\"kwacha\",\"rials\",\"rial\",\"dong\",\"dongs\",\"fuerte\",\"euro\",\n",
    "                       \"euros\",\"vatu\",\"som\",\"peso\",\"sterling\",\"sterlings\",\"soms\",\"pestos\",\n",
    "                       \"pounds\", \n",
    "                  \"pound\",\"dirham\",\"dirhams\",\"hryvnia\",\"manat\",\"manats\",\"liras\",\"lira\",\n",
    "                       \"dinar\",\"dinars\",\"pa'anga\",\"franc\",\"baht\",\"schilling\",\n",
    "                  \"somoni\",\"krona\",\"lilangeni\",\"rupee\",\"rand\",\"shilling\",\"leone\",\"riyal\",\"dobra\",\n",
    "                  \"tala\",\"ruble\",\"zloty\",\"peso\",\"sol\",\"quarani\",\"kina\",\"guinean\",\"balboa\",\"krone\",\"naira\",\n",
    "                  \"cordoba\",\"kyat\",\"metical\",\"togrog\",\"leu\",\"ouguiya\",\"rufiyaa\",\"ringgit\",\"kwacha\",\n",
    "                  \"ariary\",\"denar\",\"litas\",\"loti\",\"lats\",\"kip\",\"som\",\"won\",\"tenge\",\"yen\",\"shekel\",\"rupiah\",\n",
    "                  \"forint\",\"lempira\",\"gourde\",\"quetzal\",\"cedi\",\"lari\",\"dalasi\",\"cfp\",\"birr\",\"kroon\",\"nakfa\",\n",
    "                  \"cfa\",\"Peso\",\"koruna\",\"croatian\",\"colon\",\"yuan\",\"escudo\",\"cape\",\"riel\",\"lev\",\"real\"\n",
    "                  ,\"real\",\"mark\",\"boliviano\",\"ngultrum\",\"taka\",\"manat\",\"dram\",\"kwanza\",\"lek\",\"afghani\",\"renminbi\"]\n",
    "\n",
    "    \n",
    "    cand_right_tokens = list(get_right_ngrams(location,window=2))\n",
    "    #print len(cand_right_tokens)\n",
    "    #print cand_right_tokens#(get_right_ngrams(location,window=4))\n",
    "    for cand in cand_right_tokens:\n",
    "        #print \"[\"+cand+\"]\"\n",
    "        if cand not in list_currencies:\n",
    "            #print \"[\"+cand+\"]\"\n",
    "            #print location\n",
    "            return location\n",
    "    \n",
    "candidate_filter = location_currencies_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 24 ms, sys: 300 ms, total: 324 ms\n",
      "Wall time: 3.7 s\n",
      "('Number of candidates:', 12L)\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10 Âµs\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "('Number of candidates:', 24L)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "('Number of candidates:', 12L)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.candidates import CandidateExtractor\n",
    "\n",
    "candidate_extractor = CandidateExtractor(Location_Extraction,\n",
    "                                         [location_ngrams], [location_matcher],\n",
    "                                         candidate_filter=candidate_filter)\n",
    "\n",
    "%time candidate_extractor.apply(train_docs, split=0, parallelism=cfg['parallel'])\n",
    "print(\"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == i+1).count())\n",
    "%time\n",
    "for i, docs in enumerate([dev_docs, test_docs]):\n",
    "    candidate_extractor.apply(docs, split=i+1, parallelism=cfg['parallel'])\n",
    "    print(\"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == i+1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process BatchAnnotatorUDF-113:\n",
      "Traceback (most recent call last):\n",
      "  File \"/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    self.run()\n",
      "    for y in self.apply(x, **self.apply_kwargs):\n",
      "  File \"/lfs/local/0/jdunnmon/chtap/backup/snorkel/snorkel/udf.py\", line 156, in run\n",
      "  File \"/lfs/local/0/jdunnmon/chtap/backup/snorkel/snorkel/contrib/fonduer/fonduer/async_annotations.py\", line 210, in apply\n",
      "    for id, k, v in self.anno_generator(list(candidates)):\n",
      "  File \"/lfs/local/0/jdunnmon/chtap/backup/snorkel/snorkel/contrib/fonduer/fonduer/features/features.py\", line 11, in get_all_feats\n",
      "    for id, f, v in get_content_feats(candidates):\n",
      "  File \"/lfs/local/0/jdunnmon/chtap/backup/snorkel/snorkel/contrib/fonduer/fonduer/features/content_features.py\", line 32, in get_content_feats\n",
      "    get_tdl_feats = compile_entity_feature_generator()\n",
      "  File \"/lfs/local/0/jdunnmon/chtap/backup/snorkel/snorkel/contrib/fonduer/fonduer/features/content_features.py\", line 96, in compile_entity_feature_generator\n",
      "    m = Mention(0)\n",
      "NameError: global name 'Mention' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying location_extraction_feature to postgres\n",
      "COPY 0\n",
      "\n",
      "CPU times: user 52 ms, sys: 280 ms, total: 332 ms\n",
      "Wall time: 3.5 s\n",
      "(9, 0)\n",
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process BatchAnnotatorUDF-129:\n",
      "Traceback (most recent call last):\n",
      "  File \"/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/lfs/local/0/jdunnmon/chtap/backup/snorkel/snorkel/udf.py\", line 156, in run\n",
      "    for y in self.apply(x, **self.apply_kwargs):\n",
      "  File \"/lfs/local/0/jdunnmon/chtap/backup/snorkel/snorkel/contrib/fonduer/fonduer/async_annotations.py\", line 210, in apply\n",
      "    for id, k, v in self.anno_generator(list(candidates)):\n",
      "  File \"/lfs/local/0/jdunnmon/chtap/backup/snorkel/snorkel/contrib/fonduer/fonduer/features/features.py\", line 11, in get_all_feats\n",
      "    for id, f, v in get_content_feats(candidates):\n",
      "  File \"/lfs/local/0/jdunnmon/chtap/backup/snorkel/snorkel/contrib/fonduer/fonduer/features/content_features.py\", line 32, in get_content_feats\n",
      "    get_tdl_feats = compile_entity_feature_generator()\n",
      "  File \"/lfs/local/0/jdunnmon/chtap/backup/snorkel/snorkel/contrib/fonduer/fonduer/features/content_features.py\", line 96, in compile_entity_feature_generator\n",
      "    m = Mention(0)\n",
      "NameError: global name 'Mention' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying location_extraction_feature_updates to postgres\n",
      "COPY 0\n",
      "\n",
      "CPU times: user 52 ms, sys: 204 ms, total: 256 ms\n",
      "Wall time: 3.38 s\n",
      "(24, 0)\n",
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process BatchAnnotatorUDF-145:\n",
      "Traceback (most recent call last):\n",
      "  File \"/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    self.run()\n",
      "    for y in self.apply(x, **self.apply_kwargs):\n",
      "  File \"/lfs/local/0/jdunnmon/chtap/backup/snorkel/snorkel/udf.py\", line 156, in run\n",
      "  File \"/lfs/local/0/jdunnmon/chtap/backup/snorkel/snorkel/contrib/fonduer/fonduer/async_annotations.py\", line 210, in apply\n",
      "    for id, k, v in self.anno_generator(list(candidates)):\n",
      "  File \"/lfs/local/0/jdunnmon/chtap/backup/snorkel/snorkel/contrib/fonduer/fonduer/features/features.py\", line 11, in get_all_feats\n",
      "    for id, f, v in get_content_feats(candidates):\n",
      "  File \"/lfs/local/0/jdunnmon/chtap/backup/snorkel/snorkel/contrib/fonduer/fonduer/features/content_features.py\", line 32, in get_content_feats\n",
      "    get_tdl_feats = compile_entity_feature_generator()\n",
      "  File \"/lfs/local/0/jdunnmon/chtap/backup/snorkel/snorkel/contrib/fonduer/fonduer/features/content_features.py\", line 96, in compile_entity_feature_generator\n",
      "    m = Mention(0)\n",
      "NameError: global name 'Mention' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying location_extraction_feature_updates to postgres\n",
      "COPY 0\n",
      "\n",
      "CPU times: user 68 ms, sys: 216 ms, total: 284 ms\n",
      "Wall time: 3.41 s\n",
      "(12, 0)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer import BatchFeatureAnnotator\n",
    "\n",
    "featurizer = BatchFeatureAnnotator(Location_Extraction)\n",
    "%time F_train = featurizer.apply(split=0, replace_key_set=True, parallelism=cfg['parallel'])\n",
    "print(F_train.shape)\n",
    "%time F_dev = featurizer.apply(split=1, replace_key_set=False, parallelism=cfg['parallel'])\n",
    "print(F_dev.shape)\n",
    "%time F_test = featurizer.apply(split=2, replace_key_set=False, parallelism=cfg['parallel'])\n",
    "print(F_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add ground truth labels, add to db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
