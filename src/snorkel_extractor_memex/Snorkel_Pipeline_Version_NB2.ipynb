{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Write Labeling Functions and Train Generative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is ensure that modules are auto-reloaded at runtime to allow for development in other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set the Snorkel database location and start and connect to it.  By default, we use a PosgreSQL database backend, which can be created using `createdb DB_NAME` once psql is installed.  Note that Snorkel does *not* currently support parallel database processing with a SQLite backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Snorkel DB location\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#For network PostgreSQL\n",
    "#postgres_location = 'postgresql://jdunnmon:123@localhost:5432'\n",
    "#postgres_db_name = 'es_locs_small'\n",
    "#os.environ['SNORKELDB'] = os.path.join(postgres_location,postgres_db_name)\n",
    "\n",
    "#For local PostgreSQL\n",
    "os.environ['SNORKELDB'] = 'postgres:///es_locs_small'\n",
    "\n",
    "# Adding path above for utils\n",
    "sys.path.append('..')\n",
    "\n",
    "# For SQLite\n",
    "#db_location = '.'\n",
    "#db_name = \"es_locs_small.db\"\n",
    "#os.environ['SNORKELDB'] = '{0}:///{1}/{2}'.format(\"sqlite\", db_location, db_name)\n",
    "\n",
    "# Start Snorkel session\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Setting parallelism\n",
    "parallelism = 32\n",
    "\n",
    "# Setting random seed\n",
    "seed = 1701\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create candidate subclass and get dev set candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import create_candidate_class\n",
    "\n",
    "# Setting extraction type -- should be a subfield in your data source extractions field!\n",
    "extraction_type = 'location'\n",
    "\n",
    "# Creating candidate class\n",
    "candidate_class, candidate_class_name  = create_candidate_class(extraction_type)\n",
    "\n",
    "# Getting dev set and printing length\n",
    "cands_dev = session.query(candidate_class).filter(candidate_class.split == 1).order_by(candidate_class.id).all()\n",
    "print(f'Dev Candidates: {len(cands_dev)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Labeling Functions (LFs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from fonduer.lf_helpers import get_left_ngrams, get_right_ngrams, get_between_ngrams\n",
    "from snorkel.lf_helpers import get_tagged_text\n",
    "\n",
    "import geotext\n",
    "import geograpy\n",
    "from geograpy import extraction\n",
    "\n",
    "from gm_utils import *\n",
    "\n",
    "def lf_geograpy_entity_neg(c):\n",
    "    txt = c.location.get_span().lower()\n",
    "    sent = c.get_parent().text\n",
    "    e = extraction.Extractor(text=sent)\n",
    "    e.find_entities()\n",
    "    places = [p.lower() for p in e.places]\n",
    "    if txt not in places:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lf_geograpy_country(c):\n",
    "    txt = c.location.get_span().lower()\n",
    "    sent = c.get_parent().text\n",
    "    places = geograpy.get_place_context(text=sent)\n",
    "    if places.countries:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lf_geograpy_region(c):\n",
    "    txt = c.location.get_span().lower()\n",
    "    sent = c.get_parent().text\n",
    "    places = geograpy.get_place_context(text=sent)\n",
    "    if places.regions:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lf_geograpy_city(c):\n",
    "    txt = c.location.get_span().lower()\n",
    "    sent = c.get_parent().text\n",
    "    places = geograpy.get_place_context(text=sent)\n",
    "    if places.cities:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lf_geograpy_other(c):\n",
    "    txt = c.location.get_span().lower()\n",
    "    sent = c.get_parent().text\n",
    "    places = geograpy.get_place_context(text=sent)\n",
    "    if places.other:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def lf_following_words(c):\n",
    "    following_words = ['area', 'region', 'province', 'county', 'in']\n",
    "    return 1 if overlap(\n",
    "      following_words, \n",
    "      get_right_ngrams(c, window=3)) else 0\n",
    "\n",
    "def lf_preceding_words(c):\n",
    "    preceding_words = ['in', 'to', 'at', 'of', 'north', 'south', 'east', 'west', 'address', 'downtown', 'en']\n",
    "    return 1 if overlap(\n",
    "      preceding_words,\n",
    "      get_left_ngrams(c, window=4)) else 0\n",
    "\n",
    "def lf_preceding_words_enhanced(c):\n",
    "    preceding_words = ['based', 'back', 'come', 'new', 'located', 'location', 'you', 'your']\n",
    "    return 1 if overlap(\n",
    "      preceding_words, \n",
    "      get_left_ngrams(c, window=4)) else 0\n",
    "\n",
    "def lf_neg_preceding_words(c):\n",
    "    preceding_words = ['trained', 'show', 'from']\n",
    "    return -1 if overlap(\n",
    "      preceding_words,\n",
    "      get_left_ngrams(c, window=4)) else 0\n",
    "\n",
    "def lf_many_locations(c):\n",
    "    txt = c.location.get_span().lower()\n",
    "    sent = c.get_parent().text\n",
    "    e = extraction.Extractor(text=sent)\n",
    "    e.find_entities()\n",
    "    thresh = 3\n",
    "    return -1 if len(e.places)>thresh else 0\n",
    "\n",
    "def lf_nonletter(c):\n",
    "    txt = c.location.get_span().lower()\n",
    "    reg = re.compile(r'[^a-z ]')\n",
    "    if reg.search(txt):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf_loc(c):\n",
    "    return lf_geograpy_country(c) or lf_geograpy_region(c) or lf_geograpy_city(c)\n",
    "    \n",
    "def lf_nonloc(c):\n",
    "    return -1 if not lf_loc(c) else 0\n",
    "\n",
    "def lf_from(c):\n",
    "    return lf_preceding_words(c) and lf_loc(c)\n",
    "\n",
    "def lf_from_enhanced(c):\n",
    "    return lf_preceding_words(c) and lf_preceding_words_enhanced(c) and lf_loc(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating list of LFs to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFs = [\n",
    "    lf_geograpy_entity_neg,\n",
    "    lf_nonletter,\n",
    "    lf_nonloc,\n",
    "    lf_from,\n",
    "    lf_from_enhanced,\n",
    "    lf_neg_preceding_words,\n",
    "    lf_many_locations,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading gold dev set labels from database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating labeling functions on dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run once\n",
    "\"\"\"\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  snorkel.annotations import LabelAnnotator\n",
    "import numpy as np\n",
    "labeler = LabelAnnotator(lfs=LFs)\n",
    "\n",
    "%time L_dev = labeler.apply(split=1, parallelism=parallelism)\n",
    "L_dev.lf_stats(session, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating viewer to assist in LF development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# Can insert function here to select candidates based on arbitary criteria\n",
    "\n",
    "#Creating viewer for dev candidates\n",
    "sv = SentenceNgramViewer(cands_dev, session)\n",
    "sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once LFs are performing well, apply to entire database.  Applying to unlabeled data can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  snorkel.annotations import LabelAnnotator\n",
    "import numpy as np\n",
    "labeler = LabelAnnotator(lfs=LFs)\n",
    "\n",
    "%time L_train = labeler.apply(split=0, parallelism=parallelism)\n",
    "%time L_test = labeler.apply(split=2, parallelism=parallelism)\n",
    "\n",
    "# can also load with:\n",
    "# %time L_train = labeler.load_matrix(session, split=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "from snorkel.learning import RandomSearch\n",
    "\n",
    "# Setting parameter ranges for search\n",
    "param_ranges = {\n",
    "    'step_size' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "    'decay' : [1.0, 0.95, 0.9],\n",
    "    'epochs' : [20, 50, 100]\n",
    "}\n",
    "\n",
    "# Creating generative model\n",
    "gen_model = GenerativeModel()\n",
    "\n",
    "# Creating searcher over hyperparameters-- n is the number of models to train\n",
    "searcher = RandomSearch(GenerativeModel, param_ranges, L_train, n=5)\n",
    "\n",
    "# Searching model\n",
    "%time gen_model, run_stats = searcher.fit(L_dev, L_gold_dev, n_threads=parallelism)\n",
    "\n",
    "# Printing results of model search\n",
    "run_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing learned LF accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.weights.lf_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error analysis for generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = gen_model.error_analysis(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = SentenceNgramViewer(fp, session)\n",
    "sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting marginals, plotting training marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "L_eval = L_test\n",
    "eval_marginals = gen_model.marginals(L_eval)\n",
    "training_marginals = gen_model.marginals(L_train)\n",
    "\n",
    "# Plotting training marignals\n",
    "plt.hist(training_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dictionary of extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gm_utils import create_extractions_dict\n",
    "doc_extractions = create_extractions_dict(session, L_eval, eval_marginals, extractions=[extraction_type], dummy=True)\n",
    "\n",
    "# Uncomment to inspecting extractions dict to check format\n",
    "#doc_extractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving extractions to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Setting filename\n",
    "out_filename = \"loc_ext_test_generative.jsonl\"\n",
    "\n",
    "# Saving file to jsonl in extractions format\n",
    "with open(out_filename, 'w') as outfile:\n",
    "    for k,v in doc_extractions.items():\n",
    "        v['url'] = k\n",
    "        print(json.dumps(v), file=outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving training marginals for use with discriminative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import save_marginals\n",
    "%time save_marginals(session, L_train, training_marginals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
