{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Snorkel DB location\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#For PostgreSQL\n",
    "#postgres_location = 'postgresql://jdunnmon:123@localhost:5432'\n",
    "postgres_location = 'postgresql://saeideh:123@localhost:5432'\n",
    "#postgres_db_name = 'memex_db_snorkel_large'\n",
    "#postgres_db_name = 'memex_snorkel_db_extracted_text_10K'\n",
    "#postgres_db_name = 'memex_snorkel_db_extracted_text_150K'\n",
    "#postgres_db_name = 'memex_db_snorkel_tsv_1M'\n",
    "postgres_db_name = 'memex_db_saeideh_10k_test'\n",
    "os.environ['SNORKELDB'] = os.path.join(postgres_location,postgres_db_name)\n",
    "\n",
    "# Adding path above for utils\n",
    "sys.path.append('..')\n",
    "\n",
    "# For SQLite\n",
    "#db_location = '.'\n",
    "#db_name = \"snorkel_memex.db\"\n",
    "#os.environ['SNORKELDB'] = '{0}:///{1}/{2}'.format(\"sqlite\", db_location, db_name)\n",
    "\n",
    "# Start Snorkel session\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data source: options are content.tsv, memex_jsons\n",
    "data_source = 'content.tsv'\n",
    "\n",
    "# Setting max number of docs to ingest\n",
    "max_docs = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating a preprocessor based on the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel_utils import MemexTSVDocPreprocessor, MEMEXJsonLGZIPPreprocessor, ESTSVDocPreprocessor, retrieve_all_files\n",
    "\n",
    "if data_source == 'content.tsv':\n",
    "    data_loc = '/lfs/local/0/jdunnmon/data/memex-data/gold_labels/data_sample'\n",
    "    \n",
    "    # Setting path to MEMEX source data\n",
    "    file_path = '/lfs/local/0/jdunnmon/data/memex-data/gold_labels/content.tsv'\n",
    "\n",
    "    # Setting path to unique URL MEMEX source data\n",
    "    file_path_unique = '/dfs/scratch1/jdunnmon/data/memex-data/gold_labels/content_unique.tsv'\n",
    " \n",
    "\n",
    "    # Initializing document preprocessor\n",
    "    doc_preprocessor = MemexTSVDocPreprocessor(\n",
    "        path=file_path_unique,\n",
    "        max_docs=max_docs,\n",
    "        verbose=False,\n",
    "        clean_docs=True\n",
    "    )\n",
    "    \n",
    "elif data_source == 'es':\n",
    "    # Setting path to MEMEX source data\n",
    "    file_path_unique = '/dfs/scratch1/jdunnmon/data/memex-data/es/es_locations.tsv'\n",
    "    \n",
    "        # Initializing document preprocessor\n",
    "    doc_preprocessor = ESTSVDocPreprocessor(\n",
    "        path=file_path_unique,\n",
    "        max_docs=max_docs,\n",
    "        verbose=False,\n",
    "        clean_docs=True\n",
    "    )\n",
    "\n",
    "elif data_source == 'memex_jsons':\n",
    "    # Location on raiders\n",
    "    data_loc = '/lfs/local/0/jdunnmon/data/memex-data/gold_labels/data_sample'\n",
    "\n",
    "    # Getting all file paths\n",
    "    path_list = retrieve_all_files(data_loc)\n",
    "\n",
    "    # Applying arbitrary conditions to file path list\n",
    "    path_list = [a for a in path_list if a.endswith('gz')]\n",
    "\n",
    "    # Preprocessing documents from path_list\n",
    "    # Set \"content field\" to \"extracted_text\" to use extracted text as raw content\n",
    "    doc_preprocessor = MEMEXJsonLGZIPPreprocessor(data_loc,\\\n",
    "                                    file_list=path_list,encoding='utf-8', max_docs=max_docs, verbose=False, content_field='extracted_text')\n",
    "else:\n",
    "    raise ValueError('Invalid data source!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = doc_preprocessor._read_content_file(path_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b = a[a['content_type'] == 'text/html; charset=UTF-8']\n",
    "#s =a['extracted_text'][801].replace('\\n',' ').replace('\\t',' ')\n",
    "#\" \".join(s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 2.14 s, sys: 392 ms, total: 2.53 s\n",
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "\n",
    "# Applying corpus parser\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(list(doc_preprocessor), parallelism=8, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 10000\n",
      "Sentences: 45381\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "# Printing number of docs/sentences\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Importing gold label dict\n",
    "with open('/lfs/local/0/jdunnmon/data/memex-data/gold_labels/gold_loc.pickle', 'rb') as handle:\n",
    "    gold_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 9800 Docs, 44248 Sentences\n",
      "Dev: 100 Docs, 640 Sentences\n",
      "Test: 100 Docs, 493 Sentences\n",
      "CPU times: user 13.3 s, sys: 940 ms, total: 14.3 s\n",
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel_utils import create_test_train_splits\n",
    "\n",
    "# Getting all documents parsed by Snorkel\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "# Creating train, test, dev splits\n",
    "%time train_docs, dev_docs, test_docs, train_sents, dev_sents, test_sents = create_test_train_splits(docs, 'location', gold_dict=None, dev_frac=0.01, test_frac=0.01,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import Candidate, candidate_subclass\n",
    "\n",
    "# Designing candidate subclasses\n",
    "LocationExtraction = candidate_subclass('Location', ['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams\n",
    "from snorkel.candidates import CandidateExtractor\n",
    "#from snorkel.matchers import LocationMatcher\n",
    "from snorkel_utils import get_location_matcher, get_candidate_filter, CandidateExtractorFilter, LocationMatcher\n",
    "\n",
    "# Defining ngrams and matcher for candidate extractor\n",
    "location_ngrams   = Ngrams(n_max=3)\n",
    "#location_matcher  = get_location_matcher()\n",
    "location_matcher = LocationMatcher(longest_match_only=True)\n",
    "#candidate_filter =  get_candidate_filter()\n",
    "#cand_extractor = CandidateExtractorFilter(LocationExtraction,[location_ngrams],[location_matcher],candidate_filter=candidate_filter)\n",
    "cand_extractor    = CandidateExtractor(LocationExtraction, [location_ngrams], [location_matcher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 5.26 s, sys: 1.39 s, total: 6.66 s\n",
      "Wall time: 11.7 s\n",
      "Number of candidates: 3785\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 120 ms, sys: 316 ms, total: 436 ms\n",
      "Wall time: 3.51 s\n",
      "Number of candidates: 43\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 100 ms, sys: 292 ms, total: 392 ms\n",
      "Wall time: 3.48 s\n",
      "Number of candidates: 58\n"
     ]
    }
   ],
   "source": [
    "for k, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "    %time cand_extractor.apply(sents, split=k, parallelism=8)\n",
    "    print(\"Number of candidates:\", session.query(LocationExtraction).filter(LocationExtraction.split == k).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 43 candidate labels\n",
      "[========================================] 100%\n",
      "\n",
      "AnnotatorLabels created: 43\n",
      "CPU times: user 812 ms, sys: 40 ms, total: 852 ms\n",
      "Wall time: 902 ms\n",
      "Loading 58 candidate labels\n",
      "[========================================] 100%\n",
      "\n",
      "AnnotatorLabels created: 58\n",
      "CPU times: user 1.11 s, sys: 76 ms, total: 1.19 s\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel_utils import get_gold_labels_from_meta\n",
    "\n",
    "# Adding dev gold labels using dictionary\n",
    "%time missed_dev = get_gold_labels_from_meta(session, LocationExtraction, 'location', 1, annotator='gold', gold_dict = None)\n",
    "\n",
    "# Adding test gold labels using dictionary\n",
    "%time missed_test = get_gold_labels_from_meta(session, LocationExtraction, 'location', 2, annotator='gold', gold_dict= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel_utils import remove_gold_labels\n",
    "#Remove gold labels if you want -- uncomment!\n",
    "#remove_gold_labels(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Positive: 0.06\n"
     ]
    }
   ],
   "source": [
    "# Checking percent of gold labels that are positive\n",
    "from snorkel_utils import check_gold_perc\n",
    "perc_pos = check_gold_perc(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SANDBOX (area for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_docs[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,0,b'\"Fargo.'),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,1,b'920-850-6302'),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,2,b\"I'm back\"),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,3,b'and I need to finish what I started,'),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,4,b'clean comfortable incalls'),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,5,b\"I'm hot sexy and ready for you.\"),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,6,b\"5'5 brown eyes and a body to die for\"),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,7,b\"I'm looking to get spoiled.\"),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,8,b'Read reviews on T3R'),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,9,b\"if my legitimate pictures don't convince you go the extra mile, and bring me flowers or a flower and recieve a hot sexy picture with/of me HH/H specials Kali Sunshine\"),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,10,b'920-850-6302'),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,11,b'No AA No blocked calls INCALL ONLY.'),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,12,b'\\\\\\\\n<a href=\\\\\\\\/more.php?article=108&amp;p=1\\\\\\\\\"\"'),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,13,b'>'),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,14,b'We are on Adult Finder</a'),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,15,b'> soooo you can find us all on there if you want... sticky_dolce\\\\\\\\n<br>\\\\\\\\n'),\n",
       " Sentence(Document http://920-850-6302.escortphonelist.com/im-here-i-hope-your-near-3225785.html,16,b'<br>\"\"\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence(Document http://liveescortreviews.com/ad/sanjose/408-621-7949/1/199206,1,b'Ph@t @$$')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"{'location': 'Lisboa, Portugal',\\\\n 'title': '? ? ? ? ? ? ? ? VIOLET ? ? ? ? ? ? ? ? - Lisboa acompanhantes - '\\\\n          'backpage.com'}\"\r\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c9bc0949ada6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "doc = docs[1002]\n",
    "dict_string = doc.meta['extractions'].replace('|','').strip('\\n').strip('b').replace('\"\"','\"').replace('\\\\\"',\"\\\\\").replace('\\\\','\\\\\\\\')\n",
    "#dict_string = dict_string[1:-1]\n",
    "print(dict_string)\n",
    "import json\n",
    "a = json.loads(dict_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = eval(doc.meta['extractions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"{\\'location\\': \\'Lisboa, Portugal\\',\\\\n \\'title\\': \\'? ? ? ? ? ? ? ? VIOLET ? ? ? ? ? ? ? ? - Lisboa acompanhantes - \\'\\\\n          \\'backpage.com\\'}\"\\r\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.meta['extractions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
