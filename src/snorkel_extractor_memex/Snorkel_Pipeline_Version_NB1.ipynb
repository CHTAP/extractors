{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Build the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is ensure that modules are auto-reloaded at runtime to allow for development in other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set the Snorkel database location and start and connect to it.  By default, we use a PosgreSQL database backend, which can be created using `createdb DB_NAME` once psql is installed.  Note that Snorkel does *not* currently support parallel database processing with a SQLite backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Snorkel DB location\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#For PostgreSQL\n",
    "postgres_location = 'postgresql://jdunnmon:123@localhost:5432'\n",
    "\n",
    "postgres_db_name = 'es_locs_small'\n",
    "os.environ['SNORKELDB'] = os.path.join(postgres_location,postgres_db_name)\n",
    "\n",
    "# Adding path above for utils\n",
    "sys.path.append('..')\n",
    "\n",
    "# For SQLite\n",
    "#db_location = '.'\n",
    "#db_name = \"es_locs_small.db\"\n",
    "#os.environ['SNORKELDB'] = '{0}:///{1}/{2}'.format(\"sqlite\", db_location, db_name)\n",
    "\n",
    "# Start Snorkel session\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Setting random seed\n",
    "seed = 1701\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set the document preprocessor to read raw data into the Snorkel database.  There exist three possible data source options: JSONL files from the MEMEX project (option: `memex_jsons`), a raw tsv file of extractions from the memex project `content.tsv` (option: `content.tsv`), and tsvs with a similar format to `content.tsv` drawn from an Elasticsearch index of the data (option: `es`).  `max_docs` controls the number of documents read by the preprocessor, and `data_source` sets the location of the data.  For MEMEX json source, this should be a directory, while in all other cases it should be a tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import set_preprocessor\n",
    "\n",
    "# Set data source: options are 'content.tsv', 'memex_jsons', 'es'\n",
    "data_source = 'es'\n",
    "\n",
    "# Setting max number of docs to ingest\n",
    "max_docs = 1000\n",
    "\n",
    "# Setting location of data source\n",
    "\n",
    "# For ES:\n",
    "data_loc = '/dfs/scratch1/jdunnmon/data/memex-data/es/output_location.tsv'\n",
    "\n",
    "# Setting preprocessor\n",
    "doc_preprocessor = set_preprocessor(data_source,data_loc,\n",
    "                                    max_docs=max_docs,verbose=False,clean_docs=True,content_field='extracted_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we execute the preprocessor.  Parallelism can be changed using the `parallelism` flag.  Note that we use the Spacy parser rather than CoreNLP, as this tends to give superior results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 1.21 s, sys: 320 ms, total: 1.53 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "\n",
    "# Applying corpus parser\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(list(doc_preprocessor), parallelism=8, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the number of parsed documents and sentences in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 1000\n",
      "Sentences: 7337\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "# Printing number of docs/sentences\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating into train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 980 Docs, 7133 Sentences\n",
      "Dev: 10 Docs, 78 Sentences\n",
      "Test: 10 Docs, 126 Sentences\n",
      "CPU times: user 2.02 s, sys: 268 ms, total: 2.29 s\n",
      "Wall time: 3.37 s\n"
     ]
    }
   ],
   "source": [
    "from dataset_utils import create_test_train_splits\n",
    "\n",
    "# Getting all documents parsed by Snorkel\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "# Creating train, test, dev splits\n",
    "%time train_docs, dev_docs, test_docs, train_sents, dev_sents, test_sents = create_test_train_splits(docs, 'location', gold_dict=None, dev_frac=0.01, test_frac=0.01,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create candidate extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams\n",
    "from snorkel.candidates import CandidateExtractor\n",
    "from dataset_utils import create_candidate_class\n",
    "from snorkel_utils import get_location_matcher, get_candidate_filter, CandidateExtractorFilter, LocationMatcher\n",
    "\n",
    "# Setting extraction type -- should be a subfield in your data source extractions field!\n",
    "extraction_type = 'location'\n",
    "\n",
    "# Creating candidate class\n",
    "candidate_class, candidate_class_name  = create_candidate_class(extraction_type)\n",
    "\n",
    "# Defining ngrams for candidates\n",
    "location_ngrams   = Ngrams(n_max=3)\n",
    "\n",
    "# Uand matcher for candidate extractor\n",
    "location_matcher = LocationMatcher(longest_match_only=True)\n",
    "cand_extractor    = CandidateExtractor(candidate_class, [location_ngrams], [location_matcher])\n",
    "\n",
    "# For more complex matching/filtering behavior:\n",
    "#location_matcher  = get_location_matcher()\n",
    "#candidate_filter =  get_candidate_filter()\n",
    "#cand_extractor = CandidateExtractorFilter(LocationExtraction,[location_ngrams],[location_matcher],candidate_filter=candidate_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying candidate extractor to each split (train, dev, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 1.5 s, sys: 596 ms, total: 2.1 s\n",
      "Wall time: 6.53 s\n",
      "Number of candidates: 1708\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 68 ms, sys: 248 ms, total: 316 ms\n",
      "Wall time: 3.35 s\n",
      "Number of candidates: 22\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 80 ms, sys: 268 ms, total: 348 ms\n",
      "Wall time: 3.43 s\n",
      "Number of candidates: 24\n"
     ]
    }
   ],
   "source": [
    "# Applying candidate extractor to each split\n",
    "for k, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "    %time cand_extractor.apply(sents, split=k, parallelism=8)\n",
    "    print(\"Number of candidates:\", session.query(candidate_class).filter(candidate_class.split == k).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add gold labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 22 candidate labels\n",
      "[========================================] 100%\n",
      "\n",
      "AnnotatorLabels created: 22\n",
      "CPU times: user 420 ms, sys: 144 ms, total: 564 ms\n",
      "Wall time: 517 ms\n",
      "Loading 24 candidate labels\n",
      "[========================================] 100%\n",
      "\n",
      "AnnotatorLabels created: 24\n",
      "CPU times: user 284 ms, sys: 8 ms, total: 292 ms\n",
      "Wall time: 319 ms\n"
     ]
    }
   ],
   "source": [
    "from dataset_utils import get_gold_labels_from_meta\n",
    "\n",
    "# Adding dev gold labels using dictionary\n",
    "%time missed_dev = get_gold_labels_from_meta(session, candidate_class, extraction_type, 1, annotator='gold', gold_dict=None)\n",
    "\n",
    "# Adding test gold labels using dictionary\n",
    "%time missed_test = get_gold_labels_from_meta(session, candidate_class, extraction_type, 2, annotator='gold', gold_dict=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Positive: 0.20\n"
     ]
    }
   ],
   "source": [
    "# Checking percent of gold labels that are positive\n",
    "from dataset_utils import check_gold_perc\n",
    "perc_pos = check_gold_perc(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import remove_gold_labels\n",
    "# Remove gold labels if you want -- uncomment!\n",
    "#remove_gold_labels(session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
