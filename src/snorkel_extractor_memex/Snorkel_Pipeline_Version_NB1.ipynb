{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Snorkel DB location\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#For PostgreSQL\n",
    "postgres_location = 'postgresql://jdunnmon:123@localhost:5432'\n",
    "postgres_db_name = 'memex_db_snorkel'\n",
    "os.environ['SNORKELDB'] = os.path.join(postgres_location,postgres_db_name)\n",
    "\n",
    "# Adding path above for utils\n",
    "sys.path.append('..')\n",
    "# For SQLite\n",
    "#db_location = '.'\n",
    "#db_name = \"snorkel_memex.db\"\n",
    "#os.environ['SNORKELDB'] = '{0}:///{1}/{2}'.format(\"sqlite\", db_location, db_name)\n",
    "\n",
    "# Start Snorkel session\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel_utils import MemexTSVDocPreprocessor, MEMEXJsonLGZIPPreprocessor, retrieve_all_files\n",
    "\n",
    "# Setting path to MEMEX source data\n",
    "data_loc = '../../../data/data_sample'\n",
    "\n",
    "# Getting all file paths\n",
    "path_list = retrieve_all_files(data_loc)\n",
    "\n",
    "# Applying arbitrary conditions to file path list\n",
    "path_list = [a for a in path_list if a.endswith('gz')]\n",
    "path_list = path_list[5:6]\n",
    "\n",
    "# Setting max number of docs to ingest\n",
    "max_docs = 10000\n",
    "\n",
    "# Setting parallelism\n",
    "parallelism = 8\n",
    "\n",
    "# Preprocessing documents from path_list\n",
    "doc_preprocessor = MEMEXJsonLGZIPPreprocessor(data_loc,\\\n",
    "                                file_list=path_list,encoding='utf-8', max_docs=max_docs, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 2min 15s, sys: 17 s, total: 2min 32s\n",
      "Wall time: 15min 23s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "\n",
    "# Applying corpus parser\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(list(doc_preprocessor), parallelism=parallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 10000\n",
      "Sentences: 166822\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../../../data/data_sample/gold_loc.pickle', 'rb') as handle:\n",
    "    gold_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8000 Docs, 134982 Sentences\n",
      "Dev: 91 Docs, 1257 Sentences\n",
      "Test: 92 Docs, 1203 Sentences\n",
      "CPU times: user 27 s, sys: 7.99 s, total: 35 s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "from snorkel_utils import create_test_train_splits\n",
    "\n",
    "# Getting all documents parsed by Snorkel\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "# Creating train, test, dev splits\n",
    "%time train_docs, dev_docs, test_docs, train_sents, dev_sents, test_sents = create_test_train_splits(docs, 'location', gold_dict=gold_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import Candidate, candidate_subclass\n",
    "\n",
    "# Designing candidate subclasses\n",
    "LocationExtraction = candidate_subclass('Location', ['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import LocationMatcher\n",
    "\n",
    "# Defining ngrams and matcher for candidate extractor\n",
    "location_ngrams   = Ngrams(n_max=7)\n",
    "location_matcher  = LocationMatcher(longest_match_only=True)\n",
    "cand_extractor    = CandidateExtractor(LocationExtraction, [location_ngrams], [location_matcher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    }
   ],
   "source": [
    "for k, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "    %time cand_extractor.apply(sents, split=k, parallelism=parallelism)\n",
    "    print(\"Number of candidates:\", session.query(LocationExtraction).filter(LocationExtraction.split == k).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands_dev = session.query(LocationExtraction).filter(LocationExtraction.split == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 3981 candidate labels\n",
      "[========================================] 100%\n",
      "\n",
      "AnnotatorLabels created: 3981\n",
      "CPU times: user 35.9 s, sys: 1.27 s, total: 37.1 s\n",
      "Wall time: 45.9 s\n",
      "Loading 4172 candidate labels\n",
      "[========================================] 100%\n",
      "\n",
      "AnnotatorLabels created: 4172\n",
      "CPU times: user 37.2 s, sys: 1.12 s, total: 38.3 s\n",
      "Wall time: 47.4 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel_utils import get_gold_labels_from_meta\n",
    "\n",
    "# Adding dev gold labels using dictionary\n",
    "%time missed_dev = get_gold_labels_from_meta(session, LocationExtraction, 'location', 1, annotator='gold')\n",
    "\n",
    "# Adding test gold labels using dictionary\n",
    "%time missed_test = get_gold_labels_from_meta(session, LocationExtraction, 'location', 2, annotator='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
