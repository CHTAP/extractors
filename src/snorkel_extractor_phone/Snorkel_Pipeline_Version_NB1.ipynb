{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Build the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is ensure that modules are auto-reloaded at runtime to allow for development in other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set the Snorkel database location and start and connect to it.  By default, we use a PosgreSQL database backend, which can be created using `createdb DB_NAME` once psql is installed.  Note that Snorkel does *not* currently support parallel database processing with a SQLite backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Snorkel DB location\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#For networked PostgreSQL\n",
    "postgres_location = 'postgresql://jdunnmon:123@localhost:5432'\n",
    "postgres_db_name = 'phone_jd_30K'\n",
    "os.environ['SNORKELDB'] = os.path.join(postgres_location,postgres_db_name)\n",
    "\n",
    "#For local PostgreSQL\n",
    "#os.environ['SNORKELDB'] = 'postgres:///es_locs_small'\n",
    "\n",
    "# Adding path above for utils\n",
    "sys.path.append('../utils')\n",
    "\n",
    "# For SQLite\n",
    "#db_location = '.'\n",
    "#db_name = \"es_locs_small.db\"\n",
    "#os.environ['SNORKELDB'] = '{0}:///{1}/{2}'.format(\"sqlite\", db_location, db_name)\n",
    "\n",
    "# Start Snorkel session\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Setting random seed\n",
    "seed = 1701\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set the document preprocessor to read raw data into the Snorkel database.  There exist three possible data source options: JSONL files from the MEMEX project (option: `memex_jsons`), a raw tsv file of extractions from the memex project `content.tsv` (option: `content.tsv`), and tsvs with a similar format to `content.tsv` drawn from an Elasticsearch index of the data (option: `es`).  `max_docs` controls the number of documents read by the preprocessor, and `data_source` sets the location of the data.  For MEMEX json source, this should be a directory, while in all other cases it should be a tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import set_preprocessor, combine_dedupe\n",
    "\n",
    "# Set data source: options are 'content.tsv', 'memex_jsons', 'es'\n",
    "data_source = 'es'\n",
    "\n",
    "# Setting max number of docs to ingest\n",
    "max_docs = 36000\n",
    "\n",
    "# Setting location of data source\n",
    "\n",
    "# For ES:\n",
    "data_loc = '/lfs/local/0/jdunnmon/data/chtap/output_phone.tsv'\n",
    "\n",
    "# Optional: add tsv with additional documents to create combined tsv without duplicates\n",
    "#data_all_loc = '/dfs/scratch1/jdunnmon/data/memex-data/es/output_all.tsv'\n",
    "#data_loc = combine_dedupe(data_loc, data_all_loc, '/dfs/scratch1/jdunnmon/data/memex-data/es/combined_phone_1M.tsv')\n",
    "\n",
    "# Setting preprocessor\n",
    "doc_preprocessor = set_preprocessor(data_source, data_loc, max_docs=max_docs, verbose=True,\n",
    "                                    clean_docs=True, content_field=['memex_raw_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we execute the preprocessor.  Parallelism can be changed using the `parallelism` flag.  Note that we use the Spacy parser rather than CoreNLP, as this tends to give superior results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "\n",
    "# Applying corpus parser\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(list(doc_preprocessor), parallelism=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the number of parsed documents and sentences in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 100\n",
      "Sentences: 1707\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "# Printing number of docs/sentences\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating into train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataset_utils import create_test_train_splits\n",
    "\n",
    "# Getting all documents parsed by Snorkel\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "# Creating train, test, dev splits\n",
    "%time train_docs, dev_docs, test_docs, train_sents, dev_sents, test_sents = create_test_train_splits(docs, 'phone', gold_dict=None, dev_frac=0.1, test_frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create candidate extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import io\n",
    "import codecs\n",
    "import json\n",
    "import phonenumbers\n",
    "        \n",
    "def find_phone_number(span_input):   \n",
    "    span_input=span_input.get_span()\n",
    "    \n",
    "    lst =[]\n",
    "    for match in phonenumbers.PhoneNumberMatcher(span_input, \"US\"):\n",
    "        num = phonenumbers.format_number(match.number, phonenumbers.PhoneNumberFormat.NATIONAL)\n",
    "        lst.append(num.encode('utf-8'))\n",
    "    \n",
    "    if len(lst)!=0:\n",
    "        \n",
    "        return True\n",
    "        print(lst)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def find_phone_number_reg(span_input):\n",
    "    span_input = span_input.get_span()\n",
    "    reg1= re.findall(\"\\d{10}\",span_input )\n",
    "    reg2 = re.findall(\"(\\d{3}\\D{0,3}\\d{3}\\D{0,3}\\d{4})\", span_input)\n",
    "    reg3 = re.findall(\"^(\\+\\d{1,2}\\s)?\\(?\\d{3}\\)?[\\s*.-~]?\\d{3}[\\s*.-~]?\\d{4}$\",span_input )\n",
    "    reg4 = re.findall(\"^(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4}$)\",span_input)\n",
    "    if len(reg1)!=0  or len(reg3)!=0 or len(reg4)!=0 or len(reg2)!=0:\n",
    "\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def count_(span_input, pattern):\n",
    "    count = 0\n",
    "    while len(span_input)>0:\n",
    "        idx = span_input.find(pattern) # returns first position of character matching pattern\n",
    "        span_input = span_input[idx+len(pattern):]\n",
    "        if idx<0:\n",
    "            break\n",
    "        else:\n",
    "            count+=1\n",
    "    return count\n",
    "    \n",
    "def phone_matcher (span_input):\n",
    "    span_input = span_input.get_span()\n",
    "   \n",
    "    l1 = len([char for char in span_input if char.isdigit()])\n",
    "    for nb in ['one', 'two', 'three','four','five','six','seven','eight','nine','ten']:\n",
    "        l1+=count_(span_input,nb)\n",
    "    result =  (l1>=10 and l1<11)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from dataset_utils import CandidateExtractorFilter\n",
    "from snorkel.matchers import *\n",
    "\n",
    "phone_lambda_matcher_1 =LambdaFunctionMatcher(func=find_phone_number)\n",
    "phone_lambda_matcher =LambdaFunctionMatcher(func=find_phone_number_reg)\n",
    "phone_lambda_matcher_2 = LambdaFunctionMatcher(func=phone_matcher)\n",
    "phone_matcher = Union(phone_lambda_matcher_1,phone_lambda_matcher,phone_lambda_matcher_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams\n",
    "from snorkel.candidates import CandidateExtractor\n",
    "from dataset_utils import create_candidate_class, LocationMatcher\n",
    "\n",
    "# Setting extraction type -- should be a subfield in your data source extractions field!\n",
    "extraction_type = 'phone'\n",
    "\n",
    "# Creating candidate class\n",
    "candidate_class, candidate_class_name = create_candidate_class(extraction_type)\n",
    "\n",
    "# Defining ngrams for candidates\n",
    "ngrams = Ngrams(n_max=5)\n",
    "\n",
    "# Uand matcher for candidate extractor\n",
    "matcher = phone_matcher\n",
    "cand_extractor = CandidateExtractorFilter(candidate_class ,[ngrams],[matcher],candidate_filter=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying candidate extractor to each split (train, dev, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying candidate extractor to each split\n",
    "for k, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "    %time cand_extractor.apply(sents, split=k, parallelism=1)\n",
    "    print(\"Number of candidates:\", session.query(candidate_class).filter(candidate_class.split == k).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add gold labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import get_gold_labels_from_meta\n",
    "\n",
    "# Adding dev gold labels using dictionary\n",
    "%time missed_dev = get_gold_labels_from_meta(session, candidate_class, extraction_type, 1, annotator='gold', gold_dict=None)\n",
    "\n",
    "# Adding test gold labels using dictionary\n",
    "%time missed_test = get_gold_labels_from_meta(session, candidate_class, extraction_type, 2, annotator='gold', gold_dict=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking percent of gold labels that are positive\n",
    "from dataset_utils import check_gold_perc\n",
    "perc_pos = check_gold_perc(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import remove_gold_labels\n",
    "# Remove gold labels if you want -- uncomment!\n",
    "#remove_gold_labels(session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
