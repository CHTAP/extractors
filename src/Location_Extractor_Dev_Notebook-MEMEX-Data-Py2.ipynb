{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Parsing Files, Adding Candidates and Labels to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Loading config\n",
    "with open(\"run_config_memex.json\") as fl:\n",
    "    cfg = json.load(fl)\n",
    "cfg_params = cfg['parameters']\n",
    "\n",
    "# Setting snorkel path and output root\n",
    "import os\n",
    "from os.path import join\n",
    "output_root = join(cfg_params['output_path'],cfg_params['experiment_name'])\n",
    "\n",
    "# Old import grammar\n",
    "os.environ['FONDUERDBNAME'] = cfg['postgres_db_name']\n",
    "os.environ['SNORKELDB'] = join(cfg['postgres_location'],os.environ['FONDUERDBNAME'])\n",
    "\n",
    "# For loading input files\n",
    "import pandas as pd\n",
    "\n",
    "# For running Snorkel\n",
    "from fonduer import SnorkelSession\n",
    "from fonduer.models import candidate_subclass\n",
    "from fonduer import HTMLPreprocessor, OmniParser\n",
    "#from fonduer import Meta\n",
    "from utils import MEMEXJsonPreprocessor, HTMLListPreprocessor\n",
    "\n",
    "#old snorkel imports\n",
    "#from snorkel.contrib.fonduer import SnorkelSession\n",
    "#from snorkel.contrib.fonduer.models import candidate_subclass\n",
    "#from snorkel.contrib.fonduer import HTMLPreprocessor, OmniParser\n",
    "#from utils import HTMLListPreprocessor, MEMEXJsonPreprocessor\n",
    "\n",
    "#from sqlalchemy import create_engine\n",
    "#snorkeldb = create_engine(os.environ['SNORKELDB'], isolation_level=\"AUTOCOMMIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up jsons\n",
    "# Load up content.tsv for gold labels?\n",
    "# Run doc preprocessor on jsons to get raw content\n",
    "\n",
    "#Creating path to labeled data\n",
    "pth_labeled = cfg['data_path']\n",
    "\n",
    "# Getting labeled data file name\n",
    "fl_labeled = cfg['labeled_data_path']\n",
    "\n",
    "# Loading labeled data into dataframe\n",
    "#df_labeled = pd.read_csv(join(pth_labeled,fl_labeled),sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load html data from json files\n",
    "fl_unlabeled = cfg['unlabeled_data_path']\n",
    "path_list = os.listdir(os.path.join(cfg['data_path'],fl_unlabeled))\n",
    "\n",
    "# Start snorkel session and creating location subclass\n",
    "session = SnorkelSession()     \n",
    "#session = Meta.init(\"postgres://localhost:5432/\" + cfg['postgres_db_name']).SnorkelSession()\n",
    "Location_Extraction = candidate_subclass('location_extraction',\\\n",
    "                          [\"location\"])\n",
    "Phone_Extraction = candidate_subclass('location_extraction',\\\n",
    "                          [\"location\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting parameter for max number of docs to load from labeled/unlabeled\n",
    "#max_docs = cfg['max_docs']\n",
    "max_docs = 10\n",
    "\n",
    "# Setting location for raw data\n",
    "data_loc = join(cfg['data_path'],cfg['unlabeled_data_path'])\n",
    "\n",
    "# Setting jsons to load\n",
    "path_list = path_list\n",
    "\n",
    "# Preprocessing documents from path_list\n",
    "doc_preprocessor = MEMEXJsonPreprocessor(data_loc,\\\n",
    "                                file_list=path_list,max_docs=max_docs,encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 s, sys: 1.26 s, total: 3.35 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "# Ingest data into Fonduer via parser\n",
    "corpus_parser = OmniParser(structural=True, lingual=True, visual=False)\n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=cfg['parallel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 10\n",
      "Phrases: 1358\n"
     ]
    }
   ],
   "source": [
    "from fonduer.models import Document, Phrase\n",
    "\n",
    "# Checking database contents\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Phrases:\", session.query(Phrase).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Dividing into Test/Train, Extracting Features, Throttling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 8\n",
      "dev: 1\n",
      "test: 1\n",
      "['http://www.eroticmugshots.com/niagara-escorts/289-769-2254/?pid=52452592',\n",
      " 'http://832-356-0203.escortsincollege.com/hight-class-e-b-o-n-y-f-r-e-a-k-100-hour-outca-new-booty-in-hot-hot-hot-13943272.html',\n",
      " 'http://providence.backpage.com/FemaleEscorts/calander-girl-fitness-model-harley-davidson-poster-girl-check-me-out/5695015',\n",
      " 'http://dublin.backpage.com/MaleEscorts/boyfriend-experience-and-much-more-top-xl-0872146995-100-discreet-and-very-clean/4191287',\n",
      " 'http://springfieldmo.backpage.com/FemaleEscorts/new-years-specials-here-to-fulfill-your-fantasies-let-me-be-your-dream-come-true/23130605',\n",
      " 'http://illinois.backpage.com/FemaleEscorts/outcalls-downtown/38322308',\n",
      " 'http://watertown.classivox.com/female-escorts/315-784-1993-hey-guys-its-me-serenity-here-to-help-you-smile/v/494716/',\n",
      " 'http://illinois.backpage.com/FemaleEscorts/you-host-i-party-real-sexy-sweet-2-girls/33774853']\n",
      "['http://www.eroticmugshots.com/kansascity-escorts/913-938-7303/?pid=16205230']\n",
      "['http://joplin.backpage.com/FemaleEscorts/classic-dark-haired-beauty-for-the-man-who-will-not-settle-for-less-than-the-best/23416230']\n"
     ]
    }
   ],
   "source": [
    "# Getting all documents parsed by Fonduer\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "dev_set_sz = np.round(ld*0.1)\n",
    "test_set_sz = np.round(ld*0.1)\n",
    "train_set_sz = ld - dev_set_sz - test_set_sz\n",
    "\n",
    "# Setting up train, dev, and test sets\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "\n",
    "# Creating list of (document name, Fonduer document object) tuples\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "shuffle(data)\n",
    "\n",
    "# Adding unlabeled data to train set, \n",
    "# labaled data to dev/test sets in alternating fashion\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if i<train_set_sz:\n",
    "        train_docs.add(doc)\n",
    "    else:\n",
    "        if len(dev_docs)<=len(test_docs):\n",
    "            dev_docs.add(doc)\n",
    "        else:\n",
    "            test_docs.add(doc)\n",
    "\n",
    "#Printing length of train/test/dev sets\n",
    "print(\"train:\",len(train_docs))\n",
    "print(\"dev:\" ,len(dev_docs))\n",
    "print(\"test:\",len(test_docs))\n",
    "\n",
    "#Printing some filenames \n",
    "from pprint import pprint\n",
    "pprint([x.name for x in train_docs])\n",
    "pprint([x.name for x in dev_docs])\n",
    "pprint([x.name for x in test_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing matchers module and defining LocationMatcher\n",
    "from fonduer.snorkel.matchers import *\n",
    "location_matcher = LocationMatcher(longest_match_only=False) \n",
    "\n",
    "#importing NGrams and defining location_ngrams \n",
    "from fonduer.candidates import OmniNgrams\n",
    "location_ngrams = OmniNgrams(n_max=10, split_tokens=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.lf_helpers import *\n",
    "import re\n",
    "\n",
    "# Creating filter to eliminate mentions of currency  \n",
    "def location_currencies_filter(location):\n",
    "    list_currencies = [ \"dollar\", \"dollars\", \"lira\",\"kwacha\",\"rials\",\"rial\",\"dong\",\"dongs\",\"fuerte\",\"euro\",\n",
    "                       \"euros\",\"vatu\",\"som\",\"peso\",\"sterling\",\"sterlings\",\"soms\",\"pestos\",\n",
    "                       \"pounds\", \n",
    "                  \"pound\",\"dirham\",\"dirhams\",\"hryvnia\",\"manat\",\"manats\",\"liras\",\"lira\",\n",
    "                       \"dinar\",\"dinars\",\"pa'anga\",\"franc\",\"baht\",\"schilling\",\n",
    "                  \"somoni\",\"krona\",\"lilangeni\",\"rupee\",\"rand\",\"shilling\",\"leone\",\"riyal\",\"dobra\",\n",
    "                  \"tala\",\"ruble\",\"zloty\",\"peso\",\"sol\",\"quarani\",\"kina\",\"guinean\",\"balboa\",\"krone\",\"naira\",\n",
    "                  \"cordoba\",\"kyat\",\"metical\",\"togrog\",\"leu\",\"ouguiya\",\"rufiyaa\",\"ringgit\",\"kwacha\",\n",
    "                  \"ariary\",\"denar\",\"litas\",\"loti\",\"lats\",\"kip\",\"som\",\"won\",\"tenge\",\"yen\",\"shekel\",\"rupiah\",\n",
    "                  \"forint\",\"lempira\",\"gourde\",\"quetzal\",\"cedi\",\"lari\",\"dalasi\",\"cfp\",\"birr\",\"kroon\",\"nakfa\",\n",
    "                  \"cfa\",\"Peso\",\"koruna\",\"croatian\",\"colon\",\"yuan\",\"escudo\",\"cape\",\"riel\",\"lev\",\"real\"\n",
    "                  ,\"real\",\"mark\",\"boliviano\",\"ngultrum\",\"taka\",\"manat\",\"dram\",\"kwanza\",\"lek\",\"afghani\",\"renminbi\"]\n",
    "\n",
    "    \n",
    "    cand_right_tokens = list(get_right_ngrams(location,window=2))\n",
    "    for cand in cand_right_tokens:\n",
    "        if cand not in list_currencies:\n",
    "            return location\n",
    "\n",
    "# Setting candidate filter to location_currencies_filter\n",
    "candidate_filter = location_currencies_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80 ms, sys: 588 ms, total: 668 ms\n",
      "Wall time: 3.82 s\n",
      "Number of candidates: 13\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.68 µs\n",
      "Number of candidates: 0\n",
      "Number of candidates: 0\n"
     ]
    }
   ],
   "source": [
    "from fonduer.candidates import CandidateExtractor\n",
    "\n",
    "# Defining candidate extractor\n",
    "candidate_extractor = CandidateExtractor(Location_Extraction,\n",
    "                                         [location_ngrams], [location_matcher],\n",
    "                                         candidate_filter=candidate_filter)\n",
    "\n",
    "# Extracting candidates from each split\n",
    "%time candidate_extractor.apply(train_docs, split=0, parallelism=cfg['parallel'])\n",
    "print(\"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == 0).count())\n",
    "%time\n",
    "for i, docs in enumerate([dev_docs, test_docs]):\n",
    "    candidate_extractor.apply(docs, split=i+1, parallelism=cfg['parallel'])\n",
    "    print(\"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == i+1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cands_test = session.query(Location_Extraction).filter(Location_Extraction.split == 2).all()\n",
    "len(cands_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d20073a11f19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcands_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcands_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ind = 1\n",
    "print(cands_test[ind])\n",
    "cands_test[ind].get_parent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[location_extraction(Span(\"b'Earth'\", sentence=122795, chars=[17,21], words=[4,4])),\n",
       " location_extraction(Span(\"b'East'\", sentence=41477, chars=[0,3], words=[0,0])),\n",
       " location_extraction(Span(\"b'East'\", sentence=41017, chars=[28,31], words=[9,9])),\n",
       " location_extraction(Span(\"b'East Bay'\", sentence=41017, chars=[28,35], words=[9,10])),\n",
       " location_extraction(Span(\"b'Bay'\", sentence=41017, chars=[33,35], words=[10,10])),\n",
       " location_extraction(Span(\"b'\\xef\\xb8\\x8f'\", sentence=63026, chars=[0,0], words=[0,0])),\n",
       " location_extraction(Span(\"b'Broad River'\", sentence=49480, chars=[19,29], words=[3,4])),\n",
       " location_extraction(Span(\"b'Broad'\", sentence=49480, chars=[19,23], words=[3,3])),\n",
       " location_extraction(Span(\"b'River'\", sentence=49480, chars=[25,29], words=[4,4])),\n",
       " location_extraction(Span(\"b'Turning'\", sentence=74184, chars=[45,51], words=[10,10])),\n",
       " location_extraction(Span(\"b'South'\", sentence=109300, chars=[19,23], words=[3,3])),\n",
       " location_extraction(Span(\"b'\\xef\\xb8\\x8fIS'\", sentence=135804, chars=[9,11], words=[7,7])),\n",
       " location_extraction(Span(\"b'\\xef\\xb8\\x8fIS  '\", sentence=135804, chars=[9,13], words=[7,8])),\n",
       " location_extraction(Span(\"b'\\xef\\xb8\\x8f'\", sentence=135774, chars=[0,0], words=[0,0])),\n",
       " location_extraction(Span(\"b' '\", sentence=135804, chars=[13,13], words=[8,8])),\n",
       " location_extraction(Span(\"b'the Best'\", sentence=29401, chars=[6,13], words=[2,3])),\n",
       " location_extraction(Span(\"b'Best'\", sentence=29401, chars=[10,13], words=[3,3])),\n",
       " location_extraction(Span(\"b'the Best EXTiC'\", sentence=29401, chars=[6,19], words=[2,4])),\n",
       " location_extraction(Span(\"b'Best EXTiC'\", sentence=29401, chars=[10,19], words=[3,4])),\n",
       " location_extraction(Span(\"b'EXTiC'\", sentence=29401, chars=[15,19], words=[4,4])),\n",
       " location_extraction(Span(\"b'the'\", sentence=29401, chars=[6,8], words=[2,2])),\n",
       " location_extraction(Span(\"b'NJ area'\", sentence=12409, chars=[64,70], words=[12,13])),\n",
       " location_extraction(Span(\"b'\\xf0\\x9f\\x91\\x80'\", sentence=145830, chars=[76,76], words=[20,20])),\n",
       " location_extraction(Span(\"b'area'\", sentence=12409, chars=[67,70], words=[13,13])),\n",
       " location_extraction(Span(\"b'\\xf0\\x9f\\x91\\x80\\xf0\\x9f\\x9a\\xbf'\", sentence=145830, chars=[76,77], words=[20,21])),\n",
       " location_extraction(Span(\"b'NJ area for'\", sentence=12409, chars=[64,74], words=[12,14])),\n",
       " location_extraction(Span(\"b'\\xf0\\x9f\\x91\\x80\\xf0\\x9f\\x9a\\xbf<'\", sentence=145830, chars=[76,78], words=[20,22])),\n",
       " location_extraction(Span(\"b'area for'\", sentence=12409, chars=[67,74], words=[13,14])),\n",
       " location_extraction(Span(\"b'\\xf0\\x9f\\x91\\x80\\xf0\\x9f\\x9a\\xbf<\\n'\", sentence=145830, chars=[76,79], words=[20,23])),\n",
       " location_extraction(Span(\"b'for'\", sentence=12409, chars=[72,74], words=[14,14])),\n",
       " location_extraction(Span(\"b'\\xf0\\x9f\\x9a\\xbf<\\n'\", sentence=145830, chars=[77,79], words=[21,23])),\n",
       " location_extraction(Span(\"b'NJ'\", sentence=12409, chars=[64,65], words=[12,12])),\n",
       " location_extraction(Span(\"b'\\xf0\\x9f\\x9a\\xbf<'\", sentence=145830, chars=[77,78], words=[21,22])),\n",
       " location_extraction(Span(\"b'New Asia Model Sammie'\", sentence=96761, chars=[56,76], words=[14,17])),\n",
       " location_extraction(Span(\"b'<\\n'\", sentence=145830, chars=[78,79], words=[22,23])),\n",
       " location_extraction(Span(\"b'Model'\", sentence=96761, chars=[65,69], words=[16,16])),\n",
       " location_extraction(Span(\"b'\\xf0\\x9f\\x9a\\xbf'\", sentence=145830, chars=[77,77], words=[21,21])),\n",
       " location_extraction(Span(\"b'Asia Model Sammie'\", sentence=96761, chars=[60,76], words=[15,17])),\n",
       " location_extraction(Span(\"b'<'\", sentence=145830, chars=[78,78], words=[22,22])),\n",
       " location_extraction(Span(\"b'Model Sammie'\", sentence=96761, chars=[65,76], words=[16,17])),\n",
       " location_extraction(Span(\"b'\\n'\", sentence=145830, chars=[79,79], words=[23,23])),\n",
       " location_extraction(Span(\"b'New'\", sentence=96761, chars=[56,58], words=[14,14])),\n",
       " location_extraction(Span(\"b'Sammie'\", sentence=96761, chars=[71,76], words=[17,17])),\n",
       " location_extraction(Span(\"b'Asia'\", sentence=97870, chars=[0,3], words=[0,0])),\n",
       " location_extraction(Span(\"b'New Asia'\", sentence=96761, chars=[56,63], words=[14,15])),\n",
       " location_extraction(Span(\"b'Asia'\", sentence=96761, chars=[60,63], words=[15,15])),\n",
       " location_extraction(Span(\"b'New Asia Model'\", sentence=96761, chars=[56,69], words=[14,16])),\n",
       " location_extraction(Span(\"b'Asia Model'\", sentence=96761, chars=[60,69], words=[15,16]))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.query(Location_Extraction).filter(Location_Extraction.split == 2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 25.7 µs\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "(psycopg2.ProgrammingError) relation \"location_extraction_feature_updates\" already exists\n [SQL: 'CREATE TABLE location_extraction_feature_updates(candidate_id integer PRIMARY KEY, keys text[] NOT NULL, values real[] NOT NULL)'] (Background on this error at: http://sqlalche.me/e/f405)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1192\u001b[0m                         \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m                         context)\n\u001b[0m\u001b[1;32m   1194\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProgrammingError\u001b[0m: relation \"location_extraction_feature_updates\" already exists\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-07ad39140ed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Running for train set -- replace_key_set = True!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mF_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace_key_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallelism\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parallel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/fonduer/async_annotations.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, split, key_group, replace_key_set, update_keys, update_values, storage, ignore_keys, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;31m# Insert and update keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtable_already_exists\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mold_table_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                 \u001b[0mcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CREATE TABLE %s(candidate_id integer PRIMARY KEY, keys text[] NOT NULL, values real[] NOT NULL)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0mcopy_postgres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment_file_blob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'candidate_id, keys, values'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mremove_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment_file_blob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, object, *multiparams, **params)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \"\"\"\n\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_on_connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_text\u001b[0;34m(self, statement, multiparams, params)\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m             \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m         )\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m                 \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m                 context)\n\u001b[0m\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 util.raise_from_cause(\n\u001b[1;32m   1412\u001b[0m                     \u001b[0msqlalchemy_exception\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m                     \u001b[0mexc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m                 )\n\u001b[1;32m   1415\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mraise_from_cause\u001b[0;34m(exception, exc_info)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mcause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_tb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpy3k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb, cause)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1191\u001b[0m                         \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m                         \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m                         context)\n\u001b[0m\u001b[1;32m   1194\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             self._handle_dbapi_exception(\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py36torch/lib/python3.6/site-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute_no_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProgrammingError\u001b[0m: (psycopg2.ProgrammingError) relation \"location_extraction_feature_updates\" already exists\n [SQL: 'CREATE TABLE location_extraction_feature_updates(candidate_id integer PRIMARY KEY, keys text[] NOT NULL, values real[] NOT NULL)'] (Background on this error at: http://sqlalche.me/e/f405)"
     ]
    }
   ],
   "source": [
    "# Applying the featurizer (to get feature vector describing the input)\n",
    "from fonduer import BatchFeatureAnnotator\n",
    "session.rollback()\n",
    "featurizer = BatchFeatureAnnotator(Location_Extraction)\n",
    "# Running for train set -- replace_key_set = True!\n",
    "%time\n",
    "F_train = featurizer.apply(split=0, replace_key_set=True, parallelism=cfg['parallel'])\n",
    "session.rollback()\n",
    "print(F_train.shape)\n",
    "# Running for dev set -- replace_key_set = False! Uses same featuers as dev set\n",
    "%time \n",
    "F_dev = featurizer.apply(split=1, clear=True, replace_key_set=False, parallelism=cfg['parallel'])\n",
    "session.rollback()\n",
    "print(F_dev.shape)\n",
    "%time \n",
    "F_test = featurizer.apply(split=2, clear=True, replace_key_set=False, parallelism=cfg['parallel'])\n",
    "session.rollback()\n",
    "print(F_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Adding Gold Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import range\n",
    "import csv\n",
    "import codecs\n",
    "import pycountry\n",
    "import us\n",
    "import editdistance\n",
    "\n",
    "from snorkel.utils import ProgressBar\n",
    "from snorkel.models import GoldLabel, GoldLabelKey\n",
    "\n",
    "# Defining function for getting gold labels\n",
    "# Could go in utils file later!\n",
    "\n",
    "def lookup_country_name(cn):\n",
    "    try:\n",
    "        out = pycountry.countries.lookup(cn).name\n",
    "    except:\n",
    "        out = 'no country'\n",
    "    return out\n",
    "\n",
    "def lookup_country_alpha3(cn):\n",
    "    try:\n",
    "        out = pycountry.countries.lookup(cn).alpha_3\n",
    "    except:\n",
    "        out = 'no country'\n",
    "    return out\n",
    "\n",
    "def lookup_country_alpha2(cn):\n",
    "    try:\n",
    "        out = pycountry.countries.lookup(cn).alpha_2\n",
    "    except:\n",
    "        out = 'no country'\n",
    "    return out\n",
    "\n",
    "def lookup_state_name(cn):\n",
    "    try:\n",
    "        out = us.states.lookup(val).name\n",
    "    except:\n",
    "        out = 'no state'\n",
    "    return out\n",
    "\n",
    "def lookup_state_abbr(cn):\n",
    "    try:\n",
    "        out = us.states.lookup(val).abbr\n",
    "    except:\n",
    "        out = 'no state'\n",
    "    return out\n",
    "\n",
    "def check_editdistance(val,targets):\n",
    "    for tgt in targets:\n",
    "        if editdistance.eval(val,tgt)<=3:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def match_val_targets(val,targets):\n",
    "    if val in targets: return True\n",
    "    if lookup_country_name(val).lower() in targets: return True\n",
    "    if lookup_country_alpha2(val).lower() in targets: return True\n",
    "    if lookup_country_alpha3(val).lower() in targets: return True\n",
    "    if lookup_state_name(val).lower() in targets: return True\n",
    "    if lookup_state_abbr(val).lower() in targets: return True\n",
    "    if any([a in val for a in targets]): return True\n",
    "    if check_editdistance(val,targets): return True\n",
    "    return False\n",
    "    \n",
    "def load_chtap_labels(session, candidate_class, df, target, annotator_name='gold'):\n",
    "    \n",
    "    # Database nonsense to make sure that there is a \"gold\" annotator \n",
    "    ak = session.query(GoldLabelKey).filter(GoldLabelKey.name == annotator_name).first()\n",
    "    if ak is None:\n",
    "        ak = GoldLabelKey(name=annotator_name)\n",
    "        session.add(ak)\n",
    "        session.commit()   \n",
    "    \n",
    "    # Getting all candidates from dev/test set only (splits 1 and 2)\n",
    "    candidates = session.query(candidate_class).filter(candidate_class.split != 0).all()\n",
    "    cand_total = len(candidates)\n",
    "    print('Loading', cand_total, 'candidate labels')\n",
    "    pb = ProgressBar(cand_total)\n",
    "    labels=[]\n",
    "    \n",
    "    # For each candidate, add appropriate gold label\n",
    "    for i, c in enumerate(candidates):\n",
    "        pb.bar(i)\n",
    "        # Get document name for candidate\n",
    "        doc = c[0].sentence.document.name\n",
    "        # Get text span for candidate\n",
    "        val = c[0].get_span().lower()\n",
    "        # Get location label from labeled dataframe (input)\n",
    "        target_strings = df[df['file name']==doc][target].tolist()\n",
    "        # Handling location extraction\n",
    "        if target == 'location':\n",
    "                if target_strings == []:\n",
    "                    targets = ''\n",
    "                else:\n",
    "                    targets = target_strings[0].lower().split(',')\n",
    "                    targets = [a.strip() for a in targets]\n",
    "        # Keeping this in comments...don't know what it was for\n",
    "        #context_stable_ids = '~~'.join([i.stable_id for i in c.get_contexts()])\n",
    "        label = session.query(GoldLabel).filter(GoldLabel.key == ak).filter(GoldLabel.candidate == c).first()\n",
    "        if label is None:\n",
    "            # Matching target label string to extract span, adding TRUE label if found, FALSE if not\n",
    "            # This conditional could be improved (use regex, etc.)\n",
    "            if match_val_targets(val,targets):\n",
    "                label = GoldLabel(candidate=c, key=ak, value=1)\n",
    "            else:\n",
    "                label = GoldLabel(candidate=c, key=ak, value=-1)\n",
    "            session.add(label)\n",
    "            labels.append(label)\n",
    "    session.commit()\n",
    "    pb.close()\n",
    "    print(\"AnnotatorLabels created: %s\" % (len(labels),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1030 candidate labels\n",
      "[========================================] 100%\n",
      "AnnotatorLabels created: 1030\n"
     ]
    }
   ],
   "source": [
    "# Adding gold labels to database\n",
    "session.rollback()\n",
    "target = 'location'\n",
    "\n",
    "load_chtap_labels(session, Location_Extraction, df_labeled, target ,annotator_name='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Set Balance: 50.68 Percent Positive\n",
      "Test Set Balance: 57.24 Percent Positive\n"
     ]
    }
   ],
   "source": [
    "# Check class balance on dev/test\n",
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "print('Dev Set Balance: %0.2f Percent Positive' % (100*np.sum(L_gold_dev == 1)/L_gold_dev.shape[0]))\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "print('Test Set Balance: %0.2f Percent Positive' % (100*np.sum(L_gold_test == 1)/L_gold_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Creating LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " cand_dev = session.query(Location_Extraction).filter(Location_Extraction.split == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for true/false/abstain\n",
    "TRUE,FALSE,ABSTAIN = 1,-1,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.lf_helpers import *\n",
    "import re\n",
    "# Defining LFs\n",
    "# LF API is here: http://web.stanford.edu/~lwhsiao/api/\n",
    "\n",
    "def LF_in_breadcrumbs_1(c):\n",
    "    parent_text = c.get_parent().text\n",
    "    return FALSE if '>' in parent_text else ABSTAIN\n",
    "\n",
    "def LF_long_candidate(c):\n",
    "    parent_text = c.get_parent().text\n",
    "    return FALSE if len(parent_text) > 1000 else ABSTAIN\n",
    "\n",
    "def LF_common_real_words(c):\n",
    "    reg_pos = re.compile('other cities|since|day|escorts',re.IGNORECASE)\n",
    "    if reg_pos.search(c.get_parent().text):\n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "#def LF_in_breadcrumbs_2(c):\n",
    "#    attributes = list(get_attributes(c))\n",
    "#    return TRUE if ('class=breadcrumbs'in attributes) or ('class=inside_scroll' in attributes) else ABSTAIN\n",
    "\n",
    "def LF_head_in_tag(c):\n",
    "    tags = list(get_ancestor_tag_names(c))\n",
    "    return FALSE if 'head' in tags else TRUE\n",
    "\n",
    "def LF_body_in_tag(c):\n",
    "    tags = list(get_ancestor_tag_names(c))\n",
    "    return TRUE if 'body' in tags else FALSE\n",
    "\n",
    "def LF_table_in_tag(c):\n",
    "    tags = list(get_ancestor_tag_names(c))\n",
    "    return TRUE if 'table' in tags else ABSTAIN\n",
    "\n",
    "def LF_to_left(c):\n",
    "    return TRUE if overlap(\n",
    "      ['location','locall','outcall','stay','live','available','female escort'], \n",
    "        get_left_ngrams(c, window=3)) else FALSE\n",
    "\n",
    "def LF_to_right(c):\n",
    "    return TRUE if overlap(\n",
    "      ['escorts','incall','outcall','stay','live','available','female escort'], \n",
    "        list(get_right_ngrams(c, window=5))) else ABSTAIN\n",
    "# Need more of these...can check tutorials for inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<function LF_in_breadcrumbs_1 at 0x7f5eefc44b18>, <function LF_head_in_tag at 0x7f5ef6a6b758>, <function LF_to_right at 0x7f5e139b2a28>, <function LF_table_in_tag at 0x7f5e139b28c0>, <function LF_long_candidate at 0x7f5eefc44668>, <function LF_common_real_words at 0x7f5ef6a6ba28>]\n"
     ]
    }
   ],
   "source": [
    "# Collect LFs in list\n",
    "lfs_location = [LF_in_breadcrumbs_1,\n",
    "                #LF_in_breadcrumbs_2,\n",
    "                LF_head_in_tag,\n",
    "                #LF_body_in_tag,\n",
    "                LF_to_right,\n",
    "                #LF_to_left,\n",
    "                LF_table_in_tag,\n",
    "                LF_long_candidate,\n",
    "                LF_common_real_words]\n",
    "print (lfs_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Running Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "Copying location_extraction_label_updates to postgres\n",
      "COPY 705\n",
      "\n",
      "CPU times: user 180 ms, sys: 864 ms, total: 1.04 s\n",
      "Wall time: 5.08 s\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "Copying location_extraction_label_updates to postgres\n",
      "COPY 588\n",
      "\n",
      "CPU times: user 420 ms, sys: 804 ms, total: 1.22 s\n",
      "Wall time: 4.81 s\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "Copying location_extraction_label_updates to postgres\n",
      "COPY 442\n",
      "\n",
      "CPU times: user 124 ms, sys: 780 ms, total: 904 ms\n",
      "Wall time: 4.33 s\n",
      "(588, 10)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer import BatchLabelAnnotator\n",
    "\n",
    "# Annotating candidats using LFs (clear=True replaced existing)\n",
    "labeler = BatchLabelAnnotator(Location_Extraction, lfs=lfs_location)\n",
    "%time L_train = labeler.apply(split=0, clear=True, parallelism=cfg['parallel'],update_keys =True)\n",
    "%time L_dev = labeler.apply(split=1, clear=True, parallelism=cfg['parallel'],update_keys =True)\n",
    "%time L_test = labeler.apply(split=2, clear=True, parallelism=cfg['parallel'],update_keys =True)\n",
    "print(L_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a)Computing Individual LF Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LF_in_breadcrumbs_1\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.0\n",
      "Neg. class accuracy: 1.0\n",
      "Precision            0.0\n",
      "Recall               0.0\n",
      "F1                   0.0\n",
      "----------------------------------------\n",
      "TP: 0 | FP: 0 | TN: 249 | FN: 4\n",
      "========================================\n",
      "\n",
      "LF_head_in_tag\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.869\n",
      "Neg. class accuracy: 0.859\n",
      "Precision            0.863\n",
      "Recall               0.869\n",
      "F1                   0.866\n",
      "----------------------------------------\n",
      "TP: 259 | FP: 41 | TN: 249 | FN: 39\n",
      "========================================\n",
      "\n",
      "LF_to_right\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 1.0\n",
      "Neg. class accuracy: 0.0\n",
      "Precision            1.0\n",
      "Recall               1.0\n",
      "F1                   1.0\n",
      "----------------------------------------\n",
      "TP: 64 | FP: 0 | TN: 0 | FN: 0\n",
      "========================================\n",
      "\n",
      "LF_table_in_tag\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 1.0\n",
      "Neg. class accuracy: 0.0\n",
      "Precision            1.0\n",
      "Recall               1.0\n",
      "F1                   1.0\n",
      "----------------------------------------\n",
      "TP: 24 | FP: 0 | TN: 0 | FN: 0\n",
      "========================================\n",
      "\n",
      "LF_long_candidate\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.0\n",
      "Neg. class accuracy: 1.0\n",
      "Precision            0.0\n",
      "Recall               0.0\n",
      "F1                   0.0\n",
      "----------------------------------------\n",
      "TP: 0 | FP: 0 | TN: 249 | FN: 4\n",
      "========================================\n",
      "\n",
      "LF_common_real_words\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 1.0\n",
      "Neg. class accuracy: 0.0\n",
      "Precision            0.91\n",
      "Recall               1.0\n",
      "F1                   0.953\n",
      "----------------------------------------\n",
      "TP: 141 | FP: 14 | TN: 0 | FN: 0\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from snorkel.lf_helpers import test_LF\n",
    "for lf in lfs_location:\n",
    "    print(lf.__name__)\n",
    "    tp, fp, tn, fn = test_LF(session, lf, split=1, annotator_name='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_extraction(Span(\"Texas\", sentence=935401, chars=[29,33], words=[4,4]))\n",
      "  (0, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "# Showing a candidate from dev set\n",
    "ind = 50\n",
    "print(L_dev.get_candidate(session, ind))\n",
    "print(L_gold_dev[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 120 ms, sys: 60 ms, total: 180 ms\n",
      "Wall time: 123 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_in_breadcrumbs</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_head_in_tag</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.765306</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>259</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>249</td>\n",
       "      <td>0.863946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_body_in_tag</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_to_right</th>\n",
       "      <td>3</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>0.057823</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_in_breadcrumbs_2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_table_in_tag</th>\n",
       "      <td>5</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_in_breadcrumbs_1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>249</td>\n",
       "      <td>0.984190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_to_left</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_real_words</th>\n",
       "      <td>8</td>\n",
       "      <td>0.263605</td>\n",
       "      <td>0.263605</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>141</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_long_candidate</th>\n",
       "      <td>9</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>249</td>\n",
       "      <td>0.984190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      j  Coverage  Overlaps  Conflicts   TP  FP  FN   TN  \\\n",
       "LF_in_breadcrumbs     0  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_head_in_tag        1  1.000000  0.765306   0.059524  259  41  39  249   \n",
       "LF_body_in_tag        2  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_to_right           3  0.108844  0.108844   0.057823   64   0   0    0   \n",
       "LF_in_breadcrumbs_2   4  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_table_in_tag       5  0.040816  0.040816   0.000000   24   0   0    0   \n",
       "LF_in_breadcrumbs_1   6  0.430272  0.430272   0.000000    0   0   4  249   \n",
       "LF_to_left            7  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_common_real_words  8  0.263605  0.263605   0.059524  141  14   0    0   \n",
       "LF_long_candidate     9  0.430272  0.430272   0.000000    0   0   4  249   \n",
       "\n",
       "                      Empirical Acc.  \n",
       "LF_in_breadcrumbs                NaN  \n",
       "LF_head_in_tag              0.863946  \n",
       "LF_body_in_tag                   NaN  \n",
       "LF_to_right                 1.000000  \n",
       "LF_in_breadcrumbs_2              NaN  \n",
       "LF_table_in_tag             1.000000  \n",
       "LF_in_breadcrumbs_1         0.984190  \n",
       "LF_to_left                       NaN  \n",
       "LF_common_real_words        0.909677  \n",
       "LF_long_candidate           0.984190  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading assessing LF performance vs. gold labels\n",
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "%time L_dev.lf_stats(L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10 µs\n",
      "============================================================\n",
      "[1] Testing epochs = 1.00e+02, step_size = 1.00e-03, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "[GenerativeModel] Model saved as <GenerativeModel_0>.\n",
      "[GenerativeModel] Model saved as <GenerativeModel_best>.\n",
      "============================================================\n",
      "[2] Testing epochs = 5.00e+01, step_size = 1.00e-02, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[3] Testing epochs = 1.00e+02, step_size = 1.00e-02, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[4] Testing epochs = 1.00e+02, step_size = 1.00e-03, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[5] Testing epochs = 2.00e+01, step_size = 1.00e-05, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "[GenerativeModel] Model saved as <GenerativeModel_4>.\n",
      "[GenerativeModel] Model saved as <GenerativeModel_best>.\n",
      "============================================================\n",
      "[6] Testing epochs = 1.00e+02, step_size = 1.00e-06, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[7] Testing epochs = 1.00e+02, step_size = 1.00e-02, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[8] Testing epochs = 5.00e+01, step_size = 1.00e-02, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[9] Testing epochs = 1.00e+02, step_size = 1.00e-06, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[10] Testing epochs = 5.00e+01, step_size = 1.00e-03, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.866220735786\n",
      "============================================================\n",
      "[11] Testing epochs = 1.00e+02, step_size = 1.00e-05, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[12] Testing epochs = 5.00e+01, step_size = 1.00e-04, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[13] Testing epochs = 2.00e+01, step_size = 1.00e-06, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[14] Testing epochs = 5.00e+01, step_size = 1.00e-04, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "============================================================\n",
      "[15] Testing epochs = 1.00e+02, step_size = 1.00e-06, decay = 1.00e+00\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.927215189873\n",
      "[GenerativeModel] Model <GenerativeModel_4> loaded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>step_size</th>\n",
       "      <th>decay</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>Rec.</th>\n",
       "      <th>F-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.983221</td>\n",
       "      <td>0.927215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.869128</td>\n",
       "      <td>0.866221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  step_size  decay     Prec.      Rec.       F-1\n",
       "4       20   0.000010   1.00  0.877246  0.983221  0.927215\n",
       "5      100   0.000001   1.00  0.877246  0.983221  0.927215\n",
       "8      100   0.000001   0.95  0.877246  0.983221  0.927215\n",
       "10     100   0.000010   0.90  0.877246  0.983221  0.927215\n",
       "11      50   0.000100   0.90  0.877246  0.983221  0.927215\n",
       "12      20   0.000001   0.90  0.877246  0.983221  0.927215\n",
       "13      50   0.000100   0.95  0.877246  0.983221  0.927215\n",
       "14     100   0.000001   1.00  0.877246  0.983221  0.927215\n",
       "0      100   0.001000   1.00  0.863333  0.869128  0.866221\n",
       "1       50   0.010000   0.95  0.863333  0.869128  0.866221\n",
       "2      100   0.010000   1.00  0.863333  0.869128  0.866221\n",
       "3      100   0.001000   0.95  0.863333  0.869128  0.866221\n",
       "6      100   0.010000   1.00  0.863333  0.869128  0.866221\n",
       "7       50   0.010000   0.90  0.863333  0.869128  0.866221\n",
       "9       50   0.001000   0.95  0.863333  0.869128  0.866221"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "from snorkel.learning import RandomSearch\n",
    "\n",
    "param_ranges = {\n",
    "    'step_size' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "    'decay' : [1.0, 0.95, 0.9],\n",
    "    'epochs' : [20, 50, 100]\n",
    "}\n",
    "\n",
    "searcher = RandomSearch(GenerativeModel, param_ranges, L_train, n=15)\n",
    "\n",
    "%time\n",
    "gen_model, run_stats = searcher.fit(L_dev, L_gold_dev)\n",
    "run_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e18e3ad50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing and plotting training marginals\n",
    "train_marginals = gen_model.marginals(L_train)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84921912, 1.07673239, 0.85064543, 0.94957098, 0.85142461,\n",
       "       0.85070572, 0.83974904, 0.84747935, 0.97313617, 0.84885964])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing LF accuracies\n",
    "gen_model.weights.lf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "      <th>Learned Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_in_breadcrumbs</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.849219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_head_in_tag</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.765306</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>259</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>249</td>\n",
       "      <td>0.863946</td>\n",
       "      <td>1.076732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_body_in_tag</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.850645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_to_right</th>\n",
       "      <td>3</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>0.057823</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_in_breadcrumbs_2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.851425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_table_in_tag</th>\n",
       "      <td>5</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_in_breadcrumbs_1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>249</td>\n",
       "      <td>0.984190</td>\n",
       "      <td>0.839749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_to_left</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.847479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_real_words</th>\n",
       "      <td>8</td>\n",
       "      <td>0.263605</td>\n",
       "      <td>0.263605</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>141</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909677</td>\n",
       "      <td>0.973136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_long_candidate</th>\n",
       "      <td>9</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>249</td>\n",
       "      <td>0.984190</td>\n",
       "      <td>0.848860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      j  Coverage  Overlaps  Conflicts   TP  FP  FN   TN  \\\n",
       "LF_in_breadcrumbs     0  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_head_in_tag        1  1.000000  0.765306   0.059524  259  41  39  249   \n",
       "LF_body_in_tag        2  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_to_right           3  0.108844  0.108844   0.057823   64   0   0    0   \n",
       "LF_in_breadcrumbs_2   4  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_table_in_tag       5  0.040816  0.040816   0.000000   24   0   0    0   \n",
       "LF_in_breadcrumbs_1   6  0.430272  0.430272   0.000000    0   0   4  249   \n",
       "LF_to_left            7  0.000000  0.000000   0.000000    0   0   0    0   \n",
       "LF_common_real_words  8  0.263605  0.263605   0.059524  141  14   0    0   \n",
       "LF_long_candidate     9  0.430272  0.430272   0.000000    0   0   4  249   \n",
       "\n",
       "                      Empirical Acc.  Learned Acc.  \n",
       "LF_in_breadcrumbs                NaN      0.849219  \n",
       "LF_head_in_tag              0.863946      1.076732  \n",
       "LF_body_in_tag                   NaN      0.850645  \n",
       "LF_to_right                 1.000000      0.949571  \n",
       "LF_in_breadcrumbs_2              NaN      0.851425  \n",
       "LF_table_in_tag             1.000000      0.850706  \n",
       "LF_in_breadcrumbs_1         0.984190      0.839749  \n",
       "LF_to_left                       NaN      0.847479  \n",
       "LF_common_real_words        0.909677      0.973136  \n",
       "LF_long_candidate           0.984190      0.848860  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pringint LF stats post-learning\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "prec, rec, f1 = gen_model.score(L_dev, L_gold_dev)\n",
    "L_dev.lf_stats(L_gold_dev, gen_model.weights.lf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[1] Testing dropout = 0.00e+00, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.06s)\tAverage loss=0.788678\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 25 (0.38s)\tAverage loss=0.480607\tDev F1=69.80\n",
      "[SparseLogisticRegression] Epoch 50 (0.97s)\tAverage loss=0.355863\tDev F1=67.99\n",
      "[SparseLogisticRegression] Epoch 75 (1.27s)\tAverage loss=0.311128\tDev F1=71.17\n",
      "[SparseLogisticRegression] Epoch 100 (1.59s)\tAverage loss=0.292562\tDev F1=67.70\n",
      "[SparseLogisticRegression] Epoch 125 (1.90s)\tAverage loss=0.283264\tDev F1=64.73\n",
      "[SparseLogisticRegression] Epoch 150 (2.20s)\tAverage loss=0.277872\tDev F1=63.38\n",
      "[SparseLogisticRegression] Epoch 175 (2.50s)\tAverage loss=0.274410\tDev F1=63.70\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.87s)\tAverage loss=0.272111\tDev F1=63.86\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (3.00s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-199\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.638596491228\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_0>\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_best>\n",
      "============================================================\n",
      "[2] Testing dropout = 5.00e-01, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 25 (0.36s)\tAverage loss=0.480607\tDev F1=69.80\n",
      "[SparseLogisticRegression] Epoch 50 (0.66s)\tAverage loss=0.355863\tDev F1=67.99\n",
      "[SparseLogisticRegression] Epoch 75 (0.98s)\tAverage loss=0.311128\tDev F1=71.17\n",
      "[SparseLogisticRegression] Epoch 100 (1.29s)\tAverage loss=0.292562\tDev F1=67.70\n",
      "[SparseLogisticRegression] Epoch 125 (1.59s)\tAverage loss=0.283264\tDev F1=64.73\n",
      "[SparseLogisticRegression] Epoch 150 (1.90s)\tAverage loss=0.277872\tDev F1=63.38\n",
      "[SparseLogisticRegression] Epoch 175 (2.22s)\tAverage loss=0.274410\tDev F1=63.70\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.60s)\tAverage loss=0.272111\tDev F1=63.86\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (2.74s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-199\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.638596491228\n",
      "============================================================\n",
      "[3] Testing dropout = 0.00e+00, lr = 1.00e-02\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 25 (0.75s)\tAverage loss=0.275965\tDev F1=65.90\n",
      "[SparseLogisticRegression] Epoch 50 (1.07s)\tAverage loss=0.264111\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 75 (1.37s)\tAverage loss=0.263621\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 100 (1.67s)\tAverage loss=0.263494\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 125 (1.98s)\tAverage loss=0.263446\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 150 (2.28s)\tAverage loss=0.263417\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 175 (2.59s)\tAverage loss=0.263397\tDev F1=66.97\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.96s)\tAverage loss=0.263385\tDev F1=66.97\n",
      "[SparseLogisticRegression] Training done (3.01s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.669714285714\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_2>\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_best>\n",
      "============================================================\n",
      "[4] Testing dropout = 0.00e+00, lr = 1.00e-02\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 25 (0.36s)\tAverage loss=0.275965\tDev F1=65.90\n",
      "[SparseLogisticRegression] Epoch 50 (0.67s)\tAverage loss=0.264111\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 75 (0.97s)\tAverage loss=0.263621\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 100 (1.28s)\tAverage loss=0.263494\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 125 (1.59s)\tAverage loss=0.263446\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 150 (1.90s)\tAverage loss=0.263417\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 175 (2.21s)\tAverage loss=0.263397\tDev F1=66.97\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.59s)\tAverage loss=0.263385\tDev F1=66.97\n",
      "[SparseLogisticRegression] Training done (2.64s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.669714285714\n",
      "============================================================\n",
      "[5] Testing dropout = 0.00e+00, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 25 (0.36s)\tAverage loss=0.480607\tDev F1=69.80\n",
      "[SparseLogisticRegression] Epoch 50 (0.67s)\tAverage loss=0.355863\tDev F1=67.99\n",
      "[SparseLogisticRegression] Epoch 75 (0.97s)\tAverage loss=0.311128\tDev F1=71.17\n",
      "[SparseLogisticRegression] Epoch 100 (1.55s)\tAverage loss=0.292562\tDev F1=67.70\n",
      "[SparseLogisticRegression] Epoch 125 (1.86s)\tAverage loss=0.283264\tDev F1=64.73\n",
      "[SparseLogisticRegression] Epoch 150 (2.17s)\tAverage loss=0.277872\tDev F1=63.38\n",
      "[SparseLogisticRegression] Epoch 175 (2.48s)\tAverage loss=0.274410\tDev F1=63.70\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.84s)\tAverage loss=0.272111\tDev F1=63.86\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (2.97s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-199\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.638596491228\n",
      "============================================================\n",
      "[6] Testing dropout = 0.00e+00, lr = 1.00e-05\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.36s)\tAverage loss=0.784480\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 50 (0.67s)\tAverage loss=0.780311\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 75 (0.97s)\tAverage loss=0.776173\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 100 (1.28s)\tAverage loss=0.772065\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 125 (1.58s)\tAverage loss=0.767988\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 150 (1.89s)\tAverage loss=0.763942\tDev F1=63.09\n",
      "[SparseLogisticRegression] Epoch 175 (2.22s)\tAverage loss=0.759925\tDev F1=63.09\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseLogisticRegression] Epoch 199 (2.58s)\tAverage loss=0.756098\tDev F1=63.09\n",
      "[SparseLogisticRegression] Training done (2.63s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.630872483221\n",
      "============================================================\n",
      "[7] Testing dropout = 0.00e+00, lr = 1.00e-06\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.36s)\tAverage loss=0.788257\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 50 (0.66s)\tAverage loss=0.787837\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 75 (0.96s)\tAverage loss=0.787417\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 100 (1.27s)\tAverage loss=0.786998\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 125 (1.58s)\tAverage loss=0.786578\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 150 (1.89s)\tAverage loss=0.786159\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 175 (2.42s)\tAverage loss=0.785740\tDev F1=62.16\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.80s)\tAverage loss=0.785338\tDev F1=62.16\n",
      "[SparseLogisticRegression] Training done (2.85s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.621621621622\n",
      "============================================================\n",
      "[8] Testing dropout = 5.00e-01, lr = 1.00e-02\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.06s)\tAverage loss=0.788678\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 25 (0.40s)\tAverage loss=0.275965\tDev F1=65.90\n",
      "[SparseLogisticRegression] Epoch 50 (0.70s)\tAverage loss=0.264111\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 75 (1.02s)\tAverage loss=0.263621\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 100 (1.32s)\tAverage loss=0.263494\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 125 (1.63s)\tAverage loss=0.263446\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 150 (1.93s)\tAverage loss=0.263417\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 175 (2.23s)\tAverage loss=0.263397\tDev F1=66.97\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.61s)\tAverage loss=0.263385\tDev F1=66.97\n",
      "[SparseLogisticRegression] Training done (2.65s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.669714285714\n",
      "============================================================\n",
      "[9] Testing dropout = 5.00e-01, lr = 1.00e-02\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 25 (0.38s)\tAverage loss=0.275965\tDev F1=65.90\n",
      "[SparseLogisticRegression] Epoch 50 (0.69s)\tAverage loss=0.264111\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 75 (0.99s)\tAverage loss=0.263621\tDev F1=66.67\n",
      "[SparseLogisticRegression] Epoch 100 (1.30s)\tAverage loss=0.263494\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 125 (1.60s)\tAverage loss=0.263446\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 150 (1.91s)\tAverage loss=0.263417\tDev F1=66.82\n",
      "[SparseLogisticRegression] Epoch 175 (2.25s)\tAverage loss=0.263397\tDev F1=66.97\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.61s)\tAverage loss=0.263385\tDev F1=66.97\n",
      "[SparseLogisticRegression] Training done (2.66s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.669714285714\n",
      "============================================================\n",
      "[10] Testing dropout = 0.00e+00, lr = 1.00e-06\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.37s)\tAverage loss=0.788257\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 50 (0.68s)\tAverage loss=0.787837\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 75 (0.99s)\tAverage loss=0.787417\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 100 (1.30s)\tAverage loss=0.786998\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 125 (1.60s)\tAverage loss=0.786578\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 150 (1.91s)\tAverage loss=0.786159\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 175 (2.21s)\tAverage loss=0.785740\tDev F1=62.16\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.59s)\tAverage loss=0.785338\tDev F1=62.16\n",
      "[SparseLogisticRegression] Training done (2.64s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.621621621622\n",
      "============================================================\n",
      "[11] Testing dropout = 5.00e-01, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 25 (0.37s)\tAverage loss=0.480607\tDev F1=69.80\n",
      "[SparseLogisticRegression] Epoch 50 (0.68s)\tAverage loss=0.355863\tDev F1=67.99\n",
      "[SparseLogisticRegression] Epoch 75 (0.98s)\tAverage loss=0.311128\tDev F1=71.17\n",
      "[SparseLogisticRegression] Epoch 100 (1.29s)\tAverage loss=0.292562\tDev F1=67.70\n",
      "[SparseLogisticRegression] Epoch 125 (1.60s)\tAverage loss=0.283264\tDev F1=64.73\n",
      "[SparseLogisticRegression] Epoch 150 (1.91s)\tAverage loss=0.277872\tDev F1=63.38\n",
      "[SparseLogisticRegression] Epoch 175 (2.21s)\tAverage loss=0.274410\tDev F1=63.70\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.59s)\tAverage loss=0.272111\tDev F1=63.86\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (2.72s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-199\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.638596491228\n",
      "============================================================\n",
      "[12] Testing dropout = 5.00e-01, lr = 1.00e-05\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.37s)\tAverage loss=0.784480\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 50 (0.68s)\tAverage loss=0.780311\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 75 (1.00s)\tAverage loss=0.776173\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 100 (1.30s)\tAverage loss=0.772065\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 125 (1.62s)\tAverage loss=0.767988\tDev F1=62.63\n",
      "[SparseLogisticRegression] Epoch 150 (1.93s)\tAverage loss=0.763942\tDev F1=63.09\n",
      "[SparseLogisticRegression] Epoch 175 (2.24s)\tAverage loss=0.759925\tDev F1=63.09\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.62s)\tAverage loss=0.756098\tDev F1=63.09\n",
      "[SparseLogisticRegression] Training done (2.67s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.630872483221\n",
      "============================================================\n",
      "[13] Testing dropout = 0.00e+00, lr = 1.00e-04\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.06s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.38s)\tAverage loss=0.747784\tDev F1=63.55\n",
      "[SparseLogisticRegression] Epoch 50 (0.70s)\tAverage loss=0.709641\tDev F1=64.23\n",
      "[SparseLogisticRegression] Epoch 75 (1.00s)\tAverage loss=0.674383\tDev F1=64.36\n",
      "[SparseLogisticRegression] Epoch 100 (1.32s)\tAverage loss=0.641896\tDev F1=65.80\n",
      "[SparseLogisticRegression] Epoch 125 (1.63s)\tAverage loss=0.612017\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 150 (1.95s)\tAverage loss=0.584581\tDev F1=66.34\n",
      "[SparseLogisticRegression] Epoch 175 (2.26s)\tAverage loss=0.559421\tDev F1=66.34\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.63s)\tAverage loss=0.537254\tDev F1=66.34\n",
      "[SparseLogisticRegression] Training done (2.68s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.663430420712\n",
      "============================================================\n",
      "[14] Testing dropout = 5.00e-01, lr = 1.00e-06\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.37s)\tAverage loss=0.788257\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 50 (0.89s)\tAverage loss=0.787837\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 75 (1.20s)\tAverage loss=0.787417\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 100 (1.52s)\tAverage loss=0.786998\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 125 (1.84s)\tAverage loss=0.786578\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 150 (2.17s)\tAverage loss=0.786159\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 175 (2.48s)\tAverage loss=0.785740\tDev F1=62.16\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.85s)\tAverage loss=0.785338\tDev F1=62.16\n",
      "[SparseLogisticRegression] Training done (2.90s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.621621621622\n",
      "============================================================\n",
      "[15] Testing dropout = 0.00e+00, lr = 1.00e-04\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=114  #epochs=200  batch size=114\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.788678\tDev F1=62.16\n",
      "[SparseLogisticRegression] Epoch 25 (0.37s)\tAverage loss=0.747784\tDev F1=63.55\n",
      "[SparseLogisticRegression] Epoch 50 (0.68s)\tAverage loss=0.709641\tDev F1=64.23\n",
      "[SparseLogisticRegression] Epoch 75 (0.98s)\tAverage loss=0.674383\tDev F1=64.36\n",
      "[SparseLogisticRegression] Epoch 100 (1.29s)\tAverage loss=0.641896\tDev F1=65.80\n",
      "[SparseLogisticRegression] Epoch 125 (1.60s)\tAverage loss=0.612017\tDev F1=65.91\n",
      "[SparseLogisticRegression] Epoch 150 (1.91s)\tAverage loss=0.584581\tDev F1=66.34\n",
      "[SparseLogisticRegression] Epoch 175 (2.22s)\tAverage loss=0.559421\tDev F1=66.34\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 199 (2.62s)\tAverage loss=0.537254\tDev F1=66.34\n",
      "[SparseLogisticRegression] Training done (2.67s)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression/SparseLogisticRegression-175\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1 Score: 0.663430420712\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/grid_search/SparseLogisticRegression_2/SparseLogisticRegression_2-0\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression_2>\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "\n",
    "disc_model = SparseLogisticRegression\n",
    "param_ranges = {\n",
    "    'lr' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "    'dropout' : [0.0, 0.5]\n",
    "}\n",
    "\n",
    "model_hyperparams = {\n",
    "    'n_epochs' : 200,\n",
    "    'rebalance' : 0.5,\n",
    "    'print_freq' : 25\n",
    "}\n",
    "\n",
    "# We now add a session and probabilistic labels, as well as pass in the candidates\n",
    "# instead of the label matrix\n",
    "searcher = RandomSearch(disc_model, param_ranges, F_train, Y_train=train_marginals, n=15,\n",
    "    model_hyperparams=model_hyperparams)\n",
    "\n",
    "# We now pass in the development candidates and the gold development labels\n",
    "trained_model, run_stats = searcher.fit(F_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate discriminative on test set \n",
    "L_gold_test = np.array(load_gold_labels(session, annotator_name='gold', split=2).todense()).squeeze()\n",
    "# Get candidates, discriminative model outputs, and discriminative model predicts\n",
    "test_candidates = [F_test.get_candidate(session, i) for i in range(F_test.shape[0])]\n",
    "test_score = np.array(trained_model.predictions(F_test))\n",
    "true_pred = [test_candidates[_] for _ in np.nditer(np.where(test_score > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5475113122171946\n"
     ]
    }
   ],
   "source": [
    "# L_gold_test\n",
    "corr = [ test_score[i] == L_gold_test[i] for i in range(len(test_score))]\n",
    "acc = np.sum(corr)/len(corr)\n",
    "print (acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.918552036199095\n"
     ]
    }
   ],
   "source": [
    "# Assessing values \n",
    "gen_model_preds = (gen_model.marginals(L_test)>0.5)*2-1\n",
    "gen_corr = [ gen_model_preds[i] == L_gold_test[i] for i in range(len(gen_model_preds))]\n",
    "gen_acc = np.sum(gen_corr)/len(gen_corr)\n",
    "print(gen_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9217687074829932\n"
     ]
    }
   ],
   "source": [
    "# Assessing values\n",
    "L_gold_dev = np.array(load_gold_labels(session, annotator_name='gold', split=1).todense()).squeeze()\n",
    "gen_model_preds = (gen_model.marginals(L_dev)>0.5)*2-1\n",
    "gen_corr = [ gen_model_preds[i] == L_gold_dev[i] for i in range(len(gen_model_preds))]\n",
    "gen_acc = np.sum(gen_corr)/len(gen_corr)\n",
    "print(gen_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Creating and Saving Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#getting google place and geocoding APIs\n",
    "import googlemaps as gm\n",
    "import gmaps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import MultiPoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "maps_api_key = 'AIzaSyA0Veo5Lc6JOwDjNgQvPEhQB4AiZcrYQGI'\n",
    "gmaps.configure(api_key=maps_api_key)\n",
    "\n",
    "def get_possible_locations(plc):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    plc: string describing place to match\n",
    "\n",
    "    OUTPUTS\n",
    "    qo: full json structure returned from API call\n",
    "    cl: list of candidate location strings\n",
    "    \"\"\" \n",
    "    api_key = 'AIzaSyDbk3lLZHuQVKDRBN99_oz-p4AJjIzhA0w'\n",
    "    gms = gm.Client(key=api_key)\n",
    "    qo = gm.places.places_autocomplete(gms,plc)\n",
    "    cl = [a['description'] for a in qo]\n",
    "    return qo,cl\n",
    "\n",
    "def get_geocode(plc):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    plc: string describing place to match\n",
    "\n",
    "    OUTPUTS\n",
    "    qo full json structure returned from API call\n",
    "    (lat,lon): lat-lon tuple\n",
    "    \"\"\"\n",
    "    api_key = 'AIzaSyBlLyOaasYMgMxFGUh2jJyxIG0_pZFF_jM'\n",
    "    gms = gm.Client(key=api_key)\n",
    "    qo = gm.geocoding.geocode(gms,plc)\n",
    "    lat = qo[0]['geometry']['location']['lat']\n",
    "    lng = qo[0]['geometry']['location']['lng']\n",
    "    return qo,(lat,lng)\n",
    "\n",
    "def slice_pd_by_cont(dfm,col,val,pres=True,lower=False,union=False):\n",
    "    \"\"\"\n",
    "    Returns dataframe where column values include/exclude values in provided list\n",
    "    \n",
    "    INPUTS:\n",
    "    dfm: dataframe\n",
    "    col: column header\n",
    "    val: list of strings to include/ignore\n",
    "    pres: true to include, false to exclude\n",
    "    union: include union of these values\n",
    "    \"\"\"\n",
    "    if union:\n",
    "        val = ['|'.join(val)]\n",
    "    for vl in val:\n",
    "        if ~lower:\n",
    "            if pres:\n",
    "                dfm = dfm.loc[dfm[col].str.contains(vl,na=False)]\n",
    "            else:\n",
    "                dfm = dfm.loc[~dfm[col].str.contains(vl,na=False)]\n",
    "        else:\n",
    "            if pres:\n",
    "                dfm = dfm.loc[dfm[col].str.lower().str.contains(vl,na=False)]\n",
    "            else:\n",
    "                dfm = dfm.loc[~dfm[col].str.lower().str.contains(vl,na=False)]\n",
    "    return dfm\n",
    "\n",
    "def map_candidates_and_centroid(dfm):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    dfm: dataframe containing at least latitude, longitude\n",
    "    \n",
    "    OUTPUT\n",
    "    centroid: np array of lat/lon of location centroid\n",
    "    \"\"\"\n",
    "    df_cans = dfm\n",
    "    df_cans_map = dfm[['latitude','longitude']]\n",
    "    df_cans['lat_long'] = df_cans[['latitude', 'longitude']].apply(tuple, axis=1)\n",
    "    point_tup_lst = df_cans['lat_long'].tolist()\n",
    "    points = MultiPoint(point_tup_lst)\n",
    "    cent = np.array(points.centroid)\n",
    "    cent_df = pd.DataFrame([cent]) #this is a rough centroid estimate\n",
    "    fig = gmaps.Map()\n",
    "    can_layer = gmaps.symbol_layer(\n",
    "    df_cans_map, fill_color=\"green\", stroke_color=\"green\", scale=2)\n",
    "    cent_layer = gmaps.symbol_layer(\n",
    "    cent_df, fill_color=\"red\", stroke_color=\"red\", scale=2)\n",
    "    fig.add_layer(can_layer)\n",
    "    fig.add_layer(cent_layer)\n",
    "    fig\n",
    "    return cent,fig\n",
    "\n",
    "def get_attr(obj):\n",
    "    out = [a for a in dir(obj) if not a.startswith('__') and not callable(getattr(obj,a))]\n",
    "    return out\n",
    "\n",
    "def most_common(lt):\n",
    "    data = Counter(lt)\n",
    "    return data.most_common(1)[0][0]\n",
    "\n",
    "def get_common_country(lt):\n",
    "    country_lst = []\n",
    "    country_els = []\n",
    "    for it in lt:\n",
    "        try:\n",
    "            country = pycountry.countries.lookup(it.lower())\n",
    "            country_lst.append(country.alpha_3)\n",
    "            country_els.append(it)\n",
    "        except:\n",
    "            country = None \n",
    "    if country_lst == []:\n",
    "        return 'none',[],[]\n",
    "    return most_common(country_lst),country_lst, country_els\n",
    "\n",
    "def get_common_state(lt):\n",
    "    state_lst = []\n",
    "    state_els = []\n",
    "    for it in lt:\n",
    "        sts = [a.lower() for a in state_add_dict.keys()]\n",
    "        abbs = [a.lower() for a in state_add_dict.values()]\n",
    "        if it in sts:\n",
    "            state_lst.append(it)\n",
    "            state_els.append(it)\n",
    "        elif it in abbs:\n",
    "            state_lst.append(state_dict[it])\n",
    "            state_els.append(it)\n",
    "    if state_lst == []:\n",
    "        return 'none',[],[]\n",
    "    else:\n",
    "        return most_common(state_lst), state_lst, state_els\n",
    "\n",
    "def get_possible_locale(lt,cn,st,cn_lst,st_lst):\n",
    "    locale_list = []\n",
    "    a = [b for b in lt if b not in cn_lst and b not in st_lst]\n",
    "    for b in a:\n",
    "        locales = get_possible_locations(b)\n",
    "        locales = [c for c in locales if cn in b and st in b]\n",
    "        locale_list.append(locales)\n",
    "    return locale_list\n",
    "\n",
    "# Need to unify this!\n",
    "def lookup_state_abbrev(cn):\n",
    "    try:\n",
    "        out = state_add_dict[cn]\n",
    "    except:\n",
    "        out = 'no state'\n",
    "    return out\n",
    "\n",
    "state_dict = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "}\n",
    "state_add_dict = {v: k for k, v in state_dict.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_extractions = defaultdict(list)\n",
    "num_test_cands = F_test.shape[0]\n",
    "test_cand_preds = (gen_model.marginals(L_test)>0.5)*2-1\n",
    "for ind in range(num_test_cands):\n",
    "    cand = F_test.get_candidate(session,ind)\n",
    "    parent = cand.get_parent()\n",
    "    doc_name = parent.document.name\n",
    "    # Initializing key if it doesn't exist\n",
    "    doc_extractions[doc_name]\n",
    "    loc = cand.location.get_span().lower()\n",
    "    if test_cand_preds[ind] == 1:\n",
    "        doc_extractions[doc_name].append(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_extractions = defaultdict(set)\n",
    "for doc_name, extract_list in doc_extractions.items():\n",
    "    out_extractions[doc_name] = list(set(extract_list))\n",
    "df_out = pd.DataFrame()\n",
    "df_out = df_labeled[df_labeled['file name'].isin(list(out_extractions.keys()))]\n",
    "df_out['extracted_location'] = df_out.apply(lambda row: out_extractions[row['file name']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv('../output/location_extractions.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'austin', u'trishia austin', u'austin']\n",
      "Checking Locale 0 of 2\n",
      "> <ipython-input-170-2d31084fc3c4>(31)<module>()\n",
      "-> if lookup_country_name(probable_country).lower() in spl:\n",
      "(Pdb) spl\n",
      "['tricia cove', 'hutto', 'tx', 'usa']\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(48)<module>()\n",
      "-> count = count+1\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(26)<module>()\n",
      "-> while not_exact and count<len(locales):\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(27)<module>()\n",
      "-> print('Checking Locale %d of %d' %(count,len(locales)))\n",
      "(Pdb) n\n",
      "Checking Locale 1 of 2\n",
      "> <ipython-input-170-2d31084fc3c4>(28)<module>()\n",
      "-> c = locales[count]\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(29)<module>()\n",
      "-> spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(29)<module>()\n",
      "-> spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
      "(Pdb) spl\n",
      "['tricia cove', 'hutto', 'tx', 'usa']\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(29)<module>()\n",
      "-> spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
      "(Pdb) spl\n",
      "['tricia cove', 'hutto', 'tx', 'usa']\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(29)<module>()\n",
      "-> spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(29)<module>()\n",
      "-> spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(30)<module>()\n",
      "-> import pdb; pdb.set_trace()\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(31)<module>()\n",
      "-> if lookup_country_name(probable_country).lower() in spl:\n",
      "(Pdb) spl\n",
      "['tricia lane', 'hutto', 'tx', 'usa']\n",
      "(Pdb) count\n",
      "1\n",
      "(Pdb) spl\n",
      "['tricia lane', 'hutto', 'tx', 'usa']\n",
      "(Pdb) c\n",
      "Checking Locale 0 of 5\n",
      "> <ipython-input-170-2d31084fc3c4>(30)<module>()\n",
      "-> import pdb; pdb.set_trace()\n",
      "(Pdb) spl\n",
      "['austin', 'tx', 'usa']\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(31)<module>()\n",
      "-> if lookup_country_name(probable_country).lower() in spl:\n",
      "(Pdb) n\n",
      "> <ipython-input-170-2d31084fc3c4>(48)<module>()\n",
      "-> count = count+1\n",
      "(Pdb) lookup_country_name(probable_country).lower() in spl\n",
      "False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-2d31084fc3c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-170-2d31084fc3c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/pdb.pyc\u001b[0m in \u001b[0;36muser_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_mainpyfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbp_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbp_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/pdb.pyc\u001b[0m in \u001b[0;36minteraction\u001b[0;34m(self, frame, traceback)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_stack_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/cmd.pyc\u001b[0m in \u001b[0;36mcmdloop\u001b[0;34m(self, intro)\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_rawinput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EOF'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out_locales = defaultdict(list)\n",
    "for doc_name, extract_list in doc_extractions.items():\n",
    "    \n",
    "    # Getting country names\n",
    "    probable_country,country_list, country_els = get_common_country(extract_list)\n",
    "    if lookup_country_alpha3(probable_country) == 'USA' and len(extract_list) >1:\n",
    "        # Getting state names\n",
    "        probable_state,state_list,state_els = get_common_state(extract_list)\n",
    "    else:\n",
    "        probable_state,state_list,state_els = 'none',[],[]\n",
    "    \n",
    "    locale_list = []\n",
    "    a = [b for b in extract_list if b not in country_els and b not in state_els] #need lookup here\n",
    "    print(a)\n",
    "    if a == []:\n",
    "        if probable_state != 'none' and probable_country != 'none' and a == []:\n",
    "            locale_list = ['none,none,'+state_add_dict[probable_state]+','+probable_country]\n",
    "    else:\n",
    "        most_common_locale = most_common(a)\n",
    "        aset = list(set(a))\n",
    "        for b in aset:\n",
    "                locale_tmp = []\n",
    "                qo,locales = get_possible_locations(b)\n",
    "                not_exact = 1\n",
    "                count = 0\n",
    "                while not_exact and count<len(locales):\n",
    "                    print('Checking Locale %d of %d' %(count,len(locales)))\n",
    "                    c = locales[count]\n",
    "                    spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
    "                    import pdb; pdb.set_trace()\n",
    "                    if lookup_country_name(probable_country).lower() in spl:\n",
    "                        if lookup_state_abbrev(probable_state).lower() in spl: \n",
    "                            if spl[0].lower() == most_common_locale.lower() and len(spl) == 3:\n",
    "                                locale_list = ['none']+spl\n",
    "                                locale_list = [','.join(locale_list)]\n",
    "                                not_exact = 0\n",
    "                                print('Exact City Found')\n",
    "                            elif spl[0].lower() == most_common_locale.lower() and len(spl) == 4:\n",
    "                                locale_list = [','.join(spl)]\n",
    "                                not_exact = 0\n",
    "                                print('Exact Location Found')\n",
    "                            else:             \n",
    "                                locale_list.append(','.join(spl))  \n",
    "                                count = count+1\n",
    "                        else:\n",
    "                            count = count+1         \n",
    "                    else:\n",
    "                        count = count+1\n",
    "        \n",
    "    import pdb; pdb.set_trace()\n",
    "    #reformatting for labeling comparison\n",
    "    locale_list_out = []\n",
    "    for c in locale_list:\n",
    "        b = c.split(',')\n",
    "        print(b)\n",
    "        b[-1] = str(lookup_country_alpha3(b[-1]).lower())\n",
    "        b[-2] = state_dict[b[-2].upper()].lower()\n",
    "        locale_list_out.append(','.join(b)) \n",
    "    out_locales[doc_name] = locale_list_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ii in out_locales.keys():\n",
    "    if out_locales[ii] == []:\n",
    "        out_locales[ii] = ['none','none','none','none']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
